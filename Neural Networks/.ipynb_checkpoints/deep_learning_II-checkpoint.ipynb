{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Deep Learning and Text Analytics II</center>\n",
    "\n",
    "References:\n",
    "- General introduction\n",
    "     - https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/\n",
    "     - http://neuralnetworksanddeeplearning.com\n",
    "     - http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n",
    "- Word vector:\n",
    "     - https://code.google.com/archive/p/word2vec/\n",
    "- Keras tutorial\n",
    "     - https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "- CNN\n",
    "     - http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agenda\n",
    "- Introduction to neural networks\n",
    "- Word/Document Vectors (vector representation of words/phrases/paragraphs)\n",
    "- Convolutional neural network (CNN)\n",
    "- Application of CNN in text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word2Vector (a.k.a word embedding) and Doc2Vector\n",
    "\n",
    "### 4.1. Word2Vector\n",
    " - Vector representation of words (i.e. word vectors) learned using neural network\n",
    "   - e.g. \"apple\" : [0.35, -0.2, 0.4, ...], 'mongo':  [0.32, -0.18, 0.5, ...]\n",
    "   - Interesting properties of word vectors:\n",
    "    * **Words with similar semantics have close word vectors**\n",
    "    * **Composition**: e.g. vector(\"woman\")+vector(\"king\")-vector('man') $\\approx$ vector(\"queen\")\n",
    " - Models:\n",
    "   - **CBOW** (Continuous Bag of Words): Predict a target word based on context\n",
    "     - e.g. the fox jumped over the lazy dog\n",
    "     - Assuming symmetric context with window size 3, this sentence can create training samples: \n",
    "       - ([-, fox], the) \n",
    "       - ([the, jumped], fox) \n",
    "       - ([fox, over], jumped)\n",
    "       - ([jumped, the], over) \n",
    "       - ...\n",
    "       \n",
    "       <img src=\"cbow.png\" width=\"30%\">\n",
    "       source: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "   - **Skip Gram**: predict context based on target words\n",
    "   \n",
    "        <img src=\"skip_gram.png\" width=\"30%\">\n",
    "        source: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "   - Nagtive Sampling: \n",
    "       - When training a neural network, for each sample,  all weights are adjusted slightly so that it predicts that training sample more accurately. \n",
    "       - CBOW or skip gram models have tremendous number of weights, all of which would be updated slightly by every one of billions of training samples!\n",
    "       - Negative sampling addresses this by having **each training sample only modify a small percentage of the weights, rather than all of them**. \n",
    "       - e.g. when training with sample ([fox, over], jumped), update output weights connected to \"jumped\" along with a small number of other \"negative words\" sampled randomly\n",
    "       - For details, check http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up interactive shell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4.1.1 Train your word vector\n",
    "\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "\n",
    "# Load data\n",
    "data=pd.read_csv('../../dataset/amazon_review_large.csv', header=None)\n",
    "data.columns=['label','text']\n",
    "data.head()\n",
    "\n",
    "# tokenize each document into a list of unigrams\n",
    "# strip punctuations and leading/trailing spaces from unigrams\n",
    "# only unigrams with 2 or more characters are taken\n",
    "sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in data[\"text\"]]\n",
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train your own word vectors using gensim\n",
    "\n",
    "# gensim.models is the package for word2vec\n",
    "# check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# for detailed description\n",
    "\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# print out tracking information\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# min_count: words with total frequency lower than this are ignored\n",
    "# size: the dimension of word vector\n",
    "# window: context window, i.e. the maximum distance \n",
    "#         between the current and predicted word \n",
    "#         within a sentence (i.e. the length of ngrams)\n",
    "# workers: # of parallel threads in training\n",
    "# for other parameters, check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "wv_model = word2vec.Word2Vec(sentences, min_count=5, size=200, window=5, workers=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test word2vec model\n",
    "\n",
    "print(\"Top 5 words similar to word 'sound'\")\n",
    "wv_model.wv.most_similar('sound', topn=5)\n",
    "\n",
    "print(\"Top 5 words similar to word 'sound' but not relevant to 'film'\")\n",
    "wv_model.wv.most_similar(positive=['sound','music'], negative=['film'], topn=5)\n",
    "\n",
    "print(\"Similarity between 'movie' and 'film':\")\n",
    "wv_model.wv.similarity('movie','film') \n",
    "\n",
    "print(\"Similarity between 'movie' and 'city':\")\n",
    "wv_model.wv.similarity('movie','city') \n",
    "\n",
    "print(\"Word does not match with others in the list of \\\n",
    "['sound', 'music', 'graphics', 'actor', 'book']:\")\n",
    "wv_model.wv.doesnt_match([\"sound\", \"music\", \"graphics\", \"actor\", \"book\"])\n",
    "\n",
    "print(\"Word vector for 'movie':\")\n",
    "wv_model.wv['movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Pretrained Word Vectors\n",
    "- Google published pre-trained 300-dimensional vectors for 3 million words and phrases that were trained on Google News dataset (about 100 billion words)(https://code.google.com/archive/p/word2vec/)\n",
    "- GloVe (Global Vectors for Word Representation): Pretained word vectors from different data sources provided by Standford https://nlp.stanford.edu/projects/glove/\n",
    "- FastText by Facebook https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4.2.1: Use pretrained word vectors\n",
    "\n",
    "# download the bin file for pretrained word vectors\n",
    "# from above links, e.g. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "# Warning: the bin file is very big (over 2G)\n",
    "# You need a powerful machine to load it\n",
    "\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) \n",
    "\n",
    "model.wv.most_similar(positive=['women','king'], negative='man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Sentence/Paragraph/Document Vectors\n",
    "- So far we learned vector representation of words\n",
    "- A lot of times, our samples are sentences, paragraphs, or documents\n",
    "- How to create vector representations of sentences, paragraphs, or documents?\n",
    "  - Weighted average of word vectors (however, word order is lost as \"bag of words\")\n",
    "  - Concatenation of word vectors (large space)\n",
    "  - ??\n",
    "- Paragraph Vector: A distributed memory model (PV-DM)\n",
    "   - Word vectors are shared across paragraphs\n",
    "   - The paragraph vector is shared across all contexts generated from the same paragraph but not across paragraphs\n",
    "   - **Both pragraph vectors and word vectors** are returned\n",
    "   - Paragraph vectors can be used for document retrival or as features for classification or clustering\n",
    "  <img src=\"doc2vec.png\" width=\"50%\">\n",
    "   Source: Le Q. and Mikolov, T. Distributed Representations of Sentences and Documents https://arxiv.org/pdf/1405.4053v2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4.3.1 Train your word vector\n",
    "\n",
    "# We have tokenized sentences\n",
    "# Label each sentence with a unique tag\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "docs=[TaggedDocument(sentences[i], [str(i)]) for i in range(len(sentences)) ]\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# package for doc2vec\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "# for more parameters, check\n",
    "# https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "# initialize the model without documents\n",
    "# distributed memory model is used (dm=1)\n",
    "model = doc2vec.Doc2Vec(dm=1, min_count=5, window=5, size=200, workers=4)\n",
    "\n",
    "# build the vocabulary using the documents\n",
    "model.build_vocab(docs) \n",
    "\n",
    "# train the model in 20 epoches\n",
    "# You may need to incease epoches\n",
    "for epoch in range(30):\n",
    "    # shuffle the documents in each epoch\n",
    "    shuffle(docs)\n",
    "    # in each epoch, all samples are used\n",
    "    model.train(docs, total_examples=len(docs), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect paragraph vectors and word vectors\n",
    "\n",
    "# the pragraph vector of the first document\n",
    "model.docvecs['0']\n",
    "\n",
    "# the word vector of 'movie'\n",
    "model.wv['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check word similarity\n",
    "\n",
    "print(\"Top 5 words similar to word 'sound'\")\n",
    "model.wv.most_similar('sound', topn=5)\n",
    "\n",
    "print(\"Top 5 words similar to word 'sound' but not relevant to 'film'\")\n",
    "model.wv.most_similar(positive=['sound','music'], negative=['film'], topn=5)\n",
    "\n",
    "print(\"Similarity between 'movie' and 'film':\")\n",
    "model.wv.similarity('movie','film') \n",
    "\n",
    "print(\"Similarity between 'movie' and 'city':\")\n",
    "model.wv.similarity('movie','city') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect document similarity\n",
    "\n",
    "model.docvecs.most_similar('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convolutional Neural Networks (CNN)\n",
    "References (**highly recommended**): \n",
    "- http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "- https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- CNNs are widely used in Computer Vision\n",
    "- CNNs were responsible for **major breakthroughs** in **Image Recognition** and are the core of most Computer Vision systems including automated photo tagging, self-driving cars\n",
    "- Recently, CNNs have been applied in NLP and achieved good performance.\n",
    "<img src='cnn.png' width='90%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Convolution\n",
    "- Convolution is the technique to **extract distinguishing features** from feature spaces\n",
    "- Example: feature detection from image pixels\n",
    "  - Feature space: a matrix of pixels of 0 (black) or 1 (white)\n",
    "  - **Filter/kernal/feature Detector**: a function applied to every fixed subset of the feature matrix\n",
    "    - e.g. 3x3 filter (a 3x3 matrix $\\begin{vmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{vmatrix}$ ) slides through every area of the matrix sequentially, multiplies its values element-wise with the original matrix, then sum them up\n",
    "    - e.g. a filter (e.g. $\\begin{vmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & -4 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{vmatrix}$ ) to take difference between a pixel and its neighbors --> detect edges\n",
    "    <img src='convolution.png' width=\"60%\">\n",
    "- Typically, a larger number of filters in different sizes will be used\n",
    "- Configuration of filters\n",
    "  - filter size ($h \\text{x} w$)\n",
    "  - stride size (how much to shift a filter in each step) ($s$)\n",
    "  - number of filters (depth) ($d$)\n",
    "- Questions: \n",
    "  - With 5x5 feature space, afte apply a filter of size 3x3 with stride size 2, what will be the size of the result? \n",
    "  - Formula to calculate the size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Pooling Layer\n",
    "- Pooling layers are typically applied after the convolutional layers. \n",
    "- A pooling layer subsamples its input. \n",
    "- The most common way to do pooling is to apply a **max** operation to the result of each filter (a.k.a 1-max pooling).\n",
    "  - e.g. for the example below, by 1-max pooling, we get 8.\n",
    "  - If 100 filters have been used, then we get 100 numbers\n",
    "- Pooling can be applied over a window (e.g. 2x2)\n",
    "<img src='max_pooling.png' width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. What are CNNs\n",
    "- CNNs consists of several layers of convolutions with nonlinear activation functions like ReLU or tanh \n",
    "\n",
    "<img src='cnn.png' width='70%'>\n",
    "\n",
    "- A CNN typically contains:\n",
    "  - A **convolution layer** (not dense layer) connected to the input layer\n",
    "      - Each convolution layer applies different filters. \n",
    "      - Typically hundreds or thousands filters used. \n",
    "      - The results of filters are concatenated.\n",
    "  - A **pooling layer** is used to subsample the result of convolution layer\n",
    "  - There may be multiple layers of convolution and pooling combined. E.g. image detection\n",
    "    - 1st layer: detect edges\n",
    "    - 2nd layer: detect shape, e.g. round, square\n",
    "    - 3rd layer: wheels, doors etc.\n",
    "  - Then each result out of convolution-pooling is connected to a neuron in the output (local connections). Such  results results are high-level features used by classification algorithms. \n",
    "- During the training phase, a CNN **automatically learns the values of its filters based on the task you want to perform**. \n",
    "- Powerful capabilities of CNN:\n",
    "  - **Location Invariance**: CNN extracts distinguishing features by convolution-pooling and it does not care where these features are. So images can still be recognized after rotation and scaling.\n",
    "  - **Compositionality**: Each filter composes a local patch of lower-level features into higher-level representation. E.g., detect edges from pixels, shapes from edges, and more complex objects from shapes. \n",
    "- If you're interested in how CNNs are used in image recognition, follow the classical <a href=\"https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\">MNIST handwritten digit recognition tutorial</a>\n",
    "- Play with it! http://scs.ryerson.ca/~aharley/vis/conv/flat.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Application of CNN in Text Classification\n",
    "- Assume $m$ samples, each of which is a sentence with $n$ words (short sentences can be padded)\n",
    "- **Embedding**: In each sentence, each word can be represented as its word vector of dimension $d$ (pretrained or to be trained)\n",
    "- **Convolution**: Apply filters to n-grams of different lengths (e.g. unigram, bigrams, ...). \n",
    "   - E.g. A filter can slide through every 2 words (bigram)\n",
    "   - So, the filter size (i.e. region size) can be $1\\text{x}d$ (unigram), $2\\text{x}d$ (bigram), $3\\text{x}d$ (trigram), ...\n",
    "- At pooling layer, 1-max pooling is applied to the result of each filter. Then all results after pooling are concatenated as the input to the output layer\n",
    "  - This is equivalent to select words or phrases that are **discriminative** with regard to the classification goal\n",
    "\n",
    "<img src='cnn_text_classification.png' width='70%'>\n",
    "\n",
    "*Illustration of a Convolutional Neural Network (CNN) architecture for sentence classification. Here we depict three filter region sizes: 2, 3 and 4, each of which has 2 filters. Every filter performs convolution on the sentence matrix and generates (variable-length) feature maps. Then 1-max pooling is performed over each map, i.e., the largest number from each feature map is recorded. Thus a univariate feature vector is generated from all six maps, and these 6 features are concatenated to form a feature vector for the penultimate layer. The final softmax layer then receives this feature vector as input and uses it to classify the sentence; here we assume binary classification and hence depict two possible output states. Source: Zhang, Y., & Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification.*\n",
    "\n",
    "- Questions:\n",
    "  - How many parameters in total in the convolution layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. How to deal with overfitting - Regularization & Dropout\n",
    "- Deep neural nets with a large number of parameters can be easily suffer from overfitting \n",
    "- Typical approaches to overcome overfitting\n",
    "    - Regularization\n",
    "    - Dropout (which is also a kind of regularization technique)\n",
    "- What is dropout?\n",
    "  - During training, randomly remove units in the hidden layer from the network. Update parameters as normal, leaving dropped-out units unchanged\n",
    "  - No dropout during testing \n",
    "  - Typically, each hidden unit is set to 0 with probability 0.5\n",
    "  <img src='dropout.png' width='60%'>\n",
    "  https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    " \n",
    "- Why dropout?\n",
    "  - Hidden units cannot co-adapt with other units since a unit may not always be present \n",
    "  - Sample data usually come with noise. Dropout constrains network adaptation to the data at training time\n",
    "  - After training, only very useful neurons are kept (have high weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Example: Use CNN for Sentiment Analysis (Single-Label Classification)\n",
    "- Dataset: IMDB review\n",
    "- 25,000 movie reviews, positive or negative \n",
    "- Benchmark performance is 80-90% with CNN (https://arxiv.org/abs/1408.5882)\n",
    "- We're going to create a CNN with the following:\n",
    "  - Word embedding trained as part of CNN\n",
    "  - filters in 3 sizes:\n",
    "      - unigram (Conv1D, kernel_size=1)\n",
    "      - bigram (Conv1D, kernel_size=2)\n",
    "      - trigram (Conv1D, kernel_size=3)\n",
    "  - Maxpooling for each convolution layer\n",
    "  - Dropout\n",
    "  <img src=\"cnn_model.png\" width='60%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.1: Load data\n",
    "\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "from gensim import corpora\n",
    "\n",
    "data=pd.read_csv(\"../../dataset/imdb_reviews.csv\", header=0, delimiter=\"\\t\")\n",
    "data.head()\n",
    "len(data)\n",
    "\n",
    "# if your computer does not have enough resource\n",
    "# reduce the dataset\n",
    "data=data.loc[0:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.2 Prepocessing data: Tokenize, pad sentences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "# set the maximum number of words to be used\n",
    "MAX_NB_WORDS=10000\n",
    "\n",
    "# set sentence/document length\n",
    "MAX_DOC_LEN=500\n",
    "\n",
    "# get a Keras tokenizer\n",
    "# https://keras.io/preprocessing/text/\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data[\"review\"])\n",
    "\n",
    "# convert each document to a list of word index as a sequence\n",
    "sequences = tokenizer.texts_to_sequences(data[\"review\"])\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_sequences = pad_sequences(sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the mapping between word and its index\n",
    "tokenizer.word_index['film']\n",
    "\n",
    "# get the count of each word\n",
    "tokenizer.word_counts['film']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                        padded_sequences, data['sentiment'],\\\n",
    "                        test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.3: Create CNN model\n",
    "\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "# The dimension for embedding\n",
    "EMBEDDING_DIM=100\n",
    "\n",
    "# define input layer, where a sentence represented as\n",
    "# 1 dimension array with integers\n",
    "main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                   dtype='int32', name='main_input')\n",
    "\n",
    "# define the embedding layer\n",
    "# input_dim is the size of all words +1\n",
    "# where 1 is for the padding symbol\n",
    "# output_dim is the word vector dimension\n",
    "# input_length is the max. length of a document\n",
    "# input to embedding layer is the \"main_input\" layer\n",
    "embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                    output_dim=EMBEDDING_DIM, \\\n",
    "                    input_length=MAX_DOC_LEN,\\\n",
    "                    name='embedding')(main_input)\n",
    "\n",
    "\n",
    "# define 1D convolution layer\n",
    "# 64 filters are used\n",
    "# a filter slides through each word (kernel_size=1)\n",
    "# input to this layer is the embedding layer\n",
    "conv1d_1= Conv1D(filters=64, kernel_size=1, \\\n",
    "                 name='conv_unigram',\\\n",
    "                 activation='relu')(embed_1)\n",
    "\n",
    "# define a 1-dimension MaxPooling \n",
    "# to take the output of the previous convolution layer\n",
    "# the convolution layer produce \n",
    "# MAX_DOC_LEN-1+1 values as ouput (???)\n",
    "pool_1 = MaxPooling1D(MAX_DOC_LEN-1+1, \\\n",
    "                      name='pool_unigram')(conv1d_1)\n",
    "\n",
    "# The pooling layer creates output \n",
    "# in the size of (# of sample, 1, 64)  \n",
    "# remove one dimension since the size is 1\n",
    "flat_1 = Flatten(name='flat_unigram')(pool_1)\n",
    "\n",
    "# following the same logic to define \n",
    "# filters for bigram\n",
    "conv1d_2= Conv1D(filters=64, kernel_size=2, \\\n",
    "                 name='conv_bigram',activation='relu')(embed_1)\n",
    "pool_2 = MaxPooling1D(MAX_DOC_LEN-2+1, name='pool_bigram')(conv1d_2)\n",
    "flat_2 = Flatten(name='flat_bigram')(pool_2)\n",
    "\n",
    "# filters for trigram\n",
    "conv1d_3= Conv1D(filters=64, kernel_size=3, \\\n",
    "                 name='conv_trigram',activation='relu')(embed_1)\n",
    "pool_3 = MaxPooling1D(MAX_DOC_LEN-3+1, name='pool_trigram')(conv1d_3)\n",
    "flat_3 = Flatten(name='flat_trigram')(pool_3)\n",
    "\n",
    "# Concatenate flattened output\n",
    "z=Concatenate(name='concate')([flat_1, flat_2, flat_3])\n",
    "\n",
    "# Create a dropout layer\n",
    "# In each iteration only 50% units are turned on\n",
    "drop_1=Dropout(rate=0.5, name='dropout')(z)\n",
    "\n",
    "# Create a dense layer\n",
    "dense_1 = Dense(192, activation='relu', name='dense')(drop_1)\n",
    "# Create the output layer\n",
    "preds = Dense(1, activation='sigmoid', name='output')(dense_1)\n",
    "\n",
    "# create the model with input layer\n",
    "# and the output layer\n",
    "model = Model(inputs=main_input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.4: Show model configuration\n",
    "\n",
    "model.summary()\n",
    "#model.get_config()\n",
    "#model.get_weights()\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file='cnn_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.4: Compile the model\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", \\\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.5: Fit the model\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHES = 10\n",
    "\n",
    "# fit the model and save fitting history to \"training\"\n",
    "training=model.fit(X_train, y_train, \\\n",
    "                   batch_size=BATCH_SIZE, \\\n",
    "                   epochs=NUM_EPOCHES,\\\n",
    "                   validation_data=[X_test, y_test], \\\n",
    "                   verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.6. Investigate the training process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# plot a figure with size 20x8\n",
    "\n",
    "# the fitting history is saved as dictionary\n",
    "# covert the dictionary to dataframe\n",
    "df=pd.DataFrame.from_dict(training.history)\n",
    "df.columns=[\"train_acc\", \"train_loss\", \\\n",
    "            \"val_acc\", \"val_loss\"]\n",
    "df.index.name='epoch'\n",
    "print(df)\n",
    "\n",
    "# plot training history\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,3));\n",
    "\n",
    "df[[\"train_acc\", \"val_acc\"]].plot(ax=axes[0]);\n",
    "df[[\"train_loss\", \"val_loss\"]].plot(ax=axes[1]);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from training history:\n",
    "- As training goes on, training accuracy/loss gets always better\n",
    "- Testing accuracy/loss gets better at the beginning, the gets worse\n",
    "- This indicates that model is **overfitted** and cannot be generalized after certain point\n",
    "- Thus, we should **stop training the model when testing accuracy/loss gets worse**. \n",
    "- This analysis can be used to determine hyperparameter **NUM_EPOCHES**\n",
    "- Fortunately, this can be done automatically by **\"Early Stopping\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.6: Use early stopping to find the best model\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# the file path to save best model\n",
    "BEST_MODEL_FILEPATH=\"best_model\"\n",
    "\n",
    "# define early stopping based on validation loss\n",
    "# if validation loss is not improved in \n",
    "# an iteration compared with the previous one, \n",
    "# stop training (i.e. patience=0). \n",
    "# mode='min' indicate the loss needs to decrease \n",
    "earlyStopping=EarlyStopping(monitor='val_loss', \\\n",
    "                            patience=0, verbose=2, \\\n",
    "                            mode='min')\n",
    "\n",
    "# define checkpoint to save best model\n",
    "# which has max. validation acc\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, \\\n",
    "                             monitor='val_acc', \\\n",
    "                             verbose=2, \\\n",
    "                             save_best_only=True, \\\n",
    "                             mode='max')\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model with earlystopping and checkpoint\n",
    "# as callbacks (functions that are executed as soon as \n",
    "# an asynchronous thread is completed)\n",
    "model.fit(X_train, y_train, \\\n",
    "          batch_size=BATCH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\n",
    "          validation_data=[X_test, y_test],\\\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.7: Load the best model\n",
    "\n",
    "# load the model using the save file\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "# predict\n",
    "pred=model.predict(X_test)\n",
    "print(pred[0:5])\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.8: Put Everything as a function\n",
    "\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "              \n",
    "def cnn_model(FILTER_SIZES, \\\n",
    "              # filter sizes as a list\n",
    "              MAX_NB_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_DOC_LEN, \\\n",
    "              # max words in a doc\n",
    "              EMBEDDING_DIM=200, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS=64, \\\n",
    "              # number of filters for all size\n",
    "              DROP_OUT=0.5, \\\n",
    "              # dropout rate\n",
    "              NUM_OUTPUT_UNITS=1, \\\n",
    "              # number of output units\n",
    "              NUM_DENSE_UNITS=100,\\\n",
    "              # number of units in dense layer\n",
    "              PRETRAINED_WORD_VECTOR=None,\\\n",
    "              # Whether to use pretrained word vectors\n",
    "              LAM=0.0):            \n",
    "              # regularization coefficient\n",
    "    \n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                       dtype='int32', name='main_input')\n",
    "    \n",
    "    if PRETRAINED_WORD_VECTOR is not None:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                        trainable=False,\\\n",
    "                        name='embedding')(main_input)\n",
    "    else:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        name='embedding')(main_input)\n",
    "    # add convolution-pooling-flat block\n",
    "    conv_blocks = []\n",
    "    for f in FILTER_SIZES:\n",
    "        conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                      activation='relu', name='conv_'+str(f))(embed_1)\n",
    "        conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "        conv = Flatten(name='flat_'+str(f))(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    \n",
    "    if len(conv_blocks)>1:\n",
    "        z=Concatenate(name='concate')(conv_blocks)\n",
    "    else:\n",
    "        z=conv_blocks[0]\n",
    "        \n",
    "    drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "    dense = Dense(NUM_DENSE_UNITS, activation='relu',\\\n",
    "                    kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "    preds = Dense(NUM_OUTPUT_UNITS, activation='sigmoid', name='output')(dense)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7. Use CNN for multi-label classification\n",
    "- In multi-label classification, a document can be classified into multiple classes\n",
    "- We can use **multiple ouput units**, each responsible for predicating one class\n",
    "- For multi-label classification ($K$ classes), do the following:\n",
    "    1. Represent the labels as **indication matrix**\n",
    "        - e.g. three classes ['econ','biz','tech'] in total, \n",
    "        - sample 1: 'eco' only -> [1, 0, 0]\n",
    "        - sample 2: ['eco','biz'] ->[1, 1, 0]\n",
    "    2. Accordingly, **set output layer to have K output units**\n",
    "        - each responsible for one class\n",
    "        - each unit gives the probabability of one class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example: Yahoo News Ranked Multilabel Learning dataset (http://research.yahoo.com)\n",
    "  - A subset is selected\n",
    "  - 4 classes, 6426 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.7.1: Load and process the data\n",
    "\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load the data\n",
    "data=json.load(open(\"../../dataset/ydata.json\",'rb'))\n",
    "#data=json.load(open(\"ydata.json\",'r'))\n",
    "\n",
    "\n",
    "# shuffle the data\n",
    "shuffle(data)\n",
    "\n",
    "# split into text and label\n",
    "text,labels=zip(*data)\n",
    "text=list(text)\n",
    "labels=list(labels)\n",
    "text[1]\n",
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.7.2: create indicator matrix for labels\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y=mlb.fit_transform(labels)\n",
    "# check size of indicator matrix\n",
    "Y.shape\n",
    "# check classes\n",
    "mlb.classes_\n",
    "# check # of samples in each class\n",
    "np.sum(Y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.7.3: Load and process the data\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "# get a Keras tokenizer\n",
    "\n",
    "MAX_NB_WORDS=8000\n",
    "# documents are quite long in the dataset\n",
    "MAX_DOC_LEN=1000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text)\n",
    "voc=tokenizer.word_index\n",
    "# convert each document to a list of word index as a sequence\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "# get the mapping between words to word index\n",
    "\n",
    "# pad all sequences into the same length (the longest)\n",
    "padded_sequences = pad_sequences(sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "\n",
    "#print(padded_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.7.4: Fit the model using the function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EMBEDDING_DIM=100\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "output_units_num=len(mlb.classes_)\n",
    "num_filters=64\n",
    "\n",
    "# set the dense units\n",
    "dense_units_num= num_filters*len(FILTER_SIZES)\n",
    "\n",
    "\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 20\n",
    "\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                padded_sequences, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, \\\n",
    "                NUM_FILTERS=num_filters,\\\n",
    "                NUM_OUTPUT_UNITS=output_units_num, \\\n",
    "                NUM_DENSE_UNITS=dense_units_num)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_loss', \\\n",
    "                             verbose=2, save_best_only=True, mode='min')\n",
    "    \n",
    "training=model.fit(X_train, Y_train, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[X_test, Y_test], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.7.5: predicate using the best model\n",
    "# calculate performance\n",
    "\n",
    "# load the best model\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "pred=model.predict(X_test)\n",
    "pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.7.6: Generate performance report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.  Use Pretrained Word Vectors\n",
    "- If **the size of labeled samples is small, it's better use pretrained word vectors** \n",
    "    - e.g. google or facebook pretrained word vectors\n",
    "    - or you can train word vectors using relevant context data using gensim\n",
    "- Procedure:\n",
    "    1. Obtain/train pretrained word vectors (see Section 4.1 and Exercise 4.1.1)\n",
    "    2. Look for the word vector for each word in the vocabulary and create **embedding matrix** where each row represents one word vector\n",
    "    3. Set embedding layer with the embedding matrix and set it not trainable.\n",
    "- With well-trained word vectors, often a small sample set can also achieve good performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.8.1: Load full yahoo news dataset\n",
    "# to train the word vector\n",
    "# note this data can be unlabeled. only text is used\n",
    "import json\n",
    "\n",
    "data=json.load(open(\"../../dataset/ydata_full.json\",'r'))\n",
    "text,labels=zip(*data)\n",
    "text=list(text)\n",
    "\n",
    "sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.8.2: Train word vector using \n",
    "# the large data set\n",
    "\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# print out tracking information\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "EMBEDDING_DIM=200\n",
    "# min_count: words with total frequency lower than this are ignored\n",
    "# size: the dimension of word vector\n",
    "# window: is the maximum distance \n",
    "#         between the current and predicted word \n",
    "#         within a sentence (i.e. the length of ngrams)\n",
    "# workers: # of parallel threads in training\n",
    "# for other parameters, check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "wv_model = word2vec.Word2Vec(sentences, min_count=5, \\\n",
    "                             size=EMBEDDING_DIM, window=5, workers=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get word vector for all words in the vocabulary\n",
    "# see reference at https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "\n",
    "EMBEDDING_DIM=200\n",
    "MAX_NB_WORDS=8000\n",
    "\n",
    "# tokenizer.word_index provides the mapping \n",
    "# between a word and word index for all words\n",
    "NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "\n",
    "# \"+1\" is for padding symbol\n",
    "embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # if word_index is above the max number of words, ignore it\n",
    "    if i >= NUM_WORDS:\n",
    "        continue\n",
    "    if word in wv_model.wv:\n",
    "        embedding_matrix[i]=wv_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.8.3: Fit model using pretrained word vectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EMBEDDING_DIM=200\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "output_units_num=len(mlb.classes_)\n",
    "\n",
    "#Number of filters for each size\n",
    "num_filters=64\n",
    "\n",
    "# set the dense units\n",
    "dense_units_num= num_filters*len(FILTER_SIZES)\n",
    "\n",
    "BTACH_SIZE = 32\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# Assume we only have 500 labeled data\n",
    "# split dataset into train (70%) and test sets (20%)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                padded_sequences[0:500], Y[0:500], \\\n",
    "                test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, \\\n",
    "                NUM_FILTERS=num_filters,\\\n",
    "                NUM_OUTPUT_UNITS=output_units_num, \\\n",
    "                NUM_DENSE_UNITS=dense_units_num,\\\n",
    "                PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=1, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_loss', \\\n",
    "                             verbose=2, save_best_only=True, mode='min')\n",
    "    \n",
    "training=model.fit(X_train, Y_train, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[X_test, Y_test], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.8.4: check model configuration\n",
    "# Note that parameters from embedding layer\n",
    "# is not trainable\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.8.5: Performance evaluation\n",
    "# Let's use samples[500:1000]\n",
    "# as an evaluation set\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "pred=model.predict(padded_sequences[500:1000])\n",
    "\n",
    "Y_pred=np.copy(pred)\n",
    "Y_pred=np.where(Y_pred>0.5,1,0)\n",
    "\n",
    "Y_pred[0:10]\n",
    "Y[500:510]\n",
    "\n",
    "print(classification_report(Y[500:1000], Y_pred, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Note that we only trained the model with **500 samples**\n",
    "- The performance is only slightly lower, compared with the one trained with 6000 samples\n",
    "- This shows that pre-trained word vectors can effectively improve the classification performance in the case of small labeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9. How to select hyperparameters?\n",
    "- Fitting a neural network is a very empirical process\n",
    "- See Section 3 of \"Practical Recommendations for Gradient-Based Training of Deep Architectures\" (https://arxiv.org/abs/1206.5533) for detailed discussion\n",
    "- The following is some useful techniques to set \n",
    "  - MAX_NB_WORDS: max number words to be included in word embedding\n",
    "    - Based on word frequency histogram to include words that appear at least $n$ times\n",
    "  - MAX_DOC_LEN: max length of documents\n",
    "    - Based on document length frequency histogram to include complete sentences as many as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.9.1 Set MAX_NB_WORDS to \n",
    "# include words that appear at least K times\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# get count of each word\n",
    "df=pd.DataFrame.from_dict(tokenizer.word_counts, orient=\"index\")\n",
    "df.columns=['freq']\n",
    "print(df.head())\n",
    "\n",
    "# get histogram of word count\n",
    "df=df['freq'].value_counts().reset_index()\n",
    "df.columns=['word_freq','count']\n",
    "\n",
    "# sort by word_freq\n",
    "df=df.sort_values(by='word_freq')\n",
    "\n",
    "# convert absolute counts to precentage\n",
    "df['percent']=df['count']/len(tokenizer.word_counts)\n",
    "# get cumulative percentage\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df.iloc[0:50].plot(x='word_freq', y='cumsum');\n",
    "\n",
    "plt.show();\n",
    "\n",
    "# if set min count for word to 10, \n",
    "# what % of words can be included?\n",
    "# how many words will be included?\n",
    "# This is the parameter MAX_NB_WORDS\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.9.2 Set MAX_DOC_LEN to \n",
    "# include complete sentences as many as possible\n",
    "\n",
    "# create a series based on the length of all sentences\n",
    "sen_len=pd.Series([len(item) for item in sequences])\n",
    "\n",
    "# create histogram of sentence length\n",
    "# the \"index\" is the sentence length\n",
    "# \"counts\" is the count of sentences at a length\n",
    "df=sen_len.value_counts().reset_index().sort_values(by='index')\n",
    "df.columns=['sent_length','counts']\n",
    "\n",
    "# sort by sentence length\n",
    "# get percentage and cumulative percentage\n",
    "\n",
    "df['percent']=df['counts']/len(sen_len)\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "print(df.head(3))\n",
    "\n",
    "# From the plot, 90% sentences have length<500\n",
    "# so it makes sense to set MAX_DOC_LEN=4~500 \n",
    "df.plot(x=\"sent_length\", y='cumsum');\n",
    "plt.show();\n",
    "\n",
    "# what will be the minimum sentence length\n",
    "# such that 99% of sentences will not be truncated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
