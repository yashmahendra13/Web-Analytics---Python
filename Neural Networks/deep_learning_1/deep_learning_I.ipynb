{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# <center>Deep Learning and Text Analytics</center>\n",
    "\n",
    "References:\n",
    "- General introduction\n",
    "     - https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/\n",
    "     - http://neuralnetworksanddeeplearning.com\n",
    "     - http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n",
    "- Word vector:\n",
    "     - https://code.google.com/archive/p/word2vec/\n",
    "- Keras tutorial\n",
    "     - https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "- CNN\n",
    "     - http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agenda\n",
    "- Introduction to neural networks\n",
    "- Word/Document Vectors (vector representation of words/phrases/paragraphs)\n",
    "- Convolutionary neural network (CNN)\n",
    "- Application of CNN in text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction neural networks\n",
    "- A neural network is a computational model inspired by the way biological neural networks in the human brain process information.\n",
    "- Neural networks have been widely applied in speech recognition, computer vision and text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Single Neuron\n",
    "\n",
    "<img src=\"single_neuron.png\" width=\"60%\">\n",
    "$$h_{W,b}(x)=f(w_1x_1+w_2x_2+w_3x_3+b)$$\n",
    "- Basic components:\n",
    "    - **input** ($X$): $[x_1, x_2, x_3]$\n",
    "    - **weight** ($W$): $[w_1, w_2, w_3]$\n",
    "    - **bias**: $b$\n",
    "    - **activation** function: $f$\n",
    "- Different activation functions:\n",
    "    - **Sigmoid** (logistic function): takes a real-valued input and squashes it to range [0,1]. $$f(z)=\\frac{1}{1+e^{-z}}$$, where $z=w_1x_1+w_2x_2+w_3x_3+b$\n",
    "    - Tanh (hyperbolic tangent): takes a real-valued input and squashes it to the range [-1, 1]. $$f(z)=tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
    "    - ReLU (Rectified Linear Unit): $$f(z)=max(0,z)$$   \n",
    "    - **Softmax** (normalized exponential function): a generalization of the logistic function. If $z=[z_1, z_2, ..., z_k]$ is a $k$-dimensional vector, $$f(z)_{j \\in k}=\\frac{e^{z_j}}{\\sum_{i=1}^k{e^{z_i}}}$$ \n",
    "     - $f(z)_{j} \\in [0,1]$\n",
    "     - $\\sum_{j \\in k} {f(z)_{j}} =1 $\n",
    "     - $f(z)_{j}$ is treated as the **probability** of component $j$, a probability distribution over $k$ different possible outcomes\n",
    "     - e.g. in multi-label classification, softmax gives a probability of each label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Neural Network Model\n",
    "- A neural network is composed of many simple neurons, so that the output of a neuron can be the input of another\n",
    "- The sample neural network model has 3 input nodes, 3 hidden units, and 1 output unit\n",
    "    - input layer: the leftmost layer\n",
    "    - outout layer: the rightmost layer (produce target, i.e. prediction, classification)\n",
    "    - bias units: indicated by \"+1\" node\n",
    "    - hidden layer: the middle layer of nodes\n",
    "<img src=\"neural_network.png\" width=\"60%\"/>\n",
    "\n",
    "- $W$, $x$, and $b$ usually represented as arrays (i.e. vectorized)\n",
    "   - $w_{ij}^{(l)}$: the weight associated with the link from unit $j$ in layer $l$ to unit $i$ in layer $l+1$\n",
    "   - $W^{(1)} \\in \\mathbb{R}^{3\\text{x}3}$, $W^{(2)} \\in \\mathbb{R}^{1\\text{x}3}$, $b^{(1)} \\in \\mathbb{R}^{3\\text{x}1}$, $b^{(2)} \\in \\mathbb{R}^{1\\text{x}1}$\n",
    "   - Note $W^{(l)}x$ is the dot product between $W^{(l)}$ and $x$, i.e. $W^{(l)} \\cdot x$\n",
    "   \n",
    "- If a neural network contains more than 1 hidden layer, it's called a **deep neural network** (**deep learning**)\n",
    "- Training a neural network model is to find $W$ and $b$ that optimize some **cost function**, given tranining samples (X,Y), where X and Y can be multi-dimensional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cost function\n",
    "- Training set: m samples denoted as $(X,Y)={(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})}$\n",
    "- A typical cost function: **mean_squared_error** \n",
    "  - Sum of square error: $J(W,b;x,y)=\\frac{1}{2}||h_{W,b}(x)-y||^2$\n",
    "  - Regularization (square of each weight, or L2): $\\sum_{i, j, l}(w_{ij}^{(l)})^2$. An important mechanism to prevent overfitting\n",
    "  - Cost function:\n",
    "$$J(W,b)=\\frac{1}{m}\\sum_i^m{(\\frac{1}{2}||h_{W,b}(x)-y||^2)}+ \\frac{\\lambda}{2}\\sum_{i, j, l}(w_{ij}^{(l)})^2$$, where $\\lambda$ is **regularization coefficient**\n",
    "- Other popular cost functions\n",
    "  - **Cross-entropy cost**\n",
    "      - Let's assume a single neuron with sigmoid activation function <img src='single_neuron.png' width=\"30%\" style=\"float: right;\">\n",
    "      - Let $\\widehat y=h_{W,b}(x)$, the prediction of true value $y$. $\\widehat y, y \\in [0,1]$. \n",
    "      - Then cross-entrophy cost is defined as: $$J=-\\frac{1}{m}\\sum_{i=1}^m{y_i\\ln{\\widehat y_i}+(1-y_i)\\ln{(1-\\widehat y_i)}}$$\n",
    "      - What makes cross-entropy a good cost function\n",
    "        - It's non-negative\n",
    "        - if the neuron's output $\\widehat y$ is close to the actual value $y$ (0 or 1) for all training inputs, then the cross-entropy will be close to zero\n",
    "- For comparison between \"Sum of Square error\" and \"Cross-entropy cost\", read http://neuralnetworksanddeeplearning.com/chap3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Gradient Descent\n",
    "- An optimization algorithm used to find the values of parameters ($W, b$) of a function ($J$) that minimizes a cost function ($J(W,b)$.\n",
    "- It is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm\n",
    "  <img src='gradient_descent.png' width='80%'>\n",
    "  resource: https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/\n",
    "- It uses derivatives of cost function to determine the direction to move the parameter values in order to get a lower cost on the next iteration\n",
    "- Procedure:\n",
    "    1. initialize $W$ with random values\n",
    "    2. given samples (X,Y) as inputs, calculate dirivatives of cost function with regard to every parameter $w_{ij}^{(l)}$, i.e. $\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$\n",
    "    3. update parameters by $(w_{ij}^{(l)})^{'}=w_{ij}^{(l)}-\\alpha*\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$, where $\\alpha$ is the learning rate\n",
    "    4. repeat steps 2-3 until $w_{ij}^{(l)}$ converges\n",
    "- **Learning rate $\\alpha$**\n",
    "  - It's critical to pick the right learning rate. Big $\\alpha$ or small $\\alpha$?\n",
    "  - $\\alpha$ may need to be adapted as learning unfolds\n",
    "- Challenges of Gradient Descent\n",
    "  - It is expensive to compute $\\frac{1}{m}\\sum_i^m{(\\frac{1}{2}||h_{W,b}(x_i)-y_i||^2)}$ for all samples in each round\n",
    "  - It is difficult to compute $\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$ if a neural netowrk has many layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Stochastic Gradient Descent\n",
    "- Estimate of cost function using a subset of randomly chosen training samples (mini-batch) instead of the entire training set\n",
    "- Procedure: \n",
    "  1. pick a randomly selected mini-batch, train with them and update $W, b$, \n",
    "  2. repeat step (1) with another randomly selected mini-batch until the training set is exhausted (i.e. complete an epoch), \n",
    "  3. start over with another epoch until $W, b$ converge\n",
    "- **Hyperparameters** (parameters that control the learning of $W, b$)\n",
    "    - **Batch size**: the size of samples selected for each iteration\n",
    "    - **Epoches**: One epoch means one complete pass through the whole training set. Ususally we need to use many epoches until $W, b$ converge\n",
    "    - e.g. if your sample size is 1000, and your batch size is 200, how many iterations are needed for one epoch?\n",
    "    - e.g. if you set # of epoches to 5, how many times in total you update $W, b$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Backpropagation Algorithm -- The efficient way to calcluate gradients (i.e. partial derivatives)\n",
    "\n",
    "Forward Propagation             |  Backprogation\n",
    ":-------------------------:|:-------------------------:\n",
    "![](forward-propagation.png)  |  ![](backpropagation.png)\n",
    "input signals are passing through each layer by multiplying the weights | backpropagate the error back to each layer proportional to perspective weights, and update the weights based on attributed errors in hope to correct the error\n",
    "- Algorithm:\n",
    "  1. perform a feedforward pass, computing the activations for layers L2, L3, ... and so on up to the output layer\n",
    "  2. for output layer $n$,<br> $\\delta^{(n)} = \\frac{\\partial}{\\partial z^{(n)}}\n",
    " J(W,b; x, y) = \\frac{\\partial}{\\partial z^{(n)}}\n",
    " \\frac{1}{2} \\left\\|y - h_{W,b}(x)\\right\\|^2 = - (y - a^{(n)}) \\cdot f'(z^{(n)})$\n",
    "  3. for $l=n-1, n-2, ..., n-3, ..., 2$,<br>\n",
    "  $ \\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\cdot f'(z^{(l)})$\n",
    "  4. Compute the desired partial derivatives, which are given as:<br>\n",
    "     $ \\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b; x, y) = a^{(l)}_j \\delta_i^{(l+1)}$ <br>\n",
    "$\\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; x, y) = \\delta_i^{(l+1)}$\n",
    "- Example: \n",
    "  - $\\delta^{(3)} = \\frac{\\partial}{\\partial z^{(3)}} J(W,b; x, y) = (a^{(3)} - y) \\cdot f'(z^{(3)})$\n",
    "\n",
    "  - $ \\delta^{(2)} = \\left((W^{(2)})^T \\delta^{(3)}\\right) \\cdot f'(z^{(2)})$\n",
    "  - $ \\frac{\\partial}{\\partial W_{12}^{(2)}} J(W,b; x, y) = a^{(2)}_2 \\delta_1^{(3)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Hyperparameters\n",
    "- Hyperparameters are parameters that control the learning of $w, b$ (our learning target)\n",
    "- Summary of hyperparameters:\n",
    "    - Network structure:\n",
    "      - number of hidden layers\n",
    "      - number of neurons of each layer\n",
    "      - activation fucntion of each layer\n",
    "    - Learning rate ($\\alpha$)\n",
    "    - regularization coeffiecient ($\\lambda$)\n",
    "    - mini-batch size\n",
    "    - epoches\n",
    "- For detailed explanation, watch: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/TBvb5/parameters-vs-hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Develop your First Neural Network Model with Keras\n",
    "- Keras: \n",
    "  - high-level library for neural network models\n",
    "  - It wraps the efficient numerical computation libraries Theano and TensorFlow \n",
    "- Why Keras:\n",
    "  - Simple to get started and keep going\n",
    "  - Written in python and higly modular; easy to expand\n",
    "  - Built-in modules for some sophisticated neural network models\n",
    "- Installation\n",
    "  - pip install keras (or pip install keras --upgrade if you already have it) to install the latest version (2.0.8)\n",
    "  - pip install theano (version 0.9.0)\n",
    "  - pip install tensorflow (version 1.3.0)\n",
    "  - pip install np-utils (version 0.5.3.4)\n",
    "- Basic procedure\n",
    "  1. Load data\n",
    "  2. Define model\n",
    "  3. Compile model\n",
    "  4. Fit model\n",
    "  5. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Basic Keras Modeling Constructs\n",
    "- Sequential model:  linear stack of layers\n",
    "- Layers\n",
    "  - Dense: in a dense layer, each neuron is connected to neurons in the next layer\n",
    "  - Embedding\n",
    "  - Convolution\n",
    "  - MaxPooling\n",
    "  - ...\n",
    "- Cost (loss) functions\n",
    "  - mean_squared_error\n",
    "  - binary_crossentropy\n",
    "  - categorical_crossentropy\n",
    "  - ...\n",
    "- Optimizer (i.e. optimization algorithm)\n",
    "  - SGD (Stochastic Gradient Descent): fixed learning rate in all iterations\n",
    "  - Adagrad: adapts the learning rate to the parameters, performing larger updates for infrequent, and smaller updates for frequent parameters\n",
    "  - Adam (Adaptive Moment Estimation): computes adaptive learning rates for each parameter.\n",
    "- Metrics\n",
    "  - accuracy: a ratio of correctly predicted samples to the total samples\n",
    "  - precision/recall/f1 through sklearn package\n",
    "  - Example:\n",
    "    - acc: (90+85)/200=87%\n",
    "    - prec: \n",
    "    - recall:\n",
    "\n",
    "|        | Predicted T        |   Predicted F  |\n",
    "|:----------|-------------------:|---------------:|\n",
    "|Actual T  |  90                | 10              |\n",
    "|Actual F  |  15                | 85              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Example\n",
    "- Example: build a simple neural network model to predict diabetes using \"Pima Indians onset of diabetes database\" at http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes\n",
    "  - Columns 1-8: variables \n",
    "  - Column 9: class variable, 0 or 1\n",
    "- A sequential model with 4 layers\n",
    "  - each node is a tensor, a function of multidimensional arrays\n",
    "    - Input (L1)\n",
    "    - L2 (hidden layer, dense)\n",
    "    - L3 (hidden layer, dense)\n",
    "    - Output (dense)\n",
    "  - the model is a tensor graph (computation graph)\n",
    "\n",
    "  <img src='model.png' width='20%'>\n",
    "  <div class=\"alert alert-block alert-info\">Training a deep learning model is a very empirical process. You may need to tune the hyperparameters in many iterations</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up interactive shell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: 8, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.1. Load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data=pd.read_csv(\"pima-indians-diabetes.csv\",header=None)\n",
    "data.head()\n",
    "\n",
    "data[8].value_counts()\n",
    "\n",
    "X=data.values[:,0:8]\n",
    "y=data.values[:,8]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\ymahendr\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.2. Create Model\n",
    "\n",
    "# sequential model is a linear stack of layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# in a dense layer which each neuron is connected to \n",
    "# each neuron in the next layer\n",
    "from keras.layers import Dense\n",
    "\n",
    "# import packages for L2 regularization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# set lambda (regularization coefficient)\n",
    "lam=0.01\n",
    "\n",
    "# create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add a dense layer with 12 neurons, 8 input variables\n",
    "# and rectifier activation function (relu)\n",
    "# and L2 regularization\n",
    "# how many parameters in this layer?\n",
    "model.add(Dense(12, input_dim=8, activation='relu', \\\n",
    "                kernel_regularizer=l2(lam), name='L2') )\n",
    "\n",
    "# add another hidden layer with 8 neurons\n",
    "model.add(Dense(8, activation='relu', \\\n",
    "                kernel_regularizer=l2(lam),name='L3') )\n",
    "\n",
    "# add the output layer with sigmoid activation function\n",
    "# to return probability\n",
    "model.add(Dense(1, activation='sigmoid', name='Output'))\n",
    "\n",
    "# compile the model using binary corss entropy cost function\n",
    "# adam optimizer and accuracy\n",
    "model.compile(loss='binary_crossentropy', \\\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "L2 (Dense)                   (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "L3 (Dense)                   (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ymahendr\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'Dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-69751bf1ba15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ymahendr\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \"\"\"\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ymahendr\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ymahendr\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# pydot raises a generic Exception here,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# so no specific class can be caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[0;32m     32\u001b[0m                           ' and graphviz for `pydotprint` to work.')\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "# Exercise 3.3. Check model configuration\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Show the model in a computation graph\n",
    "# it needs pydot and graphviz\n",
    "# don't worry if you don't have them installed\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 614 samples, validate on 154 samples\n",
      "Epoch 1/150\n",
      " - 2s - loss: 1.9401 - acc: 0.6384 - val_loss: 1.3583 - val_acc: 0.5130\n",
      "Epoch 2/150\n",
      " - 1s - loss: 1.1131 - acc: 0.5863 - val_loss: 1.0756 - val_acc: 0.5455\n",
      "Epoch 3/150\n",
      " - 1s - loss: 0.9913 - acc: 0.5847 - val_loss: 1.0256 - val_acc: 0.6234\n",
      "Epoch 4/150\n",
      " - 1s - loss: 0.9365 - acc: 0.5993 - val_loss: 0.9297 - val_acc: 0.5909\n",
      "Epoch 5/150\n",
      " - 2s - loss: 0.8954 - acc: 0.6254 - val_loss: 0.8907 - val_acc: 0.5714\n",
      "Epoch 6/150\n",
      " - 2s - loss: 0.8569 - acc: 0.6482 - val_loss: 0.8596 - val_acc: 0.5714\n",
      "Epoch 7/150\n",
      " - 2s - loss: 0.8303 - acc: 0.6401 - val_loss: 0.8443 - val_acc: 0.6364\n",
      "Epoch 8/150\n",
      " - 1s - loss: 0.8143 - acc: 0.6433 - val_loss: 0.8211 - val_acc: 0.6494\n",
      "Epoch 9/150\n",
      " - 1s - loss: 0.8117 - acc: 0.6368 - val_loss: 0.8259 - val_acc: 0.6169\n",
      "Epoch 10/150\n",
      " - 1s - loss: 0.7915 - acc: 0.6515 - val_loss: 0.7892 - val_acc: 0.6429\n",
      "Epoch 11/150\n",
      " - 2s - loss: 0.7738 - acc: 0.6482 - val_loss: 0.7984 - val_acc: 0.6364\n",
      "Epoch 12/150\n",
      " - 1s - loss: 0.7649 - acc: 0.6710 - val_loss: 0.8000 - val_acc: 0.6494\n",
      "Epoch 13/150\n",
      " - 1s - loss: 0.7736 - acc: 0.6531 - val_loss: 0.7897 - val_acc: 0.6364\n",
      "Epoch 14/150\n",
      " - 1s - loss: 0.7512 - acc: 0.6645 - val_loss: 0.7782 - val_acc: 0.6299\n",
      "Epoch 15/150\n",
      " - 1s - loss: 0.7461 - acc: 0.6466 - val_loss: 0.7897 - val_acc: 0.6623\n",
      "Epoch 16/150\n",
      " - 1s - loss: 0.7676 - acc: 0.6450 - val_loss: 0.8518 - val_acc: 0.6623\n",
      "Epoch 17/150\n",
      " - 1s - loss: 0.7573 - acc: 0.6564 - val_loss: 0.8010 - val_acc: 0.6818\n",
      "Epoch 18/150\n",
      " - 1s - loss: 0.7438 - acc: 0.6873 - val_loss: 0.7800 - val_acc: 0.6039\n",
      "Epoch 19/150\n",
      " - 1s - loss: 0.7366 - acc: 0.6840 - val_loss: 0.7812 - val_acc: 0.6753\n",
      "Epoch 20/150\n",
      " - 1s - loss: 0.7235 - acc: 0.6710 - val_loss: 0.7701 - val_acc: 0.6623\n",
      "Epoch 21/150\n",
      " - 1s - loss: 0.7226 - acc: 0.6743 - val_loss: 0.8030 - val_acc: 0.6753\n",
      "Epoch 22/150\n",
      " - 1s - loss: 0.7424 - acc: 0.6661 - val_loss: 0.7906 - val_acc: 0.6818\n",
      "Epoch 23/150\n",
      " - 1s - loss: 0.7229 - acc: 0.6547 - val_loss: 0.7789 - val_acc: 0.6948\n",
      "Epoch 24/150\n",
      " - 1s - loss: 0.7477 - acc: 0.6726 - val_loss: 0.7913 - val_acc: 0.6818\n",
      "Epoch 25/150\n",
      " - 1s - loss: 0.7141 - acc: 0.6857 - val_loss: 0.7530 - val_acc: 0.6948\n",
      "Epoch 26/150\n",
      " - 1s - loss: 0.7080 - acc: 0.6857 - val_loss: 0.7684 - val_acc: 0.6948\n",
      "Epoch 27/150\n",
      " - 1s - loss: 0.7077 - acc: 0.6775 - val_loss: 0.7387 - val_acc: 0.6688\n",
      "Epoch 28/150\n",
      " - 1s - loss: 0.7126 - acc: 0.6840 - val_loss: 0.7686 - val_acc: 0.6948\n",
      "Epoch 29/150\n",
      " - 1s - loss: 0.7104 - acc: 0.6840 - val_loss: 0.7441 - val_acc: 0.7078\n",
      "Epoch 30/150\n",
      " - 1s - loss: 0.7083 - acc: 0.6938 - val_loss: 0.7372 - val_acc: 0.6494\n",
      "Epoch 31/150\n",
      " - 1s - loss: 0.6918 - acc: 0.6987 - val_loss: 0.7409 - val_acc: 0.6948\n",
      "Epoch 32/150\n",
      " - 1s - loss: 0.7191 - acc: 0.6759 - val_loss: 0.7992 - val_acc: 0.6818\n",
      "Epoch 33/150\n",
      " - 1s - loss: 0.6970 - acc: 0.6971 - val_loss: 0.7124 - val_acc: 0.7208\n",
      "Epoch 34/150\n",
      " - 1s - loss: 0.6911 - acc: 0.6987 - val_loss: 0.7140 - val_acc: 0.7273\n",
      "Epoch 35/150\n",
      " - 1s - loss: 0.6752 - acc: 0.7036 - val_loss: 0.7281 - val_acc: 0.6883\n",
      "Epoch 36/150\n",
      " - 1s - loss: 0.6755 - acc: 0.7101 - val_loss: 0.7152 - val_acc: 0.7208\n",
      "Epoch 37/150\n",
      " - 1s - loss: 0.6794 - acc: 0.7231 - val_loss: 0.7679 - val_acc: 0.6623\n",
      "Epoch 38/150\n",
      " - 1s - loss: 0.6839 - acc: 0.7117 - val_loss: 0.7129 - val_acc: 0.7013\n",
      "Epoch 39/150\n",
      " - 1s - loss: 0.6841 - acc: 0.6971 - val_loss: 0.7095 - val_acc: 0.6948\n",
      "Epoch 40/150\n",
      " - 2s - loss: 0.6681 - acc: 0.7117 - val_loss: 0.7249 - val_acc: 0.6948\n",
      "Epoch 41/150\n",
      " - 2s - loss: 0.6728 - acc: 0.6971 - val_loss: 0.7274 - val_acc: 0.6753\n",
      "Epoch 42/150\n",
      " - 2s - loss: 0.6751 - acc: 0.7068 - val_loss: 0.7124 - val_acc: 0.6753\n",
      "Epoch 43/150\n",
      " - 1s - loss: 0.6770 - acc: 0.7215 - val_loss: 0.7146 - val_acc: 0.6948\n",
      "Epoch 44/150\n",
      " - 1s - loss: 0.6604 - acc: 0.7052 - val_loss: 0.6999 - val_acc: 0.7078\n",
      "Epoch 45/150\n",
      " - 2s - loss: 0.6946 - acc: 0.7068 - val_loss: 0.7029 - val_acc: 0.6948\n",
      "Epoch 46/150\n",
      " - 2s - loss: 0.6708 - acc: 0.7134 - val_loss: 0.6944 - val_acc: 0.7208\n",
      "Epoch 47/150\n",
      " - 1s - loss: 0.6626 - acc: 0.7003 - val_loss: 0.7146 - val_acc: 0.6234\n",
      "Epoch 48/150\n",
      " - 1s - loss: 0.6513 - acc: 0.7199 - val_loss: 0.7022 - val_acc: 0.6753\n",
      "Epoch 49/150\n",
      " - 1s - loss: 0.6655 - acc: 0.7117 - val_loss: 0.7005 - val_acc: 0.7013\n",
      "Epoch 50/150\n",
      " - 1s - loss: 0.6425 - acc: 0.7215 - val_loss: 0.7234 - val_acc: 0.6818\n",
      "Epoch 51/150\n",
      " - 2s - loss: 0.6513 - acc: 0.7150 - val_loss: 0.7037 - val_acc: 0.6948\n",
      "Epoch 52/150\n",
      " - 1s - loss: 0.6419 - acc: 0.7248 - val_loss: 0.7051 - val_acc: 0.6948\n",
      "Epoch 53/150\n",
      " - 1s - loss: 0.6459 - acc: 0.7003 - val_loss: 0.6909 - val_acc: 0.7078\n",
      "Epoch 54/150\n",
      " - 1s - loss: 0.6417 - acc: 0.7182 - val_loss: 0.6959 - val_acc: 0.6818\n",
      "Epoch 55/150\n",
      " - 1s - loss: 0.6417 - acc: 0.7166 - val_loss: 0.6986 - val_acc: 0.7078\n",
      "Epoch 56/150\n",
      " - 2s - loss: 0.6457 - acc: 0.7199 - val_loss: 0.7026 - val_acc: 0.6948\n",
      "Epoch 57/150\n",
      " - 1s - loss: 0.6493 - acc: 0.7215 - val_loss: 0.6944 - val_acc: 0.7078\n",
      "Epoch 58/150\n",
      " - 1s - loss: 0.6403 - acc: 0.7166 - val_loss: 0.7019 - val_acc: 0.6818\n",
      "Epoch 59/150\n",
      " - 1s - loss: 0.6416 - acc: 0.7150 - val_loss: 0.7097 - val_acc: 0.7013\n",
      "Epoch 60/150\n",
      " - 1s - loss: 0.6400 - acc: 0.6971 - val_loss: 0.7187 - val_acc: 0.6818\n",
      "Epoch 61/150\n",
      " - 1s - loss: 0.6423 - acc: 0.7345 - val_loss: 0.6832 - val_acc: 0.7208\n",
      "Epoch 62/150\n",
      " - 1s - loss: 0.6306 - acc: 0.7231 - val_loss: 0.7149 - val_acc: 0.7208\n",
      "Epoch 63/150\n",
      " - 1s - loss: 0.6395 - acc: 0.7182 - val_loss: 0.6790 - val_acc: 0.7143\n",
      "Epoch 64/150\n",
      " - 1s - loss: 0.6562 - acc: 0.7068 - val_loss: 0.6963 - val_acc: 0.6688\n",
      "Epoch 65/150\n",
      " - 1s - loss: 0.6769 - acc: 0.6906 - val_loss: 0.7367 - val_acc: 0.6883\n",
      "Epoch 66/150\n",
      " - 1s - loss: 0.6344 - acc: 0.7394 - val_loss: 0.6898 - val_acc: 0.7078\n",
      "Epoch 67/150\n",
      " - 1s - loss: 0.6422 - acc: 0.7085 - val_loss: 0.6988 - val_acc: 0.6818\n",
      "Epoch 68/150\n",
      " - 1s - loss: 0.6408 - acc: 0.7329 - val_loss: 0.7679 - val_acc: 0.6948\n",
      "Epoch 69/150\n",
      " - 1s - loss: 0.6710 - acc: 0.6857 - val_loss: 0.7624 - val_acc: 0.6688\n",
      "Epoch 70/150\n",
      " - 1s - loss: 0.6460 - acc: 0.7280 - val_loss: 0.6994 - val_acc: 0.6883\n",
      "Epoch 71/150\n",
      " - 1s - loss: 0.6378 - acc: 0.7231 - val_loss: 0.7236 - val_acc: 0.6753\n",
      "Epoch 72/150\n",
      " - 1s - loss: 0.6444 - acc: 0.7101 - val_loss: 0.7059 - val_acc: 0.7013\n",
      "Epoch 73/150\n",
      " - 1s - loss: 0.6244 - acc: 0.7313 - val_loss: 0.6848 - val_acc: 0.6948\n",
      "Epoch 74/150\n",
      " - 1s - loss: 0.6227 - acc: 0.7313 - val_loss: 0.6955 - val_acc: 0.6948\n",
      "Epoch 75/150\n",
      " - 1s - loss: 0.6449 - acc: 0.7280 - val_loss: 0.6715 - val_acc: 0.7273\n",
      "Epoch 76/150\n",
      " - 1s - loss: 0.6231 - acc: 0.7476 - val_loss: 0.6841 - val_acc: 0.7078\n",
      "Epoch 77/150\n",
      " - 1s - loss: 0.6435 - acc: 0.7215 - val_loss: 0.6766 - val_acc: 0.7143\n",
      "Epoch 78/150\n",
      " - 1s - loss: 0.6214 - acc: 0.7280 - val_loss: 0.6713 - val_acc: 0.7143\n",
      "Epoch 79/150\n",
      " - 1s - loss: 0.6187 - acc: 0.7443 - val_loss: 0.6753 - val_acc: 0.7208\n",
      "Epoch 80/150\n",
      " - 1s - loss: 0.6300 - acc: 0.7199 - val_loss: 0.6795 - val_acc: 0.7078\n",
      "Epoch 81/150\n",
      " - 1s - loss: 0.6304 - acc: 0.7443 - val_loss: 0.6844 - val_acc: 0.7143\n",
      "Epoch 82/150\n",
      " - 1s - loss: 0.6288 - acc: 0.7117 - val_loss: 0.6779 - val_acc: 0.6948\n",
      "Epoch 83/150\n",
      " - 1s - loss: 0.6464 - acc: 0.7313 - val_loss: 0.6723 - val_acc: 0.7273\n",
      "Epoch 84/150\n",
      " - 1s - loss: 0.6224 - acc: 0.7345 - val_loss: 0.6925 - val_acc: 0.7143\n",
      "Epoch 85/150\n",
      " - 1s - loss: 0.6139 - acc: 0.7329 - val_loss: 0.6672 - val_acc: 0.7078\n",
      "Epoch 86/150\n",
      " - 1s - loss: 0.6092 - acc: 0.7476 - val_loss: 0.6711 - val_acc: 0.7208\n",
      "Epoch 87/150\n",
      " - 1s - loss: 0.6243 - acc: 0.7378 - val_loss: 0.6719 - val_acc: 0.7273\n",
      "Epoch 88/150\n",
      " - 1s - loss: 0.6117 - acc: 0.7394 - val_loss: 0.6795 - val_acc: 0.7143\n",
      "Epoch 89/150\n",
      " - 1s - loss: 0.6137 - acc: 0.7345 - val_loss: 0.6662 - val_acc: 0.6948\n",
      "Epoch 90/150\n",
      " - 1s - loss: 0.6211 - acc: 0.7329 - val_loss: 0.6803 - val_acc: 0.7078\n",
      "Epoch 91/150\n",
      " - 1s - loss: 0.6060 - acc: 0.7410 - val_loss: 0.6754 - val_acc: 0.6948\n",
      "Epoch 92/150\n",
      " - 1s - loss: 0.6123 - acc: 0.7362 - val_loss: 0.6902 - val_acc: 0.7078\n",
      "Epoch 93/150\n",
      " - 1s - loss: 0.6201 - acc: 0.7264 - val_loss: 0.6805 - val_acc: 0.6948\n",
      "Epoch 94/150\n",
      " - 1s - loss: 0.6254 - acc: 0.7231 - val_loss: 0.6999 - val_acc: 0.6688\n",
      "Epoch 95/150\n",
      " - 1s - loss: 0.6376 - acc: 0.7117 - val_loss: 0.6727 - val_acc: 0.7013\n",
      "Epoch 96/150\n",
      " - 1s - loss: 0.6137 - acc: 0.7329 - val_loss: 0.6658 - val_acc: 0.7078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/150\n",
      " - 1s - loss: 0.6139 - acc: 0.7329 - val_loss: 0.7113 - val_acc: 0.7078\n",
      "Epoch 98/150\n",
      " - 1s - loss: 0.6092 - acc: 0.7296 - val_loss: 0.7164 - val_acc: 0.6948\n",
      "Epoch 99/150\n",
      " - 1s - loss: 0.6139 - acc: 0.7362 - val_loss: 0.6702 - val_acc: 0.7143\n",
      "Epoch 100/150\n",
      " - 1s - loss: 0.6163 - acc: 0.7362 - val_loss: 0.6683 - val_acc: 0.7078\n",
      "Epoch 101/150\n",
      " - 1s - loss: 0.6012 - acc: 0.7329 - val_loss: 0.7078 - val_acc: 0.7143\n",
      "Epoch 102/150\n",
      " - 1s - loss: 0.6148 - acc: 0.7427 - val_loss: 0.6946 - val_acc: 0.7013\n",
      "Epoch 103/150\n",
      " - 1s - loss: 0.5990 - acc: 0.7362 - val_loss: 0.6753 - val_acc: 0.7013\n",
      "Epoch 104/150\n",
      " - 1s - loss: 0.6098 - acc: 0.7410 - val_loss: 0.6858 - val_acc: 0.7208\n",
      "Epoch 105/150\n",
      " - 1s - loss: 0.6167 - acc: 0.7508 - val_loss: 0.6753 - val_acc: 0.7013\n",
      "Epoch 106/150\n",
      " - 1s - loss: 0.5965 - acc: 0.7492 - val_loss: 0.6644 - val_acc: 0.7078\n",
      "Epoch 107/150\n",
      " - 1s - loss: 0.6067 - acc: 0.7394 - val_loss: 0.6656 - val_acc: 0.7143\n",
      "Epoch 108/150\n",
      " - 1s - loss: 0.5979 - acc: 0.7492 - val_loss: 0.6651 - val_acc: 0.7078\n",
      "Epoch 109/150\n",
      " - 1s - loss: 0.5936 - acc: 0.7541 - val_loss: 0.6613 - val_acc: 0.6883\n",
      "Epoch 110/150\n",
      " - 1s - loss: 0.6003 - acc: 0.7345 - val_loss: 0.6667 - val_acc: 0.7208\n",
      "Epoch 111/150\n",
      " - 1s - loss: 0.6112 - acc: 0.7378 - val_loss: 0.6606 - val_acc: 0.7078\n",
      "Epoch 112/150\n",
      " - 1s - loss: 0.5954 - acc: 0.7443 - val_loss: 0.6776 - val_acc: 0.7208\n",
      "Epoch 113/150\n",
      " - 1s - loss: 0.6070 - acc: 0.7524 - val_loss: 0.7093 - val_acc: 0.6558\n",
      "Epoch 114/150\n",
      " - 1s - loss: 0.6366 - acc: 0.6987 - val_loss: 0.6582 - val_acc: 0.7468\n",
      "Epoch 115/150\n",
      " - 1s - loss: 0.5947 - acc: 0.7508 - val_loss: 0.6578 - val_acc: 0.7273\n",
      "Epoch 116/150\n",
      " - 1s - loss: 0.6034 - acc: 0.7459 - val_loss: 0.6571 - val_acc: 0.7208\n",
      "Epoch 117/150\n",
      " - 1s - loss: 0.6145 - acc: 0.7590 - val_loss: 0.6757 - val_acc: 0.7013\n",
      "Epoch 118/150\n",
      " - 1s - loss: 0.6194 - acc: 0.7296 - val_loss: 0.6776 - val_acc: 0.7208\n",
      "Epoch 119/150\n",
      " - 1s - loss: 0.6088 - acc: 0.7476 - val_loss: 0.6671 - val_acc: 0.6883\n",
      "Epoch 120/150\n",
      " - 1s - loss: 0.6036 - acc: 0.7459 - val_loss: 0.7377 - val_acc: 0.6818\n",
      "Epoch 121/150\n",
      " - 1s - loss: 0.6033 - acc: 0.7476 - val_loss: 0.6630 - val_acc: 0.6948\n",
      "Epoch 122/150\n",
      " - 1s - loss: 0.6271 - acc: 0.7296 - val_loss: 0.6975 - val_acc: 0.6948\n",
      "Epoch 123/150\n",
      " - 1s - loss: 0.6020 - acc: 0.7492 - val_loss: 0.6617 - val_acc: 0.7403\n",
      "Epoch 124/150\n",
      " - 1s - loss: 0.5960 - acc: 0.7476 - val_loss: 0.6822 - val_acc: 0.7013\n",
      "Epoch 125/150\n",
      " - 1s - loss: 0.5978 - acc: 0.7508 - val_loss: 0.6557 - val_acc: 0.7338\n",
      "Epoch 126/150\n",
      " - 1s - loss: 0.5847 - acc: 0.7508 - val_loss: 0.6587 - val_acc: 0.7078\n",
      "Epoch 127/150\n",
      " - 1s - loss: 0.5860 - acc: 0.7573 - val_loss: 0.6476 - val_acc: 0.7532\n",
      "Epoch 128/150\n",
      " - 1s - loss: 0.5870 - acc: 0.7606 - val_loss: 0.6705 - val_acc: 0.7143\n",
      "Epoch 129/150\n",
      " - 1s - loss: 0.5940 - acc: 0.7443 - val_loss: 0.6465 - val_acc: 0.7273\n",
      "Epoch 130/150\n",
      " - 1s - loss: 0.5957 - acc: 0.7476 - val_loss: 0.6612 - val_acc: 0.7143\n",
      "Epoch 131/150\n",
      " - 1s - loss: 0.5887 - acc: 0.7459 - val_loss: 0.6779 - val_acc: 0.6883\n",
      "Epoch 132/150\n",
      " - 1s - loss: 0.5888 - acc: 0.7524 - val_loss: 0.6608 - val_acc: 0.7338\n",
      "Epoch 133/150\n",
      " - 1s - loss: 0.5840 - acc: 0.7459 - val_loss: 0.6786 - val_acc: 0.6883\n",
      "Epoch 134/150\n",
      " - 1s - loss: 0.5876 - acc: 0.7524 - val_loss: 0.6570 - val_acc: 0.7338\n",
      "Epoch 135/150\n",
      " - 1s - loss: 0.5822 - acc: 0.7508 - val_loss: 0.6545 - val_acc: 0.7597\n",
      "Epoch 136/150\n",
      " - 1s - loss: 0.5850 - acc: 0.7541 - val_loss: 0.6466 - val_acc: 0.7403\n",
      "Epoch 137/150\n",
      " - 1s - loss: 0.5800 - acc: 0.7606 - val_loss: 0.6603 - val_acc: 0.7143\n",
      "Epoch 138/150\n",
      " - 1s - loss: 0.5824 - acc: 0.7427 - val_loss: 0.6487 - val_acc: 0.7143\n",
      "Epoch 139/150\n",
      " - 1s - loss: 0.5835 - acc: 0.7590 - val_loss: 0.7117 - val_acc: 0.6948\n",
      "Epoch 140/150\n",
      " - 2s - loss: 0.5967 - acc: 0.7508 - val_loss: 0.6512 - val_acc: 0.7143\n",
      "Epoch 141/150\n",
      " - 1s - loss: 0.5857 - acc: 0.7443 - val_loss: 0.6633 - val_acc: 0.7273\n",
      "Epoch 142/150\n",
      " - 1s - loss: 0.5778 - acc: 0.7492 - val_loss: 0.6539 - val_acc: 0.7143\n",
      "Epoch 143/150\n",
      " - 1s - loss: 0.5844 - acc: 0.7622 - val_loss: 0.6558 - val_acc: 0.7403\n",
      "Epoch 144/150\n",
      " - 1s - loss: 0.5869 - acc: 0.7573 - val_loss: 0.6598 - val_acc: 0.7273\n",
      "Epoch 145/150\n",
      " - 1s - loss: 0.5758 - acc: 0.7492 - val_loss: 0.6562 - val_acc: 0.7273\n",
      "Epoch 146/150\n",
      " - 1s - loss: 0.5763 - acc: 0.7524 - val_loss: 0.7061 - val_acc: 0.6883\n",
      "Epoch 147/150\n",
      " - 1s - loss: 0.5801 - acc: 0.7410 - val_loss: 0.6462 - val_acc: 0.7273\n",
      "Epoch 148/150\n",
      " - 1s - loss: 0.5750 - acc: 0.7590 - val_loss: 0.6557 - val_acc: 0.7013\n",
      "Epoch 149/150\n",
      " - 1s - loss: 0.6032 - acc: 0.7264 - val_loss: 0.6717 - val_acc: 0.7078\n",
      "Epoch 150/150\n",
      " - 1s - loss: 0.5970 - acc: 0.7264 - val_loss: 0.6690 - val_acc: 0.7078\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.4. Fit Model\n",
    "\n",
    "# train the model with min-batch of size 10, \n",
    "# 100 epoches (# how many iterations?)\n",
    "# Keep 20% samples for test\n",
    "# shuffle data before train-test split\n",
    "# set fitting history into variable \"training\"\n",
    "\n",
    "training=model.fit(X, y, validation_split=0.2, \\\n",
    "                   shuffle=True,epochs=150, \\\n",
    "                   batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 1s 875us/step\n",
      "\n",
      "acc: 75.00%\n",
      "[[ 0.64852411]\n",
      " [ 0.30080977]\n",
      " [ 0.94323528]\n",
      " [ 0.23109812]\n",
      " [ 0.82546616]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.76      0.80       500\n",
      "          1       0.62      0.74      0.67       268\n",
      "\n",
      "avg / total       0.77      0.75      0.75       768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.5. Get prediction and performance\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# evaluate the model using samples\n",
    "scores = model.evaluate(X, y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], \\\n",
    "                        scores[1]*100))\n",
    "\n",
    "# get prediction\n",
    "predicted=model.predict(X)\n",
    "print(predicted[0:5])\n",
    "# reshape the 2-dimension array to 1-dimension\n",
    "predicted=np.reshape(predicted, -1)\n",
    "\n",
    "# decide prediction to be 1 or 0 based probability\n",
    "predicted=np.where(predicted>0.5, 1, 0)\n",
    "\n",
    "# calculate performance report\n",
    "print(metrics.classification_report(y, predicted, \\\n",
    "                                    labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
