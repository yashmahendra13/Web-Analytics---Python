{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Web Scraping II</center>\n",
    "\n",
    "References: \n",
    "https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Different ways to access data on the web\n",
    " - Scrape HTML web pages (covered in Web Scraping I)\n",
    " - Download data file directly \n",
    "    * data files such as csv, txt\n",
    "    * pdf files\n",
    " - Access data through Application Programming Interface (API), e.g. The Movie DB, Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Scrape data through API (e.g. tweets)\n",
    "- Online content providers usually provide APIs for you to access data. Two types of APIs:\n",
    "   * Python packages: e.g. tweepy package from Twitter\n",
    "   * REST APIs: e.g. TMDB APIs (https://developers.themoviedb.org/3/getting-started)\n",
    "- You need to read documentation of APIs to figure out how to access data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Access tweet stream through tweepy package\n",
    "- **Steam**: transmitting or receiving data as a steady, continuous flow (the opposite is **batch**)\n",
    "\n",
    "- Event **Listener**(or Event Handler): \n",
    "  - A procedure or function that waits for an event to occur.\n",
    "  - Event examples: a user clicking or moving the mouse, pressing a key on the keyboard, an internal timer, or a tweet arriving.\n",
    "  - A listener is in effect a loop that is programmed to react to an input or signal.\n",
    "  \n",
    "- Twitter Terminology (https://support.twitter.com/articles/166337)\n",
    "  - **@{username}**: mentioning an accounts {username} in a tweet\n",
    "  - **\\#{topic}**: a hashtag indicates a keyword or topic.\n",
    "  - **follow**: Subscribing to a Twitter account \n",
    "  - **reply**: A response to another personâ€™s Tweet\n",
    "  - **Retweet (n.)**: A tweet that you forward to your followers\n",
    "  - **like (n.)**: indicates appreciating a tweet. \n",
    "  - **timeline**: A timeline is a real-time stream of tweets. Your Home timeline, for instance, is where you see all the Tweets shared by your friends and other people you follow.\n",
    "  - **Twitter emoji**: A Twitter emoji is a specific series of letters immediately preceded by the # sign which generates an icon on Twitter such as a national flag or another small image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.1.1 define a listener which listens to tweets in real time\n",
    "\n",
    "\n",
    "import tweepy\n",
    "# to install tweepy, use: pip install tweepy\n",
    "\n",
    "# import twitter authentication module\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# import tweepy steam module\n",
    "from tweepy import Stream\n",
    "\n",
    "# import stream listener\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "# import the python package to handle datetime\n",
    "import datetime\n",
    "\n",
    "# set your keys to access tweets \n",
    "consumer_key = 'your key here'\n",
    "consumer_secret = 'your key here'\n",
    "access_token = 'your key here'\n",
    "access_secret = 'your key here'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "# Customize a tweet event listener \n",
    "# inherited from StreamListener provided by tweepy\n",
    "# This listener reacts when a tweet arrives or an error happens\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, output_file, time_limit):\n",
    "        \n",
    "            # attribute to get listener start time\n",
    "            self.start_time=datetime.datetime.now()\n",
    "            \n",
    "            # attribute to set time limit for listening\n",
    "            self.time_limit=time_limit\n",
    "            \n",
    "            # attribute to set the output file\n",
    "            self.output_file=output_file\n",
    "            \n",
    "            # initiate superclass's constructor\n",
    "            StreamListener.__init__(self)\n",
    "    \n",
    "    # on_data is invoked when a tweet comes in\n",
    "    # overwrite this method inheritted from superclass\n",
    "    # when a tweet comes in, the tweet is passed as \"data\"\n",
    "    def on_data(self, data):\n",
    "        \n",
    "        # get running time\n",
    "        running_time=datetime.datetime.now()-self.start_time\n",
    "        print(running_time)\n",
    "        \n",
    "        # check if running time is over time_limit\n",
    "        if running_time.seconds/60.0<self.time_limit:\n",
    "            \n",
    "            # ***Exception handling*** \n",
    "            # If an error is encountered, \n",
    "            # a try block code execution is stopped and transferred\n",
    "            # down to the except block. \n",
    "            # If there is no error, \"except\" block is ignored\n",
    "            try:\n",
    "                # open file in \"append\" mode\n",
    "                with open(self.output_file, 'a') as f:\n",
    "                    # Write tweet string (in JSON format) into a file\n",
    "                    f.write(data)\n",
    "                    \n",
    "                    # continue listening\n",
    "                    return True\n",
    "                \n",
    "            # if an error is encountered\n",
    "            # print out the error message and continue listening\n",
    "            \n",
    "            except BaseException as e:\n",
    "                print(\"Error on_data:\" , str(e))\n",
    "                \n",
    "                # if return \"True\", the listener continues\n",
    "                return True\n",
    "            \n",
    "        else:  # timeout, return False to stop the listener\n",
    "            print(\"time out\")\n",
    "            return False\n",
    " \n",
    "    # on_error is invoked if there is anything wrong with the listener\n",
    "    # error status is passed to this method\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        # continue listening by \"return True\"\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.2 Collect tweets with specific topics within 2 minute\n",
    "\n",
    "# initiate an instance of MyListener \n",
    "tweet_listener=MyListener(output_file=\"python.txt\",time_limit=1)\n",
    "\n",
    "# start a staeam instance using authentication and the listener\n",
    "twitter_stream = Stream(auth, tweet_listener)\n",
    "# filtering tweets by topics\n",
    "twitter_stream.filter(track=['#blockchain', '#bitcoin','#crpytocurrency','#smartcontract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.3. Collect 1% sample of all tweets within 30 seconds\n",
    "\n",
    "tweet_listener=MyListener(output_file=\"tweets.txt\",time_limit=0.5)\n",
    "twitter_stream = Stream(auth, tweet_listener)\n",
    "twitter_stream.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.1.4. Collect nhistorical tweets for a topic\n",
    "\n",
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=500\n",
    "last_id = -1\n",
    "\n",
    "query='#blockchain'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. JSON (JavaScript Object Notation)\n",
    "\n",
    "### What is JSON\n",
    "- A lightweight data-interchange format\n",
    "- \"self-describing\" and easy to understand\n",
    "- the JSON format is text only \n",
    "- Language independent: can be read and used as a data format by any programming language\n",
    "\n",
    "###  JSON Syntax Rules\n",
    "JSON syntax is derived from JavaScript object notation syntax:\n",
    "- Data is in name/value pairs\n",
    "- Data is separated by commas\n",
    "- Curly braces hold objects\n",
    "- Square brackets hold arrays\n",
    "\n",
    "### A JSON file can be easily loaded into a dictionary or a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4.1. Read/write JSON \n",
    "import json\n",
    "tweets=[]\n",
    "\n",
    "with open('python1.txt', 'r') as f:\n",
    "    # each line is one tweet string in JSON format\n",
    "    for line in f: \n",
    "        \n",
    "        # load a string in JSON format as Python dictionary\n",
    "        tweet = json.loads(line) \n",
    "              \n",
    "        tweets.append(tweet)\n",
    "\n",
    "# write the whole list back to JSON\n",
    "json.dump(tweets, open(\"all_tweets.json\",'w'))\n",
    "\n",
    "# to load the whole list\n",
    "# pay attention to json.load and json.loads\n",
    "tweets=json.load(open(\"all_tweets.json\",'r'))\n",
    "\n",
    "# open \"all_tweets.json\" and \"python.txt\" to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2. Investigating a tweet\n",
    "\n",
    "# A tweet is a dictionary\n",
    "# Some values are dictionaries too!\n",
    "# for details, check https://dev.twitter.com/overview/api/tweets\n",
    "\n",
    "print(\"# of tweets:\", len(tweets))\n",
    "first_tweet=tweets[0]\n",
    "\n",
    "print(\"\\nprint out first tweet nicely:\")\n",
    "print(json.dumps(first_tweet, indent=4))   \n",
    "\n",
    "# note the difference between \"json.dumps()\" and \"json.dump()\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.3. Investigating attributes of a tweet\n",
    "\n",
    "print(\"tweet text:\", first_tweet[\"text\"] )\n",
    "# get all hashtags (i.e. topics) in this tweet\n",
    "      \n",
    "topics=[hashtag[\"text\"] for hashtag in first_tweet[\"entities\"][\"hashtags\"]]\n",
    "print(\"\\ntopics:\", topics)\n",
    "\n",
    "# get all user_mentions in this tweet\n",
    "user_mentions=[user_mention[\"screen_name\"] for user_mention in first_tweet[\"entities\"][\"user_mentions\"]]\n",
    "print(\"\\nusers mentioned:\", user_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.4. count tweets per topic\n",
    "\n",
    "# get the number of tweets for each topic as a dictionary\n",
    "count_per_topic={}\n",
    "\n",
    "# loop through each tweet in the list\n",
    "for t in tweets:\n",
    "    # check if \"entities\" exist and \"hashtags\" exist in \"entities\"\n",
    "    if \"entities\" in t and \"hashtags\" in t[\"entities\"]:\n",
    "        # get all topics as a set (unique topics)\n",
    "        topics=set([hashtag[\"text\"].lower() for hashtag in t[\"entities\"][\"hashtags\"]])\n",
    "        \n",
    "        for topic in topics:\n",
    "            topic=topic.lower()\n",
    "            if topic in count_per_topic:\n",
    "                count_per_topic[topic]+=1\n",
    "            else:\n",
    "                count_per_topic[topic]=1\n",
    "        \n",
    "print(count_per_topic)\n",
    "\n",
    "# convert the dictionary into a list of tuples (topic, count)\n",
    "topic_count_list=count_per_topic.items()\n",
    "\n",
    "# sort the list by vcount in descending order\n",
    "sorted_topics=sorted(topic_count_list, key=lambda item:-item[1])\n",
    "print(sorted_topics)\n",
    "\n",
    "# get top 20 topics\n",
    "top_20_topics=sorted_topics[0:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.5. Visualize data\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df=pd.DataFrame.from_dict(count_per_topic, orient='index')\n",
    "df.columns=['count']\n",
    "df\n",
    "\n",
    "df.sort_values(by='count', ascending=False).iloc[0:10].plot.bar();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.6. Word Cloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\")\n",
    "wordcloud.generate_from_frequencies(frequencies=count_per_topic)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scrape data by REST APIs (TMDB)\n",
    "- A REST API is a web service that uses HTTP requests to GET, PUT, POST and DELETE data\n",
    "- requests package can be used for REST API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.1. search movies by name\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "title='finding dory'\n",
    "\n",
    "# Search API: http://api.themoviedb.org/3/search/movie\n",
    "# has two parameters: query string and api_key\n",
    "# For the get methods, parameters are attached to API URL after a \"?\"\n",
    "# Parameters are separated by \"&\"\n",
    "\n",
    "# to test, apply for an api key and use the key ere\n",
    "url=\"http://api.themoviedb.org/3/search/movie?query=\"+title+\"&api_key=<your api key>\"\n",
    "\n",
    "# invoke the API \n",
    "r = requests.get(url)\n",
    "\n",
    "# if the API call returns a successful response\n",
    "if r.status_code==200:\n",
    "    \n",
    "    # This API call returns a json object\n",
    "    # r.json() gives the json object\n",
    "    \n",
    "    if \"results\" in r.json():\n",
    "        results=r.json()[\"results\"]\n",
    "        print (json.dumps(results, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scrape pdf files\n",
    "- A number of Python libraries can handle PDFs (https://www.binpress.com/tutorial/manipulating-pdfs-with-python/167)\n",
    "- Some popular libraries:\n",
    "  * pyPDF2: support both python2 and python3\n",
    "    * To install, issue: pip install pypdf2\n",
    "  * PDFMiner: only support python2\n",
    "  * PDFQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1. downloading and parse pdf files \n",
    "\n",
    "import requests\n",
    "from PyPDF2 import  PdfFileReader\n",
    "\n",
    "# First download the pdf file\n",
    "pages=[]\n",
    "r=requests.get(\"http://ciese.org/media/live/curriculum/airproj/docs/aqiworksheet.pdf\")\n",
    "if r.status_code==200:\n",
    "    # write the content to a local file\n",
    "    with open(\"some_pdf.pdf\",\"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# Parse the pdf content. It may need further clean-up depending on the content\n",
    "pdfreader = PdfFileReader(open(\"some_pdf.pdf\", \"rb\"))\n",
    "\n",
    "#loop through each page of the pdf file\n",
    "for i in range(pdfreader.getNumPages()):\n",
    "    # get each page\n",
    "    page=pdfreader.getPage(i)\n",
    "    # extract text\n",
    "    page_content=page.extractText()\n",
    "    \n",
    "    # append the text to the list\n",
    "    pages.append(page_content)\n",
    "    \n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
