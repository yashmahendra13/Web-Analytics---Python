{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Topic Modeling </center>\n",
    "\n",
    "- This lecture is created based on \n",
    "    * Blei. D. (2012), \"Probabilistic Topic Models\", http://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf\n",
    "    * Topic Modeling with Scikit Learn: https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "- Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives.\n",
    "  - Discover the hidden themes that pervade the collection.\n",
    "  - Annotate the documents according to those themes.\n",
    "  - Use annotations to organize, summarize, and search the texts.\n",
    "- Formal Definition\n",
    "<img src='topic_modeling.png' width='70%'>\n",
    "  - **Topics**: each topic is a **distribution over words** \n",
    "    - e.g. for topic \"Gentics\", $p('gene'~|~'Genetics')~=~0.04$, $p('dna'~|~'Genetics')=0.02$\n",
    "    - $K$ topics $\\theta_1, \\theta_2, ..., \\theta_K$, $N$ words $w_1, w_2, ..., w_N$ in corpus, we need to know  $p(w_i|\\theta_j)$ for $i \\in N$ and $j\\in K$\n",
    "  - **Document ($d$)**: a **mixture of topics**\n",
    "    - e.g. for above document $d$, $p('Genetics'~|~d)=0.5$, $p('LifeScience'~|~d)=0.15$, ...\n",
    "    - In general, given document $d$ and topic $\\theta_j$, we need to know $p(\\theta_j~|~d)$, i.e. **topic proportion**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Language Model\n",
    "- Definition: given a corpus with $M$ documents, $N$ words, $K$ topics, a model contains the following probabilities:\n",
    "  - topic probability distribution in corpus: <br>$p(\\theta_j)$ for $j \\in K$, $\\sum_{j\\in K}{p(\\theta_j)}=1$\n",
    "  - topic distribution per document $d$ (document assignment): <br>$p(\\theta_j~|~d)$, $\\sum_{j\\in K}{p(\\theta_j~|~d)}=1$ \n",
    "  - word distribution per topic (why do we need to know it?): <br> $p(w_i~|~\\theta_j)$ for $i \\in N$ and $j\\in K$, $\\sum_{i\\in N}{p(w_i~|~\\theta_j)}=1$ \n",
    "  <img src='language_model.png' width='30%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to estimate these probabilities?\n",
    "### 3.1. Supervised learning - Naive Bayes\n",
    "- Topic probability: \n",
    "$$ p(\\theta_j) = \\frac{\\text{documents in topic } j} {\\text{total documents}}$$  \n",
    "- Word distribution per topic: \n",
    "$$ p(w_i~|~\\theta_j)= \\frac{\\text{count of word } w_i \\text{ in topic } j} {\\text{total word count in documents of topic }j}$$  \n",
    "- Topic distribution per document: \n",
    "$$ \\begin{array}{l}\n",
    " p(\\theta_j~|~d) = \\frac{p(d|\\theta_j) * p(\\theta_j)}{p(d)} \\text{         # Bayesian rule}\\\\\n",
    " C_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}{p(d~|~\\theta)*p(\\theta)} \\text{         # maximum a posteriori}\\\\\n",
    "    C_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}{p(w_1,w_2, ...,w_N~|~\\theta)*p(\\theta)} \\\\\n",
    "    C_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}({\\prod_{i \\in N} {p(w_i~|~\\theta)})*p(\\theta)}  \\textit{ # independence assumption}\n",
    "  \\end{array}$$  \n",
    "- Naive Bayes model is also a kind of language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Generative Model for Unsupervised learning \n",
    "- We don't have labeled data; we only observe the documents\n",
    "- We **cannot** estimate $p(\\theta_j)$ and $p(w_i~|~\\theta_j)$ as above\n",
    "- Instead, we use a **generative model** that describes how a document $d$ was created\n",
    "  1. decide on document length $N$, e.g. 100 words\n",
    "  2. decide on topic mixture (i.e. $p(\\theta_j~|~d)$), e.g. 70% about genetics and 30% about life science, ...\n",
    "  3. for each of the N words,\n",
    "     - 3.1. choose a topic from the topic mixture, e.g. \"genetics\"\n",
    "     - 3.2. choose a word from based on the probabilities of words in the topic (i.e. $p(w_i~|~\\theta_j)$), e.g. \"gene\"\n",
    "     - At the end, you may get a document such as \"gene dna life ...\"\n",
    "- We assume all documents in the dataset were generated following this process. Then we infer these probabilities from samples such that these probabilities have the maximum likelihood to generate the samples\n",
    "- Probabilities $p(w_i~|~\\theta_j)$ and $p(\\theta_j~|~d)$ are **hidden structures** to be discovered, a.k.a **latent variables**\n",
    "<img src='latent_structure.png' width='70%'>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Dirichlet Allocation (LDA)\n",
    "- A generative model which generates a document $d$ as follows:\n",
    "  1. Choose document length $N$ ∼ Poisson(ξ).\n",
    "  2. Choose topic mixture $\\theta$ ~ Dir(α).\n",
    "  3. For each of the $N$ words $w_n$:\n",
    "     - (a) Choose a topic assignment $z_n$ ∼ Multinomial(θ).\n",
    "     - (b) Choose a word $w_n$ from the topic, $z_n$ ∼ Multinomial($\\beta_{z_n}$), where $\\beta_{z_n}$ is the word distribution for assigned topic $z_n$, i.e. $p(w_n~|~z_n)$     \n",
    "- A few distributions\n",
    "  - Poisson(ξ) : a given number of events occurring in a fixed interval of time/space with rate ξ independently of the time/space since the last event\n",
    "  - Multinomial(θ) & Multinomial($\\beta$): \n",
    "    - suppose X is a vector which represents n draws of a random variable with three possible outcomes (i.e. words), say A, B, C. \n",
    "    - e.g. when n=10, an example draw of X could be x = [4,4,2], i.e., A occured 4 times, B 4 times, and C 2 times \n",
    "    - assume three outcomes have probability θ={$\\beta_A$, $\\beta_B$,$\\beta_C$} respectively (i.e. 0.5,0.3,0.2)\n",
    "    - the multinomial distribution describes the prob. mass distribution of X, $$ multinomial(X=[4,4,2]) = \\frac{10!}{4!4!2!}\\beta_A^{4}\\beta_B^{4}\\beta_C^{2}$$ \n",
    "  - Dir(α) : is a probability distribution with parameter $α, e.g. \\{α_1,α_2,α_3\\}$ to generate $θ, e.g. \\{ θ_1,θ_2,θ_3\\}$. For details of Dirichlet function, check videos e.g. https://www.youtube.com/watch?v=nfBNOWv1pgE \n",
    "  <img src='dirichlet.svg'>\n",
    "    - Dirichlet distribution is conjugate to the multinomial.\n",
    "    - Given a multinomial observation, the posterior distribution of θ is a Dirichlet.\n",
    "    - In LDA, usually $α_1=α_2=α_3=...=\\frac{1}{K}$\n",
    "- Common techniques to estimate these probabilities are EM (Expectation-Maximization), Gibbs Sampling (See Blei's paper for details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Estimate parameters by Gibbs Sampling\n",
    "- General ideas of Gibbs sampling\n",
    "  - In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult (from *Wikipedia*).\n",
    "  - e.g.  by bayes inference, $p(x, y)=p(x|y)~p(y)=p(y|x)~p(x)$. If it's difficult to determine the nature of $p(x, y)$, while it is easy to sample from $p(y|x)$  and $p(x|y)$, then we can obtain a sequence of observations approximating $p(x, y)$ by sampling $p(y|x)$ and $p(x|y)$. \n",
    "  - Let's assume:\n",
    "    * $p(y|x)$ \\~ $N(x,\\sigma_1)$\n",
    "    * $p(x|y)$ \\~ $N(y,\\sigma_2)$ \n",
    "    * Now we need to have 100 $(x,y)$ samples so that we can understand the nature of $p(x,y)$\n",
    "  - The Gibbs sampler proceeds as follows:\n",
    "    1. set $x$ and $y$ to some initial starting values \n",
    "    2. Replace $x$ by a new value obtained by sampling $x|y$, then update $y$ by sampling from $y|x$, \n",
    "    3. Repeat step 2 until the estimates of parameters converge.\n",
    "- Outline of Gibbs Sampling in LDA\n",
    "  1. Go through each document and randomly assign each word in the document to one of the $K$ topics, i.e. $p(z_i=j~|~w_i, d)$, where $z_i=j$ denotes word $i$ is assigned to topic $j$. \n",
    "  2. Calculate the following\n",
    "     - $p(w_i~|~\\theta_j)$ (word-topic matrix): calculated as the count of each word being assigned to each topic. \n",
    "     - $p(\\theta_j~|~d)$ (document-topic matrix): the number of words assigned to each topic for each document \n",
    "  3. Update $p(z_i=j~|~w_i, d)$ using $p(w_i~|~\\theta_j)$ and $p(\\theta_j~|~d)$ as follows:\n",
    "     - For each document $d$, and each word $w_i$, reassign a new topic $j$ to $w_i$, where we choose topic $j$ is sampled from [1, 2, ..., $K$] with a probability ${\\propto}~ (w_i~|~\\theta_j) * p(\\theta_j~|~d)$ (conditional posterior distribution)\n",
    "     - i.e. Given $w_i$, $d$, $z_{-i}$ (topic assignment of all other words in $d$), $$p(z_i=j~|~w_i, z_{-i}, d)~{\\propto}~p(w_i~|~z_i=j, z_{-i}, d) * p(z_i=j~|~z_{-i}, d)~{\\propto}~(w_i~|~\\theta_j) * p(\\theta_j~|~d)$$ \n",
    "  4. Repeat steps 2-3 until $p(w_i~|~\\theta_j)$ and  $p(\\theta_j~|~d)$ converge.\n",
    "- For more implementation details, check http://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate Topic Model - Perplexity\n",
    "- For a single document $d$ with $N_d$ words $\\{w_1, w_2, ..., w_{N_d}\\}$, denoted as $\\textbf{W}_d$\n",
    "$$\n",
    "  perplexity(d)= exp({H(d)}),  \n",
    "  H(d) = - \\frac{ln (p(\\textbf{W}_d))}{N_d}  \n",
    "$$\n",
    "- $p(\\textbf{W}_d)$, the probability of seeing a document $d$, can be calculated based on:\n",
    "   - word distribution per topic, i.e. $p(w_i~|~\\theta_j$, and \n",
    "   - topic mixture, i.e. $p(\\theta_j~|~d)$\n",
    "- For a test set of D with M documents\n",
    "$$ perplexity(d)= exp({H(D)}), H(D) = - \\frac{\\sum_{d \\in D} {ln   (p(\\textbf{W}_d)})}{\\sum_{d \\in D}{N_d}} $$\n",
    "- Intutition: \n",
    "  - A lower perplexity score indicates better generalization performance\n",
    "  - Minimizing H(d) is equivalent to maximizing log likelihood\n",
    "- To evaluate a topic model, calcuate perplexity on **testing dataset** (i.e. evaluating how generaalized the model is)\n",
    "- Note: if you have some labeled data, you should also conduct **external evaluation**, i.e. \n",
    "  - map each topic to a labeled class, \n",
    "  - compute precision/recall from the labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiement with LDA\n",
    "- A few libraries available for LDA: gensim, lda, sklearn\n",
    "- We use sklearn here since it has a good text preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christy bareijsza the event guru for weddings and high profile events christy bareijsza president of the red carpet events known to many as the event guru is the expert of experts in the field of event planning and weddings continues to produce stellar events and provide memorable experiences in new york ny vocus february christy bareijsza president of the red carpet events known to many as the event guru is the expert of experts in the field of event planning and weddings continues to produce stellar events and provide memorable experiences in christy bareijsza the event guru and a go to source for creatively produced events from soirees to corporate events to one of a kind special weddings that are different from the norm continue to expand business this year with her many years in the business christy really personalizes events and exceeds client expectations by taking it to another level and achieving the best results for them the red carpet events is a national full service planning and production company that designs manages and produces from start to finish high end soirees with celebrity entertainment to elaborate weddings from the wedding planning that includes the finest catering photography flowers cakes entertainment beauty services custom gift sources and more clients include american express departures magazine bombardier flexjet canyon ranch spa resort aetn international to brown forman to name a few our events calendar was full of wonderful special events and weddings in and with we re scheduling elaborate events and weddings throughout the year my team is just as passionate as i am in what we do and we have many referrals from our clients because quite frankly we re visionaries in producing specialized events to tailor made weddings states christy bareijsza cmm cmp president of the red carpet events the red carpet events has been a leader in the industry since boasting a client list that includes numerous corporations venues fashion sporting hotels and personalities alike the company has been added to the preferred vendor list of numerous fortune corporations and is part of a national network of premiere suppliers the red carpet events were ranked in and in out of the top event planning companies for new jersey biz magazine the red carpet events provides clients the highest quality events at the most competitive prices offering only the best client services available from corporate events social events event design to venue consultation for more information on the red carpet events visit www theredcarpetevents com for more information or to arrange an interview with christy bareijsza cmm cmp president of the red carpet events please contact pr specialist tamara york at tamara at tamarayorkpr dot com tamara york pr tamara york e mail information\n",
      "\n",
      "['money', 'investment-&-company-information', 'investment']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.1. Load 20 news group data\n",
    "import json\n",
    "from numpy.random import shuffle\n",
    "\n",
    "data=json.load(open('ydata.json','r'))\n",
    "\n",
    "# shuffle the data\n",
    "shuffle(data)\n",
    "\n",
    "text,label=zip(*data)\n",
    "text=list(text)\n",
    "label=list(label)\n",
    "\n",
    "print(text[0])\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaron', 'abandoned', 'abc', 'ability', 'able', 'abroad', 'absolutely', 'abuse', 'academy', 'accelerate']\n",
      "(6426, 4273)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.2. Preprocessing - Create Term Frequency Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# LDA can only use raw term counts for LDA \n",
    "tf_vectorizer = CountVectorizer(max_df=0.90, \\\n",
    "                min_df=50, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(text)\n",
    "\n",
    "# each feature is a word (bag of words)\n",
    "# get_feature_names() gives all words\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "print(tf_feature_names[0:10])\n",
    "print(tf.shape)\n",
    "\n",
    "# split dataset into train (90%) and test sets (10%)\n",
    "# the test sets will be used to evaluate proplexity of topic modeling\n",
    "X_train, X_test = train_test_split(\\\n",
    "                tf, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1f7a4cad4a7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m lda = LatentDirichletAllocation(n_components=num_topics,                                 max_iter=10,verbose=1,\n\u001b[0;32m     13\u001b[0m                                 \u001b[0mevaluate_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                                 random_state=0).fit(X_train)\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Exercise 5.3. Train LDA model\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "num_topics = 4\n",
    "\n",
    "# Run LDA. For details, check\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.perplexity\n",
    "\n",
    "# max_iter control the number of iterations \n",
    "# evaluate_every determines how often the perplexity is calculated\n",
    "# n_jobs is the number of parallel threads\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                max_iter=10,verbose=1,\n",
    "                                evaluate_every=1, n_jobs=1,\n",
    "                                random_state=0).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('company', 4097.7329422535395), ('million', 3646.8402730069529), ('year', 3288.3787458435909), ('percent', 2876.7522469879341), ('market', 2788.796747882815), ('income', 2601.2254544436028), ('quarter', 2442.1550698475585), ('financial', 2150.8632480496358), ('investment', 1972.7158580685209), ('share', 1868.8986225478534), ('billion', 1858.4448347071636), ('net', 1848.2449786148386), ('shares', 1752.020563130174), ('statements', 1750.806256200445), ('sales', 1656.5528101873608), ('fund', 1641.0306277938391), ('investors', 1613.3812554948117), ('term', 1610.8207360051265), ('stock', 1586.6853991484536), ('cash', 1446.5841525969404)]\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "[('com', 5408.1223276165229), ('new', 3558.1679675525525), ('company', 3260.2581225149183), ('www', 3096.7643615433763), ('information', 3064.9179944050807), ('business', 3042.8157692704162), ('services', 2306.0752692873534), ('technology', 1934.0792089888478), ('products', 1922.8351509529091), ('http', 1703.975928210135), ('service', 1674.9007020906711), ('industry', 1626.3183662386307), ('online', 1552.2855741784022), ('customers', 1526.5285092193055), ('solutions', 1511.5316931060588), ('said', 1454.1271122064386), ('visit', 1426.8975140391155), ('available', 1361.625727395407), ('based', 1356.8933558114763), ('world', 1302.2678919151231)]\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "[('yahoo', 4046.9085759374125), ('finance', 3133.6841060707156), ('com', 2778.2535023957603), ('news', 2453.8211298626265), ('market', 2246.9766773452634), ('free', 1953.5986172122837), ('data', 1925.3187453206181), ('information', 1793.8562178301438), ('stock', 1772.1383260267489), ('quotes', 1622.9346212991754), ('provided', 1588.5573253241378), ('stocks', 1508.2691786346659), ('sign', 1447.3797967474684), ('investing', 1430.7469648307051), ('new', 1328.9607921130209), ('company', 1308.0144640108138), ('search', 1299.7041917592453), ('zacks', 1278.261595418862), ('ap', 1217.9926600464703), ('financial', 1031.2941449753014)]\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "[('said', 11414.2631654876), ('comment', 4144.8949239292251), ('police', 3592.3061438572936), ('year', 2675.2755347503962), ('news', 2483.9560994582903), ('court', 2346.9020318555226), ('people', 2246.1159464800844), ('sign', 1955.7747061621162), ('users', 1946.8735427420168), ('new', 1908.2665093214634), ('report', 1834.1915658979931), ('years', 1818.7297056121452), ('rate', 1813.3394252366991), ('time', 1705.549456263062), ('told', 1661.4741469483988), ('state', 1561.3832325320077), ('case', 1530.3679385420642), ('man', 1491.408339877803), ('video', 1490.7214627859541), ('home', 1466.3587757164744)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.4. Check topic and word distribution per topic\n",
    "\n",
    "num_top_words=20\n",
    "\n",
    "# lda.components_ returns a KxN matrix\n",
    "# for word distribution in each topic.\n",
    "# Each row consists of \n",
    "# probability (counts) of each word in the feature space\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print (\"Topic %d:\" % (topic_idx))\n",
    "    # print out top 20 words per topic \n",
    "    words=[(tf_feature_names[i],topic[i]) for i in topic.argsort()[::-1][0:num_top_words]]\n",
    "    print(words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-db8e439197d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_top_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "\n",
    "num_top_words=50\n",
    "f, axarr = plt.subplots(2, 2, figsize=(8, 8));\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    # create a dataframe with two columns (word, weight) for each topic\n",
    "    \n",
    "    # create a word:count dictionary\n",
    "    f={tf_feature_names[i]:topic[i] for i in topic.argsort()[::-1][0:num_top_words]}\n",
    "    \n",
    "    # generate wordcloud in subplots\n",
    "    wordcloud = WordCloud(width=480, height=450, margin=0, background_color=\"black\");\n",
    "    _ = wordcloud.generate_from_frequencies(frequencies=f);\n",
    "    \n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].imshow(wordcloud, interpolation=\"bilinear\");\n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].set_title(\"Topic: \"+str(topic_idx));\n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.87837194e-01   7.73974678e-04   1.10598931e-01   7.89900328e-04]\n",
      " [  2.00541510e-03   1.53537902e-01   1.90334916e-03   8.42553334e-01]\n",
      " [  9.86093589e-02   8.90852147e-01   9.63476458e-03   9.03729710e-04]\n",
      " [  5.26863526e-03   5.32221412e-03   5.24349697e-03   9.84165654e-01]\n",
      " [  3.72123102e-01   5.73472824e-01   5.34907281e-02   9.13346123e-04]]\n",
      "[[1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.5. Assign documents to topic\n",
    "import numpy as np\n",
    "\n",
    "# Generate topic assignment of each document\n",
    "topic_assign=lda.transform(X_train)\n",
    "\n",
    "print(topic_assign[0:5])\n",
    "\n",
    "# set a probability threshold\n",
    "# the threshold determines precision/recall\n",
    "prob_threshold=0.25\n",
    "\n",
    "topics=np.copy(topic_assign)\n",
    "topics=np.where(topics>=prob_threshold, 1, 0)\n",
    "print(topics[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991.85103494\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.6. Evaluate topic models by perplexity of test data\n",
    "\n",
    "perplexity=lda.perplexity(X_test)\n",
    "print(perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Find the number of topics ($K$)\n",
    "- There are no \"golden\" rules to find K.\n",
    "- Perplexity may be one way for you to find the number of topics\n",
    "    - Typically, the best number of topics should be around the **lowest perplexity**\n",
    "- However, in practice, a few factors need to be considered:\n",
    "  - It is usually difficult for human to understand or visulaize a big number of topics (I'd suggest K < 20)\n",
    "  - You may manually scan the data to figure out possible topics in the data, but these topics may not be correlated with the hidden structure discovered by LDA\n",
    "  - Usually, after LDA, we need manually inspect each discovered topic, merge or trim topics to get semantically coherent but distinguishable topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.7. How to find the best number of topics?\n",
    "# Vary variable num_topics, e.g. set it to 2, 3, 5, ...\n",
    "# For each value, train LDA model, calculate perplexity on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "result=[]\n",
    "for num_topics in range(2,25):\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                learning_method='online', \\\n",
    "                                max_iter=10,verbose=0, n_jobs=1,\n",
    "                                random_state=0).fit(X_train)\n",
    "    p=lda.perplexity(X_test)\n",
    "    result.append([num_topics,p])\n",
    "    print(num_topics, p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result, columns=[\"K\", \"Perlexity\"]).plot.line(x='K',y=\"Perlexity\");\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "- NMF is similar to LDA with different mathematical underpinning. It decompose document-term matrix into the product of \n",
    "  - feature matrix (i.e. word distribution per topic) and \n",
    "  - weight matrix (i.e. topic mixture per document) \n",
    "- NMF is very efficient for small matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aapl', 'aaron', 'abandoned', 'abandons', 'abc', 'ability', 'able', 'aboard']\n",
      "(6426, 7805)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6.1. NMF transformation\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=20, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(text)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(tfidf_feature_names[0:10])\n",
    "print(tfidf.shape)\n",
    "\n",
    "X_train, X_test = train_test_split(tfidf, test_size=0.1, random_state=0)\n",
    "\n",
    "no_topics = 4\n",
    "\n",
    "# Run NMF\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "# init: ‘nndsvd’: Nonnegative Double Singular Value Decomposition (NNDSVD) better for sparseness\n",
    "# alpha: regularization\n",
    "\n",
    "nmf = NMF(n_components=no_topics, \\\n",
    "          random_state=1, alpha=0.01, init='nndsvd').fit(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('company', 0.74036508210084306), ('com', 0.63777863023414172), ('percent', 0.62984743226844031), ('million', 0.46593381109382698), ('business', 0.42974376451146079), ('quarter', 0.42073227747783232), ('sales', 0.40826369894643905), ('market', 0.39767646660976869), ('www', 0.39126724986127481), ('new', 0.37201271539444963), ('products', 0.35614132184657016), ('companies', 0.34451163490097247), ('statements', 0.34184716339966126), ('information', 0.32066671931600393), ('year', 0.31944377175250938), ('stock', 0.31750365325711311), ('billion', 0.31356519403250505), ('shares', 0.30937166806306488), ('services', 0.30747885805925862), ('technology', 0.30111970735989957)]\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "[('said', 1.0995160128244907), ('police', 0.86509937373860779), ('court', 0.406698709998705), ('man', 0.38034306820055769), ('comment', 0.3627650065908789), ('year', 0.31221340823666105), ('old', 0.30529146170312871), ('told', 0.30255268378416278), ('people', 0.2893640273118217), ('say', 0.27185408246071524), ('case', 0.26697278776134292), ('authorities', 0.26233169359624314), ('arrested', 0.26150379168911497), ('state', 0.2500646369058887), ('charges', 0.24614752958428815), ('prison', 0.24314612500871677), ('charged', 0.23732679590428712), ('death', 0.23460255653339787), ('drug', 0.23291372980308903), ('attorney', 0.22818436103727585)]\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "[('yahoo', 1.0601063266252249), ('finance', 0.87851541768556174), ('quotes', 0.57821180039574849), ('comment', 0.56173609112214784), ('data', 0.48851323399014673), ('sign', 0.48380945708155038), ('provided', 0.45254090604781355), ('news', 0.42663319115055931), ('market', 0.41861155523815069), ('investing', 0.41740350797214804), ('search', 0.33921269863290426), ('ap', 0.32752376790508858), ('stocks', 0.32588898222025092), ('com', 0.31178778221211811), ('free', 0.29623025979730466), ('stock', 0.28403812286417945), ('zacks', 0.26337692110606231), ('providers', 0.26331294967520802), ('information', 0.26016212776304926), ('rate', 0.25493561642040186)]\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "[('income', 1.7215226915480015), ('municipal', 0.71635238227260456), ('trust', 0.69101368006081787), ('stanley', 0.58855451184513086), ('morgan', 0.57578376810767917), ('tax', 0.44742149876662113), ('dividend', 0.44186183794473033), ('monthly', 0.43065880912591137), ('investment', 0.42829610097482967), ('declared', 0.41661370339764175), ('date', 0.38361528882553547), ('payable', 0.28048730159596297), ('share', 0.27060272904184968), ('fund', 0.2570383078818379), ('distribution', 0.25243403589170221), ('diversified', 0.24163970220637732), ('record', 0.23179407396112145), ('exempt', 0.22349113718398181), ('financial', 0.19786505830301521), ('objective', 0.19354704409548409)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6.2. Get topic words\n",
    "\n",
    "num_top_words=20\n",
    "\n",
    "# lda.components_ returns a KxN matrix\n",
    "# for word distribution in each topic.\n",
    "# Each row consists of \n",
    "# probability (counts) of each word in the feature space\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print (\"Topic %d:\" % (topic_idx))\n",
    "    # print out top 20 words per topic \n",
    "    words=[(tfidf_feature_names[i],topic[i]) \\\n",
    "           for i in topic.argsort()[::-1][0:num_top_words]]\n",
    "    print(words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9164b6bee8dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_top_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "\n",
    "num_top_words=50\n",
    "f, axarr = plt.subplots(2, 2, figsize=(8, 8));\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    # create a dataframe with two columns (word, weight) for each topic\n",
    "    \n",
    "    # create a word:count dictionary\n",
    "    f={tfidf_feature_names[i]:topic[i] \\\n",
    "       for i in topic.argsort()[::-1][0:num_top_words]}\n",
    "    \n",
    "    # generate wordcloud in subplots\n",
    "    wordcloud = WordCloud(width=480, height=450, margin=0, background_color=\"black\");\n",
    "    _ = wordcloud.generate_from_frequencies(frequencies=f);\n",
    "    \n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].imshow(wordcloud, interpolation=\"bilinear\");\n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].set_title(\"Topic: \"+str(topic_idx));\n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03209943  0.0010115   0.01927236  0.04786201]\n",
      " [ 0.01096943  0.04006777  0.          0.        ]\n",
      " [ 0.05673413  0.00226616  0.          0.        ]\n",
      " [ 0.          0.04880261  0.          0.        ]\n",
      " [ 0.04317846  0.00462181  0.00996765  0.00639977]]\n",
      "[[ 0.32020884  0.01009021  0.19225201  0.47744895]\n",
      " [ 0.21493002  0.78506998  0.          0.        ]\n",
      " [ 0.96159073  0.03840927  0.          0.        ]\n",
      " [ 0.          1.          0.          0.        ]\n",
      " [ 0.67290026  0.07202708  0.15533756  0.0997351 ]]\n",
      "[[1 0 0 1]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6.3. Assign document to topics\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "topic_assign=nmf.transform(X_train)\n",
    "\n",
    "print(topic_assign[0:5])\n",
    "\n",
    "topics=normalize(np.copy(topic_assign), axis=1, norm='l1')\n",
    "print(topics[0:5])\n",
    "\n",
    "prob_threshold=0.25\n",
    "\n",
    "topics=np.where(topics>prob_threshold, 1, 0)\n",
    "print(topics[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
