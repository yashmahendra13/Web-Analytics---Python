{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "              \n",
    "def cnn_model(FILTER_SIZES, \\\n",
    "              # filter sizes as a list\n",
    "              MAX_NB_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_DOC_LEN, \\\n",
    "              # max words in a doc\n",
    "              EMBEDDING_DIM=200, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS=64, \\\n",
    "              # number of filters for all size\n",
    "              DROP_OUT=0.5, \\\n",
    "              # dropout rate\n",
    "              NUM_OUTPUT_UNITS=1, \\\n",
    "              # number of output units\n",
    "              NUM_DENSE_UNITS=100,\\\n",
    "              # number of units in dense layer\n",
    "              PRETRAINED_WORD_VECTOR=None,\\\n",
    "              # Whether to use pretrained word vectors\n",
    "              LAM=0.0):            \n",
    "              # regularization coefficient\n",
    "    \n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                       dtype='int32', name='main_input')\n",
    "    \n",
    "    if PRETRAINED_WORD_VECTOR is not None:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                        trainable=False,\\\n",
    "                        name='embedding')(main_input)\n",
    "    else:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        name='embedding')(main_input)\n",
    "    # add convolution-pooling-flat block\n",
    "    conv_blocks = []\n",
    "    for f in FILTER_SIZES:\n",
    "        conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                      activation='relu', name='conv_'+str(f))(embed_1)\n",
    "        conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "        conv = Flatten(name='flat_'+str(f))(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    \n",
    "    if len(conv_blocks)>1:\n",
    "        z=Concatenate(name='concate')(conv_blocks)\n",
    "    else:\n",
    "        z=conv_blocks[0]\n",
    "        \n",
    "    drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "    dense = Dense(NUM_DENSE_UNITS, activation='relu',\\\n",
    "                    kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "    preds = Dense(NUM_OUTPUT_UNITS, activation='sigmoid', name='output')(dense)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  sentiment                                             review\n",
      "0  5814_8          1  With all this stuff going down at the moment w...\n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3  3630_4          0  It must be assumed that those who praised this...\n",
      "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk,string\n",
    "\n",
    "# Load data\n",
    "data=pd.read_csv('../dataset/imdb_reviews.csv', header=0, sep=\"\\t\")\n",
    "#data.columns=['label','text']\n",
    "#data['label']=data['label'].apply(lambda x:x-1)\n",
    "print(data.head())\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data=shuffle(data)\n",
    "train_set=data.iloc[0:1000]\n",
    "test_set=data.iloc[1000:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set[[\"review\",\"sentiment\"]].to_csv(\"train.csv\", header=True, index=False)\n",
    "test_set[[\"review\",\"sentiment\"]].to_csv(\"test.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "# set the maximum number of words to be used\n",
    "MAX_NB_WORDS=12000\n",
    "\n",
    "# set sentence/document length\n",
    "MAX_DOC_LEN=1000\n",
    "\n",
    "# get a Keras tokenizer\n",
    "# https://keras.io/preprocessing/text/\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train_set[\"review\"])\n",
    "\n",
    "# convert each document to a list of word index as a sequence\n",
    "sequences = tokenizer.texts_to_sequences(train_set[\"review\"])\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_sequences = pad_sequences(sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_set[\"review\"])\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_test_sequences = pad_sequences(test_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18759"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.54500, saving model to best_model\n",
      "19s - loss: 0.6971 - acc: 0.4900 - val_loss: 0.6774 - val_acc: 0.5450\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc improved from 0.54500 to 0.70000, saving model to best_model\n",
      "19s - loss: 0.6330 - acc: 0.7020 - val_loss: 0.6183 - val_acc: 0.7000\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.70000 to 0.76500, saving model to best_model\n",
      "21s - loss: 0.4889 - acc: 0.8030 - val_loss: 0.5373 - val_acc: 0.7650\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc improved from 0.76500 to 0.78500, saving model to best_model\n",
      "20s - loss: 0.2835 - acc: 0.9010 - val_loss: 0.4478 - val_acc: 0.7850\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc did not improve\n",
      "19s - loss: 0.1405 - acc: 0.9590 - val_loss: 0.4319 - val_acc: 0.7700\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc did not improve\n",
      "19s - loss: 0.0650 - acc: 0.9860 - val_loss: 0.4356 - val_acc: 0.7650\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc did not improve\n",
      "19s - loss: 0.0477 - acc: 0.9890 - val_loss: 0.5143 - val_acc: 0.7450\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "output_units_num=1\n",
    "num_filters=64\n",
    "\n",
    "# set the dense units\n",
    "dense_units_num= num_filters*len(FILTER_SIZES)\n",
    "\n",
    "BTACH_SIZE = 32\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "BEST_MODEL_FILEPATH='best_model'\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# Assume we only have 500 labeled data\n",
    "# split dataset into train (70%) and test sets (20%)\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, \\\n",
    "                EMBEDDING_DIM=300,\\\n",
    "                NUM_OUTPUT_UNITS=output_units_num, \\\n",
    "                NUM_FILTERS=num_filters,\\\n",
    "                NUM_DENSE_UNITS=dense_units_num)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=1, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training=model.fit(padded_sequences, train_set[\"sentiment\"], \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[padded_test_sequences, test_set[\"sentiment\"]], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "acc: 83.50%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.86      0.84       102\n",
      "          1       0.85      0.81      0.83        98\n",
      "\n",
      "avg / total       0.84      0.83      0.83       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "# predict\n",
    "pred=model.predict(padded_test_sequences)\n",
    "pred=np.where(pred>0.5,1,0)\n",
    "print(pred[0:5])\n",
    "# evaluate the model\n",
    "scores = model.evaluate(padded_test_sequences, test_set[\"sentiment\"], verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "print(classification_report(test_set[\"sentiment\"], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in data[\"review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-11 23:18:06,700 : INFO : collecting all words and their counts\n",
      "2018-04-11 23:18:06,701 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-04-11 23:18:07,136 : INFO : PROGRESS: at sentence #10000, processed 2374499 words, keeping 74911 word types\n",
      "2018-04-11 23:18:07,620 : INFO : PROGRESS: at sentence #20000, processed 4702314 words, keeping 105743 word types\n",
      "2018-04-11 23:18:07,839 : INFO : collected 118815 word types from a corpus of 5872604 raw words and 25000 sentences\n",
      "2018-04-11 23:18:07,840 : INFO : Loading a fresh vocabulary\n",
      "2018-04-11 23:18:07,955 : INFO : min_count=5 retains 34361 unique words (28% of original 118815, drops 84454)\n",
      "2018-04-11 23:18:07,956 : INFO : min_count=5 leaves 5744544 word corpus (97% of original 5872604, drops 128060)\n",
      "2018-04-11 23:18:08,075 : INFO : deleting the raw counts dictionary of 118815 items\n",
      "2018-04-11 23:18:08,077 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2018-04-11 23:18:08,078 : INFO : downsampling leaves estimated 4372651 word corpus (76.1% of prior 5744544)\n",
      "2018-04-11 23:18:08,079 : INFO : estimated required memory for 34361 words and 200 dimensions: 72158100 bytes\n",
      "2018-04-11 23:18:08,196 : INFO : resetting layer weights\n",
      "2018-04-11 23:18:08,633 : INFO : training model with 4 workers on 34361 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-04-11 23:18:09,638 : INFO : PROGRESS: at 5.91% examples, 1297788 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:10,642 : INFO : PROGRESS: at 10.37% examples, 1133869 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:11,650 : INFO : PROGRESS: at 15.75% examples, 1143265 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:12,652 : INFO : PROGRESS: at 20.51% examples, 1116998 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:13,654 : INFO : PROGRESS: at 26.72% examples, 1164908 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:14,654 : INFO : PROGRESS: at 33.12% examples, 1205742 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:15,670 : INFO : PROGRESS: at 38.32% examples, 1190924 words/s, in_qsize 6, out_qsize 1\n",
      "2018-04-11 23:18:16,674 : INFO : PROGRESS: at 43.29% examples, 1177550 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:17,684 : INFO : PROGRESS: at 48.20% examples, 1166382 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:18,687 : INFO : PROGRESS: at 52.89% examples, 1151799 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:19,688 : INFO : PROGRESS: at 57.72% examples, 1141677 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:20,693 : INFO : PROGRESS: at 63.94% examples, 1159817 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:21,695 : INFO : PROGRESS: at 70.42% examples, 1179460 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:22,695 : INFO : PROGRESS: at 76.96% examples, 1196512 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:23,698 : INFO : PROGRESS: at 83.36% examples, 1209994 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:24,703 : INFO : PROGRESS: at 89.78% examples, 1222662 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-11 23:18:25,706 : INFO : PROGRESS: at 95.88% examples, 1228370 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-11 23:18:26,482 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-11 23:18:26,485 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-11 23:18:26,492 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-11 23:18:26,498 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-11 23:18:26,499 : INFO : training on 29363020 raw words (21863480 effective words) took 17.9s, 1223915 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# print out tracking information\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "EMBEDDING_DIM=200\n",
    "wv_model = word2vec.Word2Vec(sentences, min_count=5, \\\n",
    "                             size=EMBEDDING_DIM, window=5, workers=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-11 23:24:58,354 : INFO : loading projection weights from /Users/rliu/bluemix/bia660/GoogleNews-vectors-negative300.bin\n",
      "2018-04-11 23:25:34,084 : INFO : loaded (3000000, 300) matrix from /Users/rliu/bluemix/bia660/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "wv_model = gensim.models.KeyedVectors.\\\n",
    "load_word2vec_format('/Users/rliu/bluemix/bia660/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "\n",
    "#model.wv.most_similar(positive=['women','king'], negative='man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "# set the maximum number of words to be used\n",
    "MAX_NB_WORDS=12000\n",
    "\n",
    "# set sentence/document length\n",
    "MAX_DOC_LEN=1000\n",
    "\n",
    "# get a Keras tokenizer\n",
    "# https://keras.io/preprocessing/text/\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train_set[\"review\"])\n",
    "\n",
    "# convert each document to a list of word index as a sequence\n",
    "sequences = tokenizer.texts_to_sequences(train_set[\"review\"])\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_sequences = pad_sequences(sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_set[\"review\"])\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_test_sequences = pad_sequences(test_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639\n",
      "['and', 'a', 'of', 'to', '10', \"i'm\", \"i've\", \"i'd\", \"i'll\", \"film's\", \"'\", '15', '–', 'lundgren', \"movie's\", \"80's\", '30', '000', 'paulie', '100', 'alvin', 'favourite', '90', '20', 'theatre', 'amrita', 'humour', '50', '70s', 'tarzan', 'lugosi', 'burgade', \"70's\", 'macarthur', \"'the\", 'mildred', 'herschel', '40', 'zandalee', 'karloff', 'zombi', 'sandler', 'walken', 'keaton', '13', 'maclean', \"freddy's\", 'gundam', '1972', 'digicorp', 'carface', 'foxx', 'flynn', 'othello', '11', 'grey', '60', \"character's\", 'ritter', \"moore's\", '1945', \"keaton's\", 'byron', \"director's\", 'mst3k', 'silverman', 'rukh', 'kazan', '14', 'fulci', '80s', 'gilliam', '60s', 'bettie', '2006', '1950s', 'mukhsin', '2000', \"davis'\", 'bsg', 'kolchak', 'dolph', '400', 'tolstoy', '1930s', 'heston', 'widmark', 'palance', 'carrey', 'timberlake', 'brando', 'sarandon', \"60's\", 'northam', 'truman', '80', '12', '45', 'tmnt', 'eastwood', 'callahan', 'soylent', 'fetchit', 'weller', 'cedric', 'micheaux', \"'scoop'\", 'romero', '17', '1960s', 'woolsey', 'realised', \"1950's\", '1996', 'hermann', '25', 'sinatra', 'damme', 'polanski', '3000', '700', '13th', '1976', 'esther', 'orked', \"paulie's\", 'tyrannus', \"macarthur's\", 'blaine', 'malone', '1970', 'shapiro', 'patton', 'matthau', \"crowe's\", 'stepin', 'mirren', 'mathieu', \"welles'\", 'iphigenia', \"euripides'\", 'louque', \"'paris'\", 'jindabyne', 'alexandre', 'kazuhiro', 'pacino', '00', '1940s', 'brosnan', '200', '2002', \"fulci's\", 'bouchet', '70', 'veronika', 'slade', 'audiard', '22', 'thursby', 'centre', 'bafta', 'gwyneth', 'paltrow', 'judgement', '1968', '1989', '16', 'modine', '1983', 'favour', 'léaud', 'busey', 'troma', \"person's\", 'ustinov', \"'s\", '1987', \"friend's\", \"hollywood's\", 'greenstreet', \"flynn's\", 'nastassja', 'kinski', 'girlfight', 'sooraj', 'chawla', '44', 'deanna', '1970s', 'branagh', \"lundgren's\", 'welles', 'cacoyannis', 'agamemnon', 'duval', 'farnsworth', 'laurence', 'tashan', 'seuss', 'whoville', 'thierry', 'yoshida', 'colman', 'granger', \"hillyer's\", '1956', 'rathbone', '48', \"'em\", 'mostel', 'steiner', 'mclaglen', \"brother's\", '99', \"romero's\", \"someone's\", 'deluise', '1980s', 'mcnamara', \"o'neal\", 'cartwright', 'natali', 'hitchcock', 'samways', 'realises', '30s', 'darkwolf', 'knightley', \"byron's\", 'spoilt', 'trelkovsky', 'isabelle', 'streisand', \"streisand's\", \"norman's\", 'mazursky', \"1970's\", '1960', \"might've\", 'malden', '1986', '1958', 'casablanca', 'keitel', \"smith's\", 'togan', 'krueger', 'valerie', '1933', 'stratten', '1982', 'quaid', 'wwii', 'bacall', 'eyre', 'colours', \"90's\", 'caprica', 'belzer', 'bathsheba', 'sammo', '65', 'putain', 'higgins', \"girl's\", \"father's\", \"daughter's\", 'demme', 'rosanna', 'arquette', 'shakespearian', 'colton', 'kapur', 'spielberg', 'seagal', 'bassenger', 'ashleigh', \"o'neill\", '1969', 'savalas', \"henry's\", 'cheech', '1939', \"god's\", 'faye', \"30's\", \"peck's\", 'mcqueen', 'crispin', 'swanson', 'armand', 'halperin', '1936', 'carlyle', \"taj's\", 'beban', 'ramones', 'reinhold', 'benet', 'everytown', \"goa'uld\", 'apophis', 'durbin', 'milland', 'wendigo', 'juhee', 'margaux', 'realise', 'dolemite', 'hickcock', \"america's\", \"hitler's\", 'coburn', 'levant', 'marlon', 'carell', \"ford's\", 'gypo', \"'80s\", 'abrams', 'galipeau', 'nguyen', 'cambodia', '2007', 'shue', \"miraglia's\", '1971', 'can´t', '23', 'masterson', 'reginald', 'fassbinder', 'barrymore', 'pilate', 'madsen', 'zerelda', \"herschel's\", 'pryor', '50s', 'ewoks', 'leia', \"paul's\", 'wray', \"price's\", '1953', '1944', '1966', \"latter's\", 'harron', 'cartwrights', 'patric', 'soderbergh', 'colour', \"thompson's\", \"griffith's\", \"1920's\", \"subject's\", \"england's\", '1994', 'dietrich', 'adjani', 'tepper', 'springsteen', 'rhys', 'harrelson', \"show's\", 'fairbanks', '19th', 'ciannelli', 'beals', 'sabrina', '35', 'marjorie', 'cambell', 'arkin', 'kristofferson', 'jagger', \"family's\", \"andrews'\", 'preminger', \"kubrick's\", 'buñuel', 'newhart', 'whoopi', 'uranus', 'kellerman', 'kober', 'springwood', 'haden', \"stewart's\", '18', \"'art'\", 'emil', 'dunne', 'hayward', \"king's\", 'uriah', \"scott's\", \"mj's\", 'scorsese', 'ingram', 'affleck', \"killer's\", 'cecil', \"pryor's\", \"'70s\", 'paxton', 'cavanagh', 'cancelled', 'moynahan', 'christensen', \"allen's\", \"1960's\", 'sabu', 'veidt', 'errol', '40s', 'tristan', 'laputa', 'foch', 'sammi', 'higgin', 'kemble', 'barjatya', 'alok', 'anupam', 'kher', 'hahk', 'christy', \"'68\", 'maguire', 'crowe', 'gettaway', 'tatum', 'dassin', 'grendel', 'beowulf', 'wheezer', 'chuckie', \"kris's\", '58', 'hanzo', 'fellini', \"falk's\", 'altman', \"altman's\", 'duvall', 'spacek', 'i’d', '2005', 'hackett', 'orson', 'odysseus', 'euripides', 'haines', 'hearst', \"davies'\", 'tsiang', 'clifford', 'gazzara', 'audrey', 'hepburn', \"dawson's\", \"don's\", \"'this\", 'pokémon', 'garfield', 'krell', '42nd', \"baker's\", \"rosemary's\", 'cassavetes', 'truffaut', 'moreau', 'binoche', 'lemmon', \"lincoln's\", \"peckinpah's\", 'buscemi', \"macarthur'\", 'yash', \"goldsworthy's\", \"seuss'\", \"'ny'\", \"son's\", 'deewaar', 'amitabh', 'akshaye', 'khanna', 'nicolas', \"rukh's\", \"branagh's\", 'carrere', 'kenner', 'rgv', 'abydos', \"'retarded'\", 'auer', 'winninger', \"'dirty\", \"colman's\", 'hasso', 'mcgovern', 'paget', 'ghosthouse', 'nikhil', 'takia', \"christmas'\", 'tintin', 'hemingway', 'bancroft', 'gackt', \"sho's\", \"'arthur'\", 'guilgud', 'delia', 'estelle', 'lindum', 'jeanette', '21st', 'kornbluth', 'mahoney', '88', \"breathless's\", 'sorvino', 'forsythe', 'talor', 'mitchum', \"girl'\", 'starbuck', 'edna', \"widmark's\", 'udo', 'ferrell', 'culkin', 'binnie', 'faust', 'costello', 'zucker', \"earth's\", \"city's\", 'humphrey', '1979', 'lucio', 'mattei', 'kafka', '115', 'miraglia', 'sybil', 'danning', 'celie', 'shug', 'natasha', 'crighton', \"madonna's\", 'cybill', 'corinne', 'frownland', '95', \"'i\", \"system'\", '£1', \"ship's\", \"sandler's\", 'cromwell', \"fassbinder's\", 'schygulla', \"gibson's\", '30th', 'roseanne', \"moses'\", 'madeline', '1950', 'hadley', 'endor', \"lando's\", \"anyone's\", \"devos'\", 'cassel', 'eleniak', 'berenger', \"beery's\", 'millie', 'gretchen', 'penelope', '1954', 'honourable', 'pernell', 'landon', 'lorne', '120', 'manojlovic', 'cinemagic', 'vincenzo', 'cronenberg', 'tahiti', \"'a\", 'benicio', 'guevara', \"pollack's\", \"tucci's\", \"'straight\", 'roxy', 'ramon', '300', 'graboids', \"black's\", '90s', 'segal', 'mantegna', 'centred', \"writer's\", \"carrey's\", '43', 'woodhouse', 'britons', '1991', 'blythe', 'ravenna', 'pisa', 'attenborough', 'leibman', 'venantini', \"tolkien's\", 'margotta', '1993', 'mononoke', 'nausicaa', 'gabrielle', \"chip's\", 'malcom', \"cop's\", 'tilly', 'redgrave', 'minelli', '1990', '1959', '2003', 'rko', 'flashdance', 'eustache', 'françoise', 'lebrun', 'fawcett', \"marjorie's\", \"cambell's\", 'pierson', 'surtees', \"cinema's\", \"'good\", \"dean's\", \"preminger's\", 'hecht', '1999', \"mukhsin's\", \"orked's\", '1998', 'ginty', 'elisabeth', 'bunuel', 'bernal', \"williams'\", \"foxx's\", 'epps', 'antoine', \"jackson's\", '18th', 'clive', \"dvd's\", '1942', 'sahan', \"actor's\", 'shyamalan', 'kitt', 'viggo', 'mortensen', 'redford', '26', \"montana's\", 'zane', \"craven's\", '1934', 'gertrude', \"wilson's\", 'tamerlane', 'corregidor', 'usaffe', 'bogdanovich', 'candoli', 'kristel', \"greene's\", 'alistair', 'ebeneezer', '28', '34', '21', '34th', '1937', 'sedahl', \"taylor's\", 'klemper', 'evangelion', 'gunbuster', 'lushious', 'dumbland', 'revelled', \"bronte's\", \"jane's\", 'artie', 'dominique', 'alain', 'delon', 'jarmusch', 'tunnelvision', '1951', 'fricker', 'mcconaughey', 'adrien', 'massey', 'kieron', 'jayne', \"author's\", 'spectre', '1928', 'jannings', \"command'\", 'korda', 'gayniggers', 'defence', 'govinda', 'gleason', 'axe', 'heathcliff', 'jacknife', 'krimi', \"disney's\", 'gadgetmobile', \"elephant's\", \"viewer's\", \"carlito's\", \"abc's\", 'gere', 'daines', '1895', 'travelling', '54', '2004', \"greenstreet's\", '1973', 'frye', 'kusturica', \"'my\", 'jaffar', 'corman', 'raoul', 'stallone', 'gershwin', \"singin'\", 'cassidy', 'yvaine', 'gaiman', \"director'\", \"harry's\", '20th', '1949', '1955', 'macready', 'n64', 'hodges', 'hillyer', 'beulah', 'qayamat', 'taktarov', \"audience's\", 'keanu', \"story's\", \"willy's\", 'invisibilation', \"fetchit's\", 'ernst', '1967', '1984', \"true's\", 'lom', 'jurgens', 'horton', 'jessup', 'programme', \"robertson's\", \"simmons'\", \"streep's\", 'helge', \"club's\", 'frawley', 'kundera', \"che's\", 'bolivia', 'kridge', \"cashier's\", '1930', 'nosbusch', 'meryl', 'streep', '12th', 'loesser', 'kaye', 'toomey', 'mankiewicz', 'kwouk', 'manchu', \"karloff's\", 'basinger', \"barbara's\", \"attenborough's\", 'greenaway', 'columbo', 'bluth', 'annabelle', 'coppola', 'wtc', \"'cheap'\", \"50's\", \"farnham's\", 'mcshane', 'mcgavin', 'there’s', 'aztec', 'zemeckis', '1974', \"bunuel's\", 'iago', 'hickox', 'devito', 'bartel', 'hayworth', 'jovi', 'calchas', 'homeric', \"agamemnon's\", 'kazakos', 'irene', 'tatiana', \"kazakos'\", \"wife's\", 'giannini', \"cliché's\", 'albertson', 'theatres', 'peppoire', 'demille', \"peppoire's\", 'noland', 'burlinson', 'sigrid', 'dennehy', \"trent's\", 'vinnie', 'harilall', \"others'\", '98', 'nielsen', '1977', '2001', 'robby', \"planet's\", 'sibrel', 'eloise', 'robeson', 'debenning', '1961', \"holmes's\", \"capote's\", 'weissmuller', 'hiroshima', 'julien', 'duvivier', 'yves', 'renoir', 'yipe', 'ventresca', 'harbour', 'fonda', \"bsg's\", '1915', 'behaviour', 'ralphie', 'mcbride', \"baldwin's\", \"patton'\", 'kareena', 'bremner', 'spacecamp', 'goldsworthy', 'momsen', 'kacey', \"'paris\", \"t'aime'\", \"'monster'\", \"'n'\", 'ramone', '1952', 'broderick', 'mcadams', 'forlani', 'priyadarshan', \"carpenter's\", \"wells'\", 'heyerdahl', 'pantoliano', \"leonard's\", \"genre's\", 'minnelli', 'antwone', 'hilbrand', 'sondra', 'janos', 'corbetts', 'loder', 'fishburne', \"sutherland's\", \"lee's\", 'rudolf', 'sholay', 'mohanlal', 'devgan', 'sushmita', \"'six\", '2036', 'johansson', \"alvin's\", 'elisha', 'lamberto', 'bava', \"daniel's\", \"jack's\", \"teal'c\", 'connolly', 'nuyorican', 'nuyoricans', 'phoenixville', \"harry'\", \"'sudden\", \"impact'\", \"adam's\", 'don´t', 'biko', 'heldar', \"cukor's\", 'nyugen', 'dwivedi', \"schwartz's\", 'spader', \"script's\", \"movie'\", 'fanfan', 'lollo', 'vittorio', 'hillsborough', 'ringu', 'soraj', 'ishk', 'vishk', 'willem', 'penquin', \"mann's\", '1948', 'mescaleros', 'efx', \"chuck's\", 'razzie', 'knef', 'hasselhoff', \"hasselhoff's\", \"'emotion'\", \"love'\", 'sawant', 'napton', \"'columbo'\", \"'corky\", \"romano'\", 'falk', \"'finding\", 'hergé', 'mariel', 'sisters’', 'aikens', 'noelle', 'bronson', 'pinchot', 'lerner', 'ekin', \"their's\", 'zd', '1997', \"novak's\", 'rosenstrasse', 'karvan', \"tony's\", \"2000's\", \"'i'm\", 'digimon', 'gamera', \"hoffman's\", 'chertkov', 'mcavoy', 'vampira', 'dolores', 'leif', \"chris'\", \"husband's\", \"svendsen's\", \"dolemite's\", 'thurman', 'deveraux', \"hanks'\", 'khoi', 'arnie', 'crawley', 'corbin', 'bernsen', \"abbey's\", 'sg1', \"'u'\", \"massey's\", 'maltin', '67', 'zabriskie', 'antonioni', \"era's\", 'chupke', 'preity', 'zinta', 'diwani', \"simon's\", \"josh's\", \"'chinese\", 'lunohod', 'lawton', \"'government\", \"media'\", \"perry's\", 'tanglefoot', 'gruesom', 'beatty', 'incorruptable', 'maddonna', 'gleanne', 'headly', \"tracy's\", 'girlfrined', 'pruneface', 'patinkin', 'manlis', \"o'ross\", 'sudetanland', 'czechs', 'austrians', '1943', 'vanesa', \"want's\", 'navajo', 'yuma', \"provo's\", \"susan's\", 'brickman', 'bugrade', \"graduate'\", \"'birthday\", 'weclome', \"rathbone's\", \"crosby's\", 'beckinsell', \"pullman's\", 'anhalt', \"kazan's\", 'burgendy', 'rogen', 'sharkboy', 'lavagirl', \"d'linz\", \"jeanette's\", 'groucho', \"6's\", \"claude's\", \"mike's\", 'sequiturs', 'labelled', 'saget', 'straithrain', '12s', 'cogsworth', 'potts', 'lansbury', \"potts'\", 'offence', \"'rudy'\", \"'awakenings'\", \"owl's\", 'sargents', 'thuy', 'keisha', 'kier', 'virgine', 'ledoyen', 'berlinale', \"almodovar's\", 'geddes', \"zero's\", \"cleaner's\", 'foetus', \"original's\", 'stefano', \"mainetti's\", 'fabio', \"frizzi's\", \"'79\", \"tyler's\", 'gilliamesque', \"gilliam's\", \"monkey's\", 'rumour', \"bourne's\", \"damon's\", \"bacon's\", 'uccide', 'wildenbrück', \"grandfather's\", \"bouchet's\", \"leo's\", 'calibro', 'malfatti', \"nicolai's\", 'increses', 'nettie', 'henstridge', 'keneth', '1957', 'madoona', 'dudikoff', \"\\x91b'movie\", \"\\x91spawn'\", \"games'\", 'macnamara', 'there´s', 'they´re', 'bg´s', 'you´ll', 'it´s', 'phisics', 'sufered', '´cos', 'chiba', 'jeffries', 'alexs', \"corinne's\", 'stiflers', 'umecki', '106', 'beefheart', 'btas', 'twoface', 'clayface', \"lucy's\", \"'lucifer'\", \"male's\", 'americian', '30am', 'farscape', \"hetfield's\", \"'electrical\", \"expert's\", \"pc's\", \"'hot\", \"'dickie'\", 'shecker', \"maughan's\", 'unbenownst', 'rainer', 'werner', \"gi's\", 'desny', 'oswald', \"maria's\", \"schygulla's\", 'voss', 'latifah', \"fallon's\", 'caiaphas', 'blakely', \"michell's\", \"savala's\", 'hatfield', 'formulmatic', 'sinny', 'cornyness', 'pungee', \"'we\", 'vosloo', 'adjustin', 'laraine', \"pharaoh's\", 'gilford', \"zerelda's\", \"sodom's\", \"jesus'\", 'hoitytoityness', 'darkheart', 'colourful', 'bethany', 'scandinavia', 'bergstrom', 'selldal', 'wollter', 'sellam', 'looooooooong', \"'beauty\", \"beast'\", \"bertolucci's\", \"'moral'\", \"'symbolism'\", \"'forbidden'\", \"audiard's\", 'more\\u2028much', 'emmanuelle', \"carla's\", 'devos', 'ericka', 'baywatch', \"'90s\", 'guignol', \"'missing'\", \"brahm's\", 'cregar', '\\u2028though', '73', 'spoilerwarning', 'débutante', \"boss's\", 'torme', 'soxers', 'borge', 'klaw', 'yeager', 'guinevere', 'terrorising', \"tc's\", \"'coerced\", \"'into\", 'alejandro', 'amenábar', \"trace's\", '2020', 'rohauer', \"cartwrights'\", 'baccarin', \"morena's\", \"something's\", 'bonner', 'barrimore', 'elams', \"'spy'\", 'wgbh', 'deighton', \"holm's\", \"'largo\", \"winch'\", 'roden', 'sisley', 'radivoje', 'bukvic', 'goran', 'croatian', 's500', 'extremal', \"london's\", \"schoolteacher's\", \"tom'\", \"thaw's\", 'ronnies', \"bumpkin'\", 'digicorps', \"cameo's\", 'trebor', 'subor', \"'times'\", 'lazerov', 'mcdermott', \"n'sync\", \"laurie's\", \"guy'\", 'seger', \"guys'\", 'indira', \"varma's\", 'adrienne', \"morrison's\", 'tucci', \"sheets'\", \"'cutting\", \"edge'\", \"'enshrined\", \"mediocrity'\", 'bulgaria', '1000', 'eliza', 'headey', \"'thinner'\", 'bachman', 'althou', 'prettymuch', 'werewolfworld', 'aplus', 'horrortitles', \"brat's\", 'skellington', \"connell's\", 'lughnasa', 'danner', \"'get\", 'loren', 'especically', 'shadley', 'guiccioli', 'childe', \"shelley's\", 'missolonghi', \"'less'\", '1816', \"rohmer's\", \"duke'\", 'domini', 'enfilren', 'marlene', 'androvsky', 'algiers', \"tenant'\", \"'frantic'\", \"truffaut's\", \"'l'histoire\", \"d'adele\", \"'knife\", \"water'\", 'crapo', 'nelkin', 'macchio', 'poston', \"mad's\", 'blackploitation', 'roadwarrior', 'venantino', 'briganti', 'dardano', 'sacchetti', 'luca', 'paura', 'morti', 'viventi', \"mary's\", 'manji', 'oafy', 'tolkien', \"'lucky'\", \"nicholson's\", 'larner', \"part's\", 'japon', 'palme', \"miyazaki's\", 'hardesty', 'crandall', \"hoover's\", 'esamples', 'mcdowall', \"mamet's\", 'mamet', 'huckabees', 'huppert', 'cinematography\\u2028', 'unger', 'durang', 'ignatius', \"'meat'\", '1980', '1988', 'ifans', 'boorman', 'mcgregor', 'organising', '1975', 'disreguarded', \"christopher's\", \"'wizards\", \"kingdom'\", 'htv', 'norseman', \"derek's\", 'fontaine', 'jaffe', 'rudyard', 'kipling', 'eduard', \"'bushwhackers'\", 'eyeroller', '911', \"'tim\", \"ben'\", 'geewiz', \"journey's\", \"''your\", \"thing''\", 'dougray', \"''dark''\", 'isabel', \"allende's\", \"book's\", \"brady's\", \"kapow's\", 'catwomen', 'schertler', 'memama', 'worthing', 'farrah', 'scarwid', 'alfrie', 'woodard', 'elequence', \"vonnegut's\", \"nolte's\", 'selfloathing', 'sherryl', 'pinnocioesque', 'sastifyingly', \"bettie's\", 'barbra', \"esther's\", \"loggins'\", \"kristofferson's\", \"watt's\", \"ned's\", 'bacchus', 'clunes', 'ballarat', \"hunting'\", 'baise', \"moi'\", \"'flirt'\", 'blige', 'undescribably', 'tilda', 'swinton', 'woodeness', 'comotose', \"'intensity'\", 'betacam', 'gladys', \"stooges'\", 'keays', 'lmn', 'corin', 'nemec', 'madchen', \"amick's\", \"nemec's\", \"margaret's\", \"protagonist's\", 'tierney', \"needn't\", \"wagner's\", 'dreyfus', 'braga', 'addams', 'untrumpeted', 'filmfour', \"cypher's\", \"pacula's\", 'breuer', 'nietzsche', \"breuer's\", \"bully's\", 'bgr', 'koboi', \"'girls'\", \"grandparents'\", 'selleck', \"kevin's\", \"verhoeven's\", 'sasquatsh', 'henriksen', \"prosecution's\", \"'fridge\", \"killin'\", \"'blood'\", \"'plan\", \"9'\", 'bosnia', 'bosnian', 'inarritu', 'lalouche', 'imamura', 'osbourne', \"metal's\", 'benjamenta', \"bear's\", \"gore's\", 'paradiso', 'georg', \"gerard's\", 'iben', 'hjejle', 'overracting', 'tylo', \"pepper's\", \"floyd's\", \"'chaplain\", \"drawer'\", 'kieth', 'weis', 'demonicus', 'willian', \"jamie's\", 'hutchison', 'fuqua', \"government's\", 'inutterably', 'lili', 'doright', 'quigon', 'neeson', 'queenish', \"sally's\", 'moriarty', 'dondaro', 'cassell', 'warnicki', \"meredith's\", 'pyle', 'manfredi', 'carrol', 'wallece', \"prophet's\", \"diplomat's\", 'nevsky', 'leni', \"reifenstal's\", \"bbc's\", 'goodtimes', 'manchuria', 'gokbakar', 'dikkat', 'cikabilir', 'preciosities', 'hannibal', 'shockmovie', \"uzi's\", 'blablabla', 'ertha', \"kitt's\", \"tiffany's\", 'highsmith', 'objectifier', \"victor's\", \"'snowwhite'\", \"pm's\", \"strain's\", 'griffith', 'aragorn', \"priestly's\", 'missoula', \"maclean's\", 'englund', 'greenblatt', \"outskirt's\", 'talalay', \"know's\", \"death's\", \"other's\", 'coccio', 'hepo', \"italians'\", 'councellor', \"hartmen's\", \"o'day\", 'dvorak', '1932', 'nicholls', \"anne's\", 'heggie', 'cuthbert', 'westley', 'marilla', 'messinger', 'turakistan', 'haliburton', 'aykroyd', 'cusack', 'cusacks', 'pointeblank', 'marisa', 'tomeihere', 'lorenzo', 'kinmont', \"oj's\", 'zooey', 'deschanel', \"'x'\", \"history's\", \"sam'\", \"abandon'\", 'bataan', \"return'\", 'leyte', \"camp's\", 'toonami', 'sailormoon', 'ryo', 'candolis', 'duning', \"tree's\", 'navajos', \"grandpa's\", 'arliss', 'valliant', 'wwi', 'pabst', \"brecht's\", 'lenya', 'insectish', \"creame's\", \"dicken's\", 'cratchitt', 'kitrosser', \"maugham's\", \"rko's\", \"howard's\", 'claudette', 'barretts', 'wimpole', '33']\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS=12000\n",
    "EMBEDDING_DIM=300\n",
    "\n",
    "# tokenizer.word_index provides the mapping \n",
    "# between a word and word index for all words\n",
    "NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "\n",
    "# \"+1\" is for padding symbol\n",
    "embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "ignored_words=[]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # if word_index is above the max number of words, ignore it\n",
    "    if i >= NUM_WORDS:\n",
    "        continue\n",
    "    if word in wv_model.wv:\n",
    "        embedding_matrix[i]=wv_model.wv[word]\n",
    "    else:\n",
    "        ignored_words.append(word)\n",
    "        \n",
    "print(len(ignored_words))\n",
    "print(ignored_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.48500, saving model to best_model\n",
      "15s - loss: 0.7047 - acc: 0.5210 - val_loss: 0.7026 - val_acc: 0.4850\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc improved from 0.48500 to 0.61000, saving model to best_model\n",
      "14s - loss: 0.6503 - acc: 0.6180 - val_loss: 0.6318 - val_acc: 0.6100\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc did not improve\n",
      "13s - loss: 0.5724 - acc: 0.7190 - val_loss: 0.5950 - val_acc: 0.6100\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc improved from 0.61000 to 0.80500, saving model to best_model\n",
      "13s - loss: 0.4735 - acc: 0.7930 - val_loss: 0.4609 - val_acc: 0.8050\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc did not improve\n",
      "14s - loss: 0.3371 - acc: 0.8760 - val_loss: 0.4180 - val_acc: 0.8000\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc did not improve\n",
      "13s - loss: 0.2858 - acc: 0.8880 - val_loss: 0.3986 - val_acc: 0.8050\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc improved from 0.80500 to 0.85500, saving model to best_model\n",
      "13s - loss: 0.1930 - acc: 0.9340 - val_loss: 0.3610 - val_acc: 0.8550\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc did not improve\n",
      "13s - loss: 0.1513 - acc: 0.9470 - val_loss: 0.3348 - val_acc: 0.8350\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc did not improve\n",
      "13s - loss: 0.1134 - acc: 0.9620 - val_loss: 0.3317 - val_acc: 0.8450\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc did not improve\n",
      "13s - loss: 0.0953 - acc: 0.9670 - val_loss: 0.3579 - val_acc: 0.8450\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc did not improve\n",
      "13s - loss: 0.0626 - acc: 0.9860 - val_loss: 0.3502 - val_acc: 0.8400\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EMBEDDING_DIM=300\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "output_units_num=1\n",
    "num_filters=64\n",
    "\n",
    "# set the dense units\n",
    "dense_units_num= num_filters*len(FILTER_SIZES)\n",
    "\n",
    "BTACH_SIZE = 32\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# Assume we only have 500 labeled data\n",
    "# split dataset into train (70%) and test sets (20%)\n",
    "\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, \\\n",
    "                EMBEDDING_DIM=300,\\\n",
    "                NUM_OUTPUT_UNITS=output_units_num, \\\n",
    "                NUM_FILTERS=num_filters,\\\n",
    "                NUM_DENSE_UNITS=dense_units_num,\\\n",
    "                PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_acc', patience=3, verbose=2, mode='max')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training=model.fit(padded_sequences, train_set[\"sentiment\"], \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[padded_test_sequences, test_set[\"sentiment\"]], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "acc: 70.50%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.77      0.72        98\n",
      "          1       0.74      0.65      0.69       102\n",
      "\n",
      "avg / total       0.71      0.70      0.70       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "# predict\n",
    "pred=model.predict(padded_test_sequences)\n",
    "pred=np.where(pred>0.5,1,0)\n",
    "print(pred[0:5])\n",
    "# evaluate the model\n",
    "scores = model.evaluate(padded_test_sequences, test_set[\"label\"], verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "print(classification_report(test_set[\"label\"], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         freq\n",
      "several    40\n",
      "posters     5\n",
      "have      911\n",
      "quoted      1\n",
      "renoir      6\n",
      "   word_freq  count   percent    cumsum\n",
      "0          1   8356  0.495170  0.495170\n",
      "1          2   2688  0.159289  0.654459\n",
      "2          3   1386  0.082133  0.736593\n",
      "3          4    832  0.049304  0.785896\n",
      "4          5    560  0.033185  0.819081\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEMCAYAAADK231MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1416f8eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise 5.9.1 Set MAX_NB_WORDS to \n",
    "# include words that appear at least K times\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# get count of each word\n",
    "df=pd.DataFrame.from_dict(tokenizer.word_counts, orient=\"index\")\n",
    "df.columns=['freq']\n",
    "print(df.head())\n",
    "\n",
    "# get histogram of word count\n",
    "df=df['freq'].value_counts().reset_index()\n",
    "df.columns=['word_freq','count']\n",
    "\n",
    "# sort by word_freq\n",
    "df=df.sort_values(by='word_freq')\n",
    "\n",
    "# convert absolute counts to precentage\n",
    "df['percent']=df['count']/len(tokenizer.word_counts)\n",
    "# get cumulative percentage\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df.iloc[0:50].plot(x='word_freq', y='cumsum');\n",
    "\n",
    "plt.show();\n",
    "\n",
    "# if set min count for word to 10, \n",
    "# what % of words can be included?\n",
    "# how many words will be included?\n",
    "# This is the parameter MAX_NB_WORDS\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sent_length  counts  percent   cumsum\n",
      "188           10       1  0.00125  0.00125\n",
      "195           28       1  0.00125  0.00250\n",
      "184           36       1  0.00125  0.00375\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAELCAYAAAAiIMZEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxlY/ws2mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1416d4550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a series based on the length of all sentences\n",
    "sen_len=pd.Series([len(item) for item in sequences])\n",
    "\n",
    "# create histogram of sentence length\n",
    "# the \"index\" is the sentence length\n",
    "# \"counts\" is the count of sentences at a length\n",
    "df=sen_len.value_counts().reset_index().sort_values(by='index')\n",
    "df.columns=['sent_length','counts']\n",
    "\n",
    "# sort by sentence length\n",
    "# get percentage and cumulative percentage\n",
    "\n",
    "df['percent']=df['counts']/len(sen_len)\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "print(df.head(3))\n",
    "\n",
    "# From the plot, 90% sentences have length<500\n",
    "# so it makes sense to set MAX_DOC_LEN=4~500 \n",
    "df.plot(x=\"sent_length\", y='cumsum');\n",
    "plt.show();\n",
    "\n",
    "# what will be the minimum sentence length\n",
    "# such that 99% of sentences will not be truncated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
