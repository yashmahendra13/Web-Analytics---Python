{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Using NLTK (I)</center>\n",
    "\n",
    "References:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    " - https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    " - http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK installation\n",
    " 1. Install NLTK package using: pip install nltk \n",
    " 2. Open your python editor (Jupyter Notebook, Spyder etc.) and type the following comands below. Select \"all packages\" to install data included in NLTK, including corpora and books. It may take a few minutes to download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "\n",
    " - Objectives:\n",
    "   * Split documents into tokens or segments\n",
    "   * Clean up tokens and annotate tokens\n",
    "   * Extract features from tokens for further text mining tasks\n",
    " - Basic processing steps:\n",
    "   * Tokenization: split documents into individual words or segments\n",
    "   * Remove stop words and filter tokens\n",
    "   * POS (part of speech) Tagging\n",
    "   * Normalization: Stemming, Lemmatization\n",
    "   * Named Entity Recognition (NER)\n",
    "   * Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    "   * Create document-to-term matrix (bag of words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import re    # import re module\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.1. Load the text for analysis\n",
    "\n",
    "text='''`strange days' chronicles the last two days of 1999 in los angeles. \n",
    " as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips. \n",
    " he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him. \n",
    " this film features good performances, impressive film-making technique and breath-taking crowd scenes. \n",
    " director kathryn bigelow knows her stuff and does not hesitate to use it. \n",
    " but as a whole, this is an unsatisfying movie. \n",
    " the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama. \n",
    " not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. \n",
    " the film just ends up preachy, unexciting and uninvolving.'''\n",
    "\n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Tokenization\n",
    " - **Definition**: the process of breaking a stream of textual content up into words, terms, symbols, or some other meaningful elements called tokens.\n",
    "    * Word (Unigram)\n",
    "    * Bigram (Two consecutive words)\n",
    "    * Trigram (Three consecutive words)\n",
    "    * Sentence\n",
    " - Different methods exist:\n",
    "    * Split by regular expression patterns\n",
    "    * NLTK's word tokenizer\n",
    "    * NLTK's regular expression tokenizer (customizable)\n",
    " - None of them can be perfect for any tokenization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.1. Simply split the text by one or more non-word characters\n",
    "\n",
    "# \\W+: one or more non-words\n",
    "tokens = re.split(r\"\\W+\", text)   \n",
    "\n",
    "# get the number of tokens\n",
    "\n",
    "print(len(tokens))                   \n",
    "print(tokens)                     \n",
    "\n",
    "# Pros: no punctuation, just words\n",
    "# Cons: breath-taking and film-making, doesn't\n",
    "# are split into two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.2 NLTK's word tokenizer: \n",
    "\n",
    "# break down text into words and punctuations\n",
    "\n",
    "# invoke NLTK's word tokenizer\n",
    "tokens = nltk.word_tokenize(text)    \n",
    "print(len(tokens) )                   \n",
    "print (tokens)       \n",
    "\n",
    "# Pros: words are well tokenized, \n",
    "# e.g. breath-taking and film-making each is captured as one word\n",
    "# doesn't becomes does n't\n",
    "# Pros: need to remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leading or trailing punctuations\n",
    "\n",
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "tokens=[token.strip(string.punctuation) for token in tokens]\n",
    "\n",
    "# remove empty tokens\n",
    "tokens=[token.strip() for token in tokens if token.strip()!='']\n",
    "print(len(tokens) )\n",
    "print(tokens)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.2 NLTK's regular expression tokenizer (customizable)\n",
    "\n",
    "# Pattern can be customized to your need\n",
    "\n",
    "# a word is defined as a sequence of word characters  \n",
    "# followed by optional word characters or \"-|'\" \n",
    "# ended with a word character\n",
    "\n",
    "# e.g. film-making, doesn't\n",
    "\n",
    "pattern=r'\\w[\\w\\-\\']*\\w'                        \n",
    "\n",
    "\n",
    "# call NLTK's regular expression tokenization\n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "print(len(tokens))\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.3 Use NLTK's regular expression tokenizer \n",
    "# to define sentences (i.e. starts with non-space character, \n",
    "# ends with !?.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.2. Vocabulary \n",
    " - Vocabulary: the set of unique tokens  \n",
    " - Dictionary: typicallly, the vocabulary of a text can be represented as a dictionary \n",
    "    * Key: word\n",
    "    * Value: count of the word\n",
    " - Find what words are frequently used (stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2.1 \n",
    "# Get vocabulary and dictionary of text\n",
    "\n",
    "vocabulary= set(tokens)                                        \n",
    "# set() convert a list to a set without any duplicates\n",
    "print (vocabulary)\n",
    "\n",
    "# tokens.count(word) returns the count of the word in tokens (list)\n",
    "dictionary={word: tokens.count(word) for word in vocabulary}\n",
    "# by default, dictionary is sorted by key\n",
    "print(\"\\nsort by word\")\n",
    "print (dictionary)\n",
    "\n",
    "# find what are the frequent words\n",
    "# sort the dictionary by value\n",
    "# sorted(iterable, key) sorts an iterable object by the comparison key\n",
    "# lambda: anonymous function defined without a name. \n",
    "# lambda item:-item[1] sorts the list by the 2nd element in a descending order\n",
    "print(\"\\nsort by frequency\")\n",
    "print(sorted(dictionary.items(), key=lambda item:-item[1]))\n",
    "\n",
    "# what kind of words usually have high frequency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.3. Stop words and word filtering\n",
    "\n",
    " - Stop words: a set of commonly used words, have very little meaning, and cannot differentiate a text from others, such as \"and\", \"the\" etc. \n",
    " - Stop words are typically ignored in NLP processing or by search engine\n",
    " - Stop words usually are application specific. You can define your own stop words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.3.1\n",
    "# get NLTK English stop words\n",
    "# You can modify this list by adding more stop words or remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words+=[\"film\", \"films\"]\n",
    "print (stop_words)\n",
    "\n",
    "# filter stop words out of the dictionary\n",
    "# by creating a new dictionary\n",
    "\n",
    "filtered_dictionary={word: dictionary[word] \\\n",
    "                     for word in dictionary \\\n",
    "                     if word not in stop_words}\n",
    "print(\"\\nsort dictionary without stop words by frequency\")\n",
    "print(sorted(filtered_dictionary.items(), key=lambda item:-item[1]))\n",
    "\n",
    "print(len(filtered_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3.2\n",
    "# Find positive words \n",
    "\n",
    "with open(\"positive-words.txt\",'r') as f:\n",
    "    positive_words=[line.strip() for line in f]\n",
    "\n",
    "#print(positive_words)\n",
    "positive_tokens=[token for token in tokens \\\n",
    "                 if token in positive_words]\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Can you use positive/negative words to determine the sentiment?\n",
    "\n",
    "- Naive sentiment analysis:\n",
    "  - Find positive/negative words\n",
    "  - If more positive words than negative, then positive\n",
    "  - Otherwise, negative\n",
    "- Note the sentence: \n",
    "  -  \"the problem is that the writers, james cameron and jay cocks , were **<font color=\"red\">too ambitious</font>**, aiming for a film with social relevance, thrills, and drama. **<font color=\"red\">not that ambitious</font>** film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. the film just ends up preachy, unexciting and uninvolving.\"\n",
    "- How to deal with negation?\n",
    "- Some useful rules:\n",
    "    - Negative sentiment: \n",
    "      - negative words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - positive words preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "    - Positive sentiment (in the similar fashion):\n",
    "      - positive words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - negative terms following a negation within  $n$ (e.g. three) words in the same sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3.1 # check if a positive word is preceded by negation words\n",
    "# e.g. not, too, n't, no, cannot\n",
    "\n",
    "negations=['not', 'too', 'n\\'t', 'no', 'cannot', 'neither','nor']\n",
    "tokens = nltk.word_tokenize(text)  \n",
    "\n",
    "print(tokens)\n",
    "\n",
    "positive_tokens=[]\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in positive_words:\n",
    "        if idx>0:\n",
    "            if tokens[idx-1] not in negations:\n",
    "                positive_tokens.append(token)\n",
    "        else:\n",
    "            positive_tokens.append(token)\n",
    "\n",
    "\n",
    "print(positive_tokens)\n",
    "\n",
    "# what if the positive word is preceded by a negation within 3 words in a sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
