{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Using NLTK\n",
    "\n",
    "references:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    " - https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    " - http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK installation\n",
    " 1. Install NLTK package using: pip install nltk \n",
    " 2. Open python and type the following comands below. Select \"all packages\" to install data included in NLTK, including corpora, and books. It may take a few minutes to download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "\n",
    " - Objectives:\n",
    "   * Split documents into words, punctuation sysmbols, or segments\n",
    "   * Understand vocabulary of the text\n",
    "   * Extract features for further text mining tasks\n",
    " - Basic processing steps:\n",
    "   * Tokenization: split documents into individual words and punctuation symbols\n",
    "   * Remove stop words and filter tokens\n",
    "   * POS (part of speech) Tagging\n",
    "   * Normalization: Stemming, Lemmatization\n",
    "   * Named Entity Recognition (NER)\n",
    "   * Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    "   * Create document-to-term matrix (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oil prices soar to all-time record. stocks end up near year end. money funds rose in latest week. stocks up; traders eye crude oil prices. dollar rising broadly on record trade gain'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "news=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]\n",
    "text=\". \".join(news).lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Tokenization\n",
    " - **Definition**: the process of breaking a stream of textual content up into words, terms, symbols, or some other meaningful elements called tokens.\n",
    "    * Word (Unigram)\n",
    "    * Sentence\n",
    "    * Bigram (Two consecutive words)\n",
    "    * Trigram (Three consecutive words)\n",
    "   \n",
    " - Different methods exist:\n",
    "    * Split by regular expression patterns\n",
    "    * NLTK's word tokenizer\n",
    "    * NLTK's regular expression tokenizer (customizable)\n",
    " - None of them can be perfect for any tokenization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['oil prices soar to all-time record.',\n",
       " 'stocks end up near year end.',\n",
       " 'money funds rose in latest week.',\n",
       " 'stocks up; traders eye crude oil prices.',\n",
       " 'dollar rising broadly on record trade gain']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.2.1. Segmentation by Sentences\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "len(sentences)\n",
    "sentences\n",
    "\n",
    "# what patterns can be used to segment \n",
    "# text into sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Bigrams (2 consecutive words),  Trigrams (3 consecutive words), or in general n-grams\n",
    " - Why bigrams and trigrams?\n",
    " - How to get bigrams or trigrams:\n",
    "    1. First tokenize text into unigrams\n",
    "    2. Slice through the list of unigrams to get bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('oil', 'prices'), ('prices', 'soar'), ('soar', 'to'), ('to', 'all-time'), ('all-time', 'record'), ('record', '.'), ('.', 'stocks'), ('stocks', 'end'), ('end', 'up'), ('up', 'near'), ('near', 'year'), ('year', 'end'), ('end', '.'), ('.', 'money'), ('money', 'funds'), ('funds', 'rose'), ('rose', 'in'), ('in', 'latest'), ('latest', 'week'), ('week', '.'), ('.', 'stocks'), ('stocks', 'up'), ('up', ';'), (';', 'traders'), ('traders', 'eye'), ('eye', 'crude'), ('crude', 'oil'), ('oil', 'prices'), ('prices', '.'), ('.', 'dollar'), ('dollar', 'rising'), ('rising', 'broadly'), ('broadly', 'on'), ('on', 'record'), ('record', 'trade'), ('trade', 'gain')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('oil', 'prices', 'soar'),\n",
       " ('prices', 'soar', 'to'),\n",
       " ('soar', 'to', 'all-time'),\n",
       " ('to', 'all-time', 'record'),\n",
       " ('all-time', 'record', '.'),\n",
       " ('record', '.', 'stocks'),\n",
       " ('.', 'stocks', 'end'),\n",
       " ('stocks', 'end', 'up'),\n",
       " ('end', 'up', 'near'),\n",
       " ('up', 'near', 'year'),\n",
       " ('near', 'year', 'end'),\n",
       " ('year', 'end', '.'),\n",
       " ('end', '.', 'money'),\n",
       " ('.', 'money', 'funds'),\n",
       " ('money', 'funds', 'rose'),\n",
       " ('funds', 'rose', 'in'),\n",
       " ('rose', 'in', 'latest'),\n",
       " ('in', 'latest', 'week'),\n",
       " ('latest', 'week', '.'),\n",
       " ('week', '.', 'stocks'),\n",
       " ('.', 'stocks', 'up'),\n",
       " ('stocks', 'up', ';'),\n",
       " ('up', ';', 'traders'),\n",
       " (';', 'traders', 'eye'),\n",
       " ('traders', 'eye', 'crude'),\n",
       " ('eye', 'crude', 'oil'),\n",
       " ('crude', 'oil', 'prices'),\n",
       " ('oil', 'prices', '.'),\n",
       " ('prices', '.', 'dollar'),\n",
       " ('.', 'dollar', 'rising'),\n",
       " ('dollar', 'rising', 'broadly'),\n",
       " ('rising', 'broadly', 'on'),\n",
       " ('broadly', 'on', 'record'),\n",
       " ('on', 'record', 'trade'),\n",
       " ('record', 'trade', 'gain')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.3.1. Get bigrams from the text                       \n",
    "\n",
    "# get unigrams\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "# bigrams are formed from unigrams\n",
    "# nltk.bigram returns an iterator\n",
    "bigrams=list(nltk.bigrams(tokens))\n",
    "print(bigrams)\n",
    "\n",
    "# trigrams\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.4. Collocation\n",
    " - Most bigrams or trigrams may sound odd. However, we need to pay attention to frequent bigrams or trigrams\n",
    " - **Collocation**: an expression consisting of two or more words that correspond to some conventional way of saying things, e.g. red wine, United States, graduate students etc.\n",
    "    - Collocations are not fully compositional in that there is usually an element of meaning added to the combination.\n",
    " - Question: how to find collocations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 'stocks'),\n",
       " ('oil', 'prices'),\n",
       " ('.', 'dollar'),\n",
       " ('.', 'money'),\n",
       " (';', 'traders'),\n",
       " ('all-time', 'record'),\n",
       " ('broadly', 'on'),\n",
       " ('crude', 'oil'),\n",
       " ('dollar', 'rising'),\n",
       " ('end', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(',', 'and'),\n",
       " (',', '\"'),\n",
       " ('of', 'the'),\n",
       " (\"'\", 's'),\n",
       " ('in', 'the'),\n",
       " ('said', ','),\n",
       " ('said', 'to'),\n",
       " ('.', 'He'),\n",
       " ('the', 'land'),\n",
       " ('.', 'The')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.1. Get collocation\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# bigram association measures\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# construct bigrams using words from our example\n",
    "finder = BigramCollocationFinder.from_words(\\\n",
    "         nltk.word_tokenize(text))\n",
    "\n",
    "# the corpus is too small\n",
    "finder.nbest(bigram_measures.raw_freq, 10)  \n",
    "\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "finder = BigramCollocationFinder.from_words(\\\n",
    "        nltk.corpus.genesis.words('english-web.txt'))\n",
    "#finder.nbest(bigram_measures.pmi, 10)  \n",
    "finder.nbest(bigram_measures.raw_freq, 10) \n",
    "\n",
    "# Note that the most frequent bigrams are very odd\n",
    "# how to fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('God', 'said'),\n",
       " ('one', 'hundred'),\n",
       " ('Jacob', 'said'),\n",
       " ('Yahweh', 'God'),\n",
       " ('Yahweh', 'said'),\n",
       " ('years', 'old'),\n",
       " ('seven', 'years'),\n",
       " ('Joseph', 'said'),\n",
       " ('every', 'man'),\n",
       " ('five', 'years')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.2. Find collocation by filter\n",
    "\n",
    "import string\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(\\\n",
    "        nltk.corpus.genesis.words('english-web.txt'))\n",
    "\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words\\\n",
    "                         or w.strip(string.punctuation)=='')\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 10) \n",
    "\n",
    "# better?\n",
    "# how to get rid of \"xxx said\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 How to find collocations - PMI\n",
    "- By frequency (perhaps with filter)\n",
    "- Pointwise Mutual Information (PMI)\n",
    "  - giving two words $w_1, w_2$, $$PMI(w_1,w_2)=\\log{\\frac{p(w_1,w_2)}{p(w_1)*p(w_2)}}$$\n",
    "  - Some observations:\n",
    "    - if $w_1$ and $w_2$ are independent, $PMI(w_1,w_2)=0$\n",
    "    - if $w_1$ is completely dependent on $w_2$, i.e. $p(w_1,w_2)=p(w_2)$, $PMI(w_1,w_2)=\\frac{1}{p(w_1)}$. In this case, what if $w_1$ just appear once in the corpus? \n",
    "    - PMI favors less frequent collocations \n",
    "    - how to fix it?\n",
    "- Other methods:\n",
    "  - t test\n",
    "  - chi_square test\n",
    "  - likelihood ratio\n",
    "- reference: \n",
    "   - https://nlp.stanford.edu/fsnlp/promo/colloc.pdf\n",
    "   - http://www.nltk.org/howto/collocations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Allon', 'Bacuth'),\n",
       " ('Ashteroth', 'Karnaim'),\n",
       " ('Ben', 'Ammi'),\n",
       " ('En', 'Mishpat'),\n",
       " ('Jegar', 'Sahadutha'),\n",
       " ('Salt', 'Sea'),\n",
       " ('Whoever', 'sheds'),\n",
       " ('appoint', 'overseers'),\n",
       " ('aromatic', 'resin'),\n",
       " ('cutting', 'instrument')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.1. Metrics for Collocations\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "finder = BigramCollocationFinder.from_words(\\\n",
    "        nltk.corpus.genesis.words('english-web.txt'))\n",
    "# find top-n bigrams by pmi\n",
    "finder.nbest(bigram_measures.pmi, 10) \n",
    "\n",
    "# filter bigrams by frequency\n",
    "#finder.apply_freq_filter(5)\n",
    "#finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POS (Part of Speech) Tagging\n",
    "\n",
    " - What is POS Tagging:\n",
    "   * The process of marking up a word in a text as corresponding to a particular part of speech (e.g. nouns, verbs, adjectives, adverbs etc.), based on both **its definition**, as well as its **context** — adjacent and related words in a phrase, sentence, or paragraph. \n",
    " - Why POS Tagging: \n",
    "   * **disambiguation**: A word may have different meanings. POS tag is a potential strong signal for word sense disambiguation. For example, \"I fish a fish\"\n",
    "   * **Phrase extraction**: Use POS rules to define accepted phrases (or information unit), or collocations for indexing and retrieval:\n",
    "     * Adj + Noun, e.g. nice house\n",
    "     * Verb + Noun, e.g. play football\n",
    "     * typical collocation patterns (https://nlp.stanford.edu/fsnlp/promo/colloc.pdf):\n",
    "       - Adj + Noun: e.g. linear function\n",
    "       - Noun + Noun: e.g. regression coefficient\n",
    "       - Adj + Adj + Noun: e.g. Gaussian random variable\n",
    "       - Noun + Adj + Noun: e.g. mean squared error\n",
    "       - Noun + Noun + Noun: e.g. class probability function\n",
    "       - Noun + Preposition + Noun: e.g. dregrees of freedom\n",
    "   * **Filter tokens**:  some POS have less importance in retrieval, e.g. stopwords such as ‘a’, ‘an’, ‘the’, and other glue words like 'in', 'on', 'of' etc.\n",
    "   * Find other forms of a word based on POS\n",
    "        * Noun: plural and singular\n",
    "        * Verb: past, present and future tense\n",
    "        * Adjective: positive, comparative, and superlative\n",
    " - List of Penn Treebank Tags can be found at https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    " - A tagger (program for tagging) is trained based on a corpus using machine learning approaches. It may not be very accurate when applying it your corpus.\n",
    "   - Stanford tagger (~97%)\n",
    "   - NLTK default tagger (PerceptronTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.1. To find all tags in treebank\n",
    "nltk.help.upenn_tagset()\n",
    "\n",
    "# find the meaning of a specific tag\n",
    "nltk.help.upenn_tagset('JJ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oil', 'NN'),\n",
       " ('prices', 'NNS'),\n",
       " ('soar', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('all-time', 'JJ'),\n",
       " ('record', 'NN'),\n",
       " ('.', '.'),\n",
       " ('stocks', 'NNS'),\n",
       " ('end', 'VBP'),\n",
       " ('up', 'RP'),\n",
       " ('near', 'IN'),\n",
       " ('year', 'NN'),\n",
       " ('end', 'NN'),\n",
       " ('.', '.'),\n",
       " ('money', 'NN'),\n",
       " ('funds', 'NNS'),\n",
       " ('rose', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('latest', 'JJS'),\n",
       " ('week', 'NN'),\n",
       " ('.', '.'),\n",
       " ('stocks', 'NNS'),\n",
       " ('up', 'RP'),\n",
       " (';', ':'),\n",
       " ('traders', 'NNS'),\n",
       " ('eye', 'NN'),\n",
       " ('crude', 'VBP'),\n",
       " ('oil', 'NN'),\n",
       " ('prices', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('dollar', 'NN'),\n",
       " ('rising', 'VBG'),\n",
       " ('broadly', 'RB'),\n",
       " ('on', 'IN'),\n",
       " ('record', 'NN'),\n",
       " ('trade', 'NN'),\n",
       " ('gain', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 4.2. NLTK POS Tagging\n",
    "\n",
    "# The input to the tagging function is a list of words\n",
    "\n",
    "# tokenize the text\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "# tag each tokenized word\n",
    "tagged_tokens= nltk.pos_tag(tokens)\n",
    "\n",
    "tagged_tokens\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('oil', 'NN'), ('prices', 'NNS')),\n",
       " (('prices', 'NNS'), ('soar', 'VB')),\n",
       " (('soar', 'VB'), ('to', 'TO')),\n",
       " (('to', 'TO'), ('all-time', 'JJ')),\n",
       " (('all-time', 'JJ'), ('record', 'NN')),\n",
       " (('record', 'NN'), ('.', '.')),\n",
       " (('.', '.'), ('stocks', 'NNS')),\n",
       " (('stocks', 'NNS'), ('end', 'VBP')),\n",
       " (('end', 'VBP'), ('up', 'RP')),\n",
       " (('up', 'RP'), ('near', 'IN')),\n",
       " (('near', 'IN'), ('year', 'NN')),\n",
       " (('year', 'NN'), ('end', 'NN')),\n",
       " (('end', 'NN'), ('.', '.')),\n",
       " (('.', '.'), ('money', 'NN')),\n",
       " (('money', 'NN'), ('funds', 'NNS')),\n",
       " (('funds', 'NNS'), ('rose', 'VBD')),\n",
       " (('rose', 'VBD'), ('in', 'IN')),\n",
       " (('in', 'IN'), ('latest', 'JJS')),\n",
       " (('latest', 'JJS'), ('week', 'NN')),\n",
       " (('week', 'NN'), ('.', '.')),\n",
       " (('.', '.'), ('stocks', 'NNS')),\n",
       " (('stocks', 'NNS'), ('up', 'RP')),\n",
       " (('up', 'RP'), (';', ':')),\n",
       " ((';', ':'), ('traders', 'NNS')),\n",
       " (('traders', 'NNS'), ('eye', 'NN')),\n",
       " (('eye', 'NN'), ('crude', 'VBP')),\n",
       " (('crude', 'VBP'), ('oil', 'NN')),\n",
       " (('oil', 'NN'), ('prices', 'NNS')),\n",
       " (('prices', 'NNS'), ('.', '.')),\n",
       " (('.', '.'), ('dollar', 'NN')),\n",
       " (('dollar', 'NN'), ('rising', 'VBG')),\n",
       " (('rising', 'VBG'), ('broadly', 'RB')),\n",
       " (('broadly', 'RB'), ('on', 'IN')),\n",
       " (('on', 'IN'), ('record', 'NN')),\n",
       " (('record', 'NN'), ('trade', 'NN')),\n",
       " (('trade', 'NN'), ('gain', 'NN'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('all-time', 'record'), ('latest', 'week')]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.3. Extract Phrases by POS\n",
    "\n",
    "# Extract phrases in pattern of adjective + noun\n",
    "# i.e. nice house, growing market\n",
    "\n",
    "bigrams=list(nltk.bigrams(tagged_tokens))\n",
    "bigrams\n",
    "\n",
    "phrases=[ (x[0],y[0]) for (x,y) in bigrams \\\n",
    "         if x[1].startswith('JJ') \\\n",
    "         and y[1].startswith('NN')]\n",
    "\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prices', 'soar'), ('stocks', 'end'), ('funds', 'rose'), ('eye', 'crude'), ('dollar', 'rising')]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.4. Extract Noun+Verb, \n",
    "# i.e. prices soar\n",
    "phrases=[ (x[0],y[0]) for (x,y) in bigrams \\\n",
    "         if x[1].startswith('NN') \\\n",
    "         and y[1].startswith('VB')]\n",
    "\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalization: Stemming & Lemmatization\n",
    " - What is normalization:\n",
    "   - Converts a list of words in **different surface forms** to a more **uniform form**, e.g.\n",
    "        * a word with different forms, e.g. organize, organizes, organized, and organizing\n",
    "        * families of derivationally related words with similar meanings, such as democracy, democratic, and democratization.\n",
    " - Why normalization\n",
    "   - **improve text matching**: in many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "   - reduce featue space generated from text\n",
    " - Stemming and lemmatization are two common techinques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Stemming \n",
    "\n",
    "* **Stemming**: reducing inflected (or sometimes derived) words to their **stem, base or root** form. \n",
    "   * For example, **crying** -> **cri**. \n",
    "   * Stemming may not generate a real word, but a root form. \n",
    "   * The stemming program is called stemmer. \n",
    "       * Famous stemers are Porter stemmer, Lancaster Stemmer, Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem of organizing/organized/organizes/organization\n",
      "organ\n",
      "organ\n",
      "organ\n",
      "organ\n",
      "\n",
      "Stem of crying\n",
      "cri\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.1.1. Stermming Using Porter Stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "print(\"Stem of organizing/organized/organizes/organization\")\n",
    "print(porter_stemmer.stem('organizing'))\n",
    "print(porter_stemmer.stem('organized'))\n",
    "print(porter_stemmer.stem('organizes'))\n",
    "print(porter_stemmer.stem('organization'))\n",
    "\n",
    "print(\"\\nStem of crying\")\n",
    "print(porter_stemmer.stem('crying'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Lemmatization\n",
    "\n",
    "* **Lemmatization**: determining the lemma for a given word, \n",
    "   * A lemma is a word which stands at the head of a definition in a dictionary, e.g. run (lemma),  runs, ran and running (inflections) \n",
    "   * Lemmatization is a complex task involving understanding context and determining the part of speech of a word in a sentence \n",
    "      * e.g. \"organized\" (verb or adjective?)\n",
    "   * The widely used Lemmatization method is based on WordNet, a large lexical database of English.\n",
    "\n",
    "* **Difference** between stemming and lemmatization: \n",
    "   * a stemmer operates on a single word **without knowledge of the context**, and therefore cannot discriminate between words which have different meanings depending on part of speech. While, lemmatization **requires context and POS tags**. \n",
    "   * Stemming may not generate a real word, but lemmization always generates real words.\n",
    "   *  However, stemmers are typically easier to implement and run faster with reduced accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organizing (verb) organize\n",
      "organized (verb) organize\n",
      "organized (adjective) organized\n",
      "organization (noun) organization\n",
      "crying (adjective) crying\n",
      "crying (verb) cry\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.2.1. Lemmatization\n",
    "\n",
    "# wordnet lemmatizer takes POS tag as a parameter\n",
    "# However, wordnet has its own tag set, \n",
    "# different from treebank tag set\n",
    "# The default POS tag is noun \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"organizing (verb)\", \\\n",
    "      wordnet_lemmatizer.lemmatize\\\n",
    "      ('organizing', wordnet.VERB))\n",
    "print('organized (verb)', \\\n",
    "      wordnet_lemmatizer.lemmatize\\\n",
    "      ('organized', wordnet.VERB))\n",
    "print('organized (adjective)',\\\n",
    "      wordnet_lemmatizer.lemmatize('organized', wordnet.ADJ))\n",
    "print('organization (noun)',\\\n",
    "      wordnet_lemmatizer.lemmatize('organization'))\n",
    "print('crying (adjective)',\\\n",
    "      wordnet_lemmatizer.lemmatize('crying', wordnet.ADJ))\n",
    "print('crying (verb)', \\\n",
    "      wordnet_lemmatizer.lemmatize('crying', wordnet.VERB))\n",
    "\n",
    "# compare the result with Exercise 5.1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('oil', 'NN'), ('prices', 'NNS'), ('soar', 'VB'), ('to', 'TO'), ('all-time', 'JJ'), ('record', 'NN'), ('.', '.'), ('stocks', 'NNS'), ('end', 'VBP'), ('up', 'RP'), ('near', 'IN'), ('year', 'NN'), ('end', 'NN'), ('.', '.'), ('money', 'NN'), ('funds', 'NNS'), ('rose', 'VBD'), ('in', 'IN'), ('latest', 'JJS'), ('week', 'NN'), ('.', '.'), ('stocks', 'NNS'), ('up', 'RP'), (';', ':'), ('traders', 'NNS'), ('eye', 'NN'), ('crude', 'VBP'), ('oil', 'NN'), ('prices', 'NNS'), ('.', '.'), ('dollar', 'NN'), ('rising', 'VBG'), ('broadly', 'RB'), ('on', 'IN'), ('record', 'NN'), ('trade', 'NN'), ('gain', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.2.2. Lemmatize the tokens from text \n",
    "          \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# first tokenize the text\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "# then find the POS tag of each word\n",
    "# tagged_token is a list of (word, pos_tag)\n",
    "tagged_tokens= nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)\n",
    "\n",
    "# wordnet and treebank have different tagging systems\n",
    "# define a mapping between wordnet tags and POS tags as a function\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \n",
    "    # if pos tag starts with 'J'\n",
    "    if pos_tag.startswith('J'):\n",
    "        # return wordnet tag \"ADJ\"\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    # if pos tag starts with 'V'\n",
    "    elif pos_tag.startswith('V'):\n",
    "        # return wordnet tag \"VERB\"\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    # if pos tag starts with 'N'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        # return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # be default, return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# get lemmatized tokens\n",
    "        # lemmatize every word in tagged_tokens\n",
    "le_words=[wordnet_lemmatizer.lemmatize\\\n",
    "          (word, get_wordnet_pos(tag)) \\\n",
    "          # tagged_tokens is a list of tuples (word, tag)\n",
    "          for (word, tag) in tagged_tokens \\\n",
    "          # remove stop words\n",
    "          if word not in stop_words and \\\n",
    "          # remove punctuations\n",
    "          word not in string.punctuation]\n",
    "\n",
    "# nltk.FreqDist gives you the freq distribution \n",
    "# of items in a list \n",
    "# it's similar to a dictionary\n",
    "word_dist=nltk.FreqDist(le_words)\n",
    "\n",
    "# notice rising/rose -> rise, prices -> price\n",
    "# notice the two \"end\" tokens are tagged as VBP and NN perspectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 4),\n",
       " ('oil', 2),\n",
       " ('prices', 2),\n",
       " ('record', 2),\n",
       " ('stocks', 2),\n",
       " ('end', 2),\n",
       " ('up', 2),\n",
       " ('soar', 1),\n",
       " ('to', 1),\n",
       " ('all-time', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil 2 0.05405405405405406\n",
      "prices 2 0.05405405405405406\n",
      "soar 1 0.02702702702702703\n",
      "to 1 0.02702702702702703\n",
      "all-time 1 0.02702702702702703\n",
      "record 2 0.05405405405405406\n",
      ". 4 0.10810810810810811\n",
      "stocks 2 0.05405405405405406\n",
      "end 2 0.05405405405405406\n",
      "up 2 0.05405405405405406\n",
      "near 1 0.02702702702702703\n",
      "year 1 0.02702702702702703\n",
      "money 1 0.02702702702702703\n",
      "funds 1 0.02702702702702703\n",
      "rose 1 0.02702702702702703\n",
      "in 1 0.02702702702702703\n",
      "latest 1 0.02702702702702703\n",
      "week 1 0.02702702702702703\n",
      "; 1 0.02702702702702703\n",
      "traders 1 0.02702702702702703\n",
      "eye 1 0.02702702702702703\n",
      "crude 1 0.02702702702702703\n",
      "dollar 1 0.02702702702702703\n",
      "rising 1 0.02702702702702703\n",
      "broadly 1 0.02702702702702703\n",
      "on 1 0.02702702702702703\n",
      "trade 1 0.02702702702702703\n",
      "gain 1 0.02702702702702703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({('.', '.'): 4,\n",
       "          (';', ':'): 1,\n",
       "          ('all-time', 'JJ'): 1,\n",
       "          ('broadly', 'RB'): 1,\n",
       "          ('crude', 'VBP'): 1,\n",
       "          ('dollar', 'NN'): 1,\n",
       "          ('end', 'NN'): 1,\n",
       "          ('end', 'VBP'): 1,\n",
       "          ('eye', 'NN'): 1,\n",
       "          ('funds', 'NNS'): 1,\n",
       "          ('gain', 'NN'): 1,\n",
       "          ('in', 'IN'): 1,\n",
       "          ('latest', 'JJS'): 1,\n",
       "          ('money', 'NN'): 1,\n",
       "          ('near', 'IN'): 1,\n",
       "          ('oil', 'NN'): 2,\n",
       "          ('on', 'IN'): 1,\n",
       "          ('prices', 'NNS'): 2,\n",
       "          ('record', 'NN'): 2,\n",
       "          ('rising', 'VBG'): 1,\n",
       "          ('rose', 'VBD'): 1,\n",
       "          ('soar', 'VB'): 1,\n",
       "          ('stocks', 'NNS'): 2,\n",
       "          ('to', 'TO'): 1,\n",
       "          ('trade', 'NN'): 1,\n",
       "          ('traders', 'NNS'): 1,\n",
       "          ('up', 'RP'): 2,\n",
       "          ('week', 'NN'): 1,\n",
       "          ('year', 'NN'): 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({('.', 'dollar'): 1,\n",
       "          ('.', 'money'): 1,\n",
       "          ('.', 'stocks'): 2,\n",
       "          (';', 'traders'): 1,\n",
       "          ('all-time', 'record'): 1,\n",
       "          ('broadly', 'on'): 1,\n",
       "          ('crude', 'oil'): 1,\n",
       "          ('dollar', 'rising'): 1,\n",
       "          ('end', '.'): 1,\n",
       "          ('end', 'up'): 1,\n",
       "          ('eye', 'crude'): 1,\n",
       "          ('funds', 'rose'): 1,\n",
       "          ('in', 'latest'): 1,\n",
       "          ('latest', 'week'): 1,\n",
       "          ('money', 'funds'): 1,\n",
       "          ('near', 'year'): 1,\n",
       "          ('oil', 'prices'): 2,\n",
       "          ('on', 'record'): 1,\n",
       "          ('prices', '.'): 1,\n",
       "          ('prices', 'soar'): 1,\n",
       "          ('record', '.'): 1,\n",
       "          ('record', 'trade'): 1,\n",
       "          ('rising', 'broadly'): 1,\n",
       "          ('rose', 'in'): 1,\n",
       "          ('soar', 'to'): 1,\n",
       "          ('stocks', 'end'): 1,\n",
       "          ('stocks', 'up'): 1,\n",
       "          ('to', 'all-time'): 1,\n",
       "          ('trade', 'gain'): 1,\n",
       "          ('traders', 'eye'): 1,\n",
       "          ('up', ';'): 1,\n",
       "          ('up', 'near'): 1,\n",
       "          ('week', '.'): 1,\n",
       "          ('year', 'end'): 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A little more NLTK.FreqDist()\n",
    "\n",
    "word_dist=nltk.FreqDist(tokens)\n",
    "\n",
    "# find the top 10\n",
    "word_dist.most_common(10)\n",
    "\n",
    "# print frequency of each word\n",
    "for word in word_dist:\n",
    "    print(word, word_dist[word],word_dist.freq(word) )\n",
    "    \n",
    "# get frequency of tagged token\n",
    "word_dist=nltk.FreqDist(tagged_tokens)\n",
    "word_dist\n",
    "\n",
    "# get frequency of bigrams (or any list of items)\n",
    "bigrams=nltk.bigrams(tokens)\n",
    "bigram_dist=nltk.FreqDist(bigrams)\n",
    "bigram_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Named Entity Recognition (NER)\n",
    "\n",
    "- Definition: find and classify real word entities (Person, Organization, Event etc.) in text\n",
    "- Example: sentence \"Jim bought 300 shares of Acme Corp. in 2006\" can be annotated as \"**[Jim]<sub>Person</sub>** bought 300 shares of **[Acme Corp.]<sub>Organization</sub>** in 2006\"\n",
    "- Uses of NER:\n",
    "   *  Information Extraction: extract clear, factual information, i.e., Who did what to whom when?\n",
    "   *  Named entities can be indexed, and their relations can be extracted.\n",
    "   *  Sentiment can be attributed to companies or products\n",
    "   *  For question answering, answers are often named entities.\n",
    "- Techniques for NER\n",
    "   * Regular expression: Telephone numbers, emails, Capital names (e.g. Capitalized word + {city,  center, river}\n",
    "      * Adantages: simple and sometimes effective\n",
    "      * Disadvantage: \n",
    "         * first word of a sentence is capitalized; sometimes, titles are all capitalized; new proper names constantly emerges (e.g. movie titles, books, etc.)\n",
    "         * proper names may be ambiguous, e.g. Jordan can be *person* or *location*\n",
    "   * Supervised learning (IOB) (https://arxiv.org/abs/cmp-lg/9505040)\n",
    "       1. Collect a set of representative training documents\n",
    "       2. Label each token for its entity class (I: inside entity, B: begining entity) or other (O)\n",
    "       3. Design feature extractors appropriate to the text and classes, e.g. current word, pre/next word, pos tags etc.\n",
    "       4. Train a sequence classifier to predict the labels from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Jim/NNP)\n",
      "  bought/VBD\n",
      "  300/CD\n",
      "  shares/NNS\n",
      "  of/IN\n",
      "  (ORGANIZATION Acme/NNP Corp./NNP)\n",
      "  in/IN\n",
      "  2006/CD\n",
      "  ./.)\n",
      "PERSON [[('Jim', 'NNP')]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6.1. Use NLTK for Named Entity Recognition\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    " \n",
    "sentence = \"Jim bought 300 shares of Acme Corp. in 2006.\"\n",
    "\n",
    "# the input to ne_chunk is list of (token, pos tag) tuples\n",
    "ner_tree=ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "# ne_chunk returns a tree\n",
    "print(ner_tree)\n",
    "\n",
    "# get PERSON out of the tree\n",
    "person=[]\n",
    "for t in ner_tree.subtrees():\n",
    "    if t.label() == 'PERSON':\n",
    "        person.append(t.leaves())\n",
    "print(\"PERSON\",person)\n",
    "\n",
    "# how to extract organization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    " - Motivation: How to identify important words (or phrases, named entities) in a text in a collecton or corpus? When search for documents, we'd like to have these important words are matched.\n",
    " - Intuition: \n",
    "   * In a document, if a word/term/phrase is repeated many times, it is likely important. \n",
    "   * However, if it appears in most of the documents in the corpus, then it has little discriminating power in determining relevance. \n",
    "   * For instance, a collection of documents on the auto industry is likely to have the term auto in almost every document. Search by \"auto\" you may get all the documents. \n",
    " - ** TF-IDF**: is composed by two terms: \n",
    "      - **TF(Term Frequency)**: which measures how frequently a term, say w, occurs in a document. \n",
    "      - **IDF (Inverse Document Frequency)**: measures how important a term is within the corpus. \n",
    " \n",
    " - TF-IDF provides another way to remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Term Frequency (TF)\n",
    "- Measures how frequently a term, say w, occurs in a document, say $d$. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. \n",
    "- Thus, the frequency of $w$ in $d$, denoted as $freq(w,d)$ is often divided by the document length (a.k.a. the total number of terms in the document, denoted as $|d|$) as a way of normalization: $$tf(w,d) = \\frac{freq(w,d)}{|d|}$$\n",
    "- Example: d=\"Stocks end up near year end\"\n",
    "   * tf('Stocks',d)=?\n",
    "   * tf('end',d)=?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Inverse Document Frequency (IDF)\n",
    "- Measures how important a term is within the corpus. \n",
    "- However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. \n",
    "- Thus we need to weigh down the frequent terms while scale up the rare ones. \n",
    "- Let $|D|$ denote the number of documents, $df(w,D)$ denotes the number of documents with term $w$ in them. Then, $$idf(w) = ln(\\frac{|D|}{df(w,D)})+1$$ Or a smoothed version: $$idf(w) = ln(\\frac{|D|+1}{df(w,D)+1})+1$$\n",
    "- Examples: \n",
    "  * Considering dataset:\n",
    "       1. \"Oil prices soar to all-time record\", \n",
    "       2. \"Stocks end up near year end\", \n",
    "       3. \"Money funds rose in latest week\",\n",
    "       4. \"Stocks up; traders eye crude oil prices\",\n",
    "       5. \"Dollar rising broadly on record trade gain\"\n",
    "  * idf('Stocks')=?\n",
    "  * idf('all-time')=?\n",
    "  * Discussion:\n",
    "     * What words get very low IDF score?\n",
    "     * What words get very high IDF score?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. TF-IDF \n",
    "- Let $s(w,d)=tf(w,d) * idf(w)$, normalize the TF-IDF score of each word in a document normalized by the Euclidean norm, then \n",
    "   $$tfidf(w,d)=\\frac{s(w,d)}{\\sqrt{\\sum_{w \\in d}{s(w,d)^2}}}$$\n",
    "- For details of Euclidean norm (a.k.a L2 norm), see https://stanford.edu/class/ee103/lectures/norm/norm_slides.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 7.1. computing tf-idf\n",
    "\n",
    "\n",
    "import nltk, re, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# library for normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# numpy is the package for matrix caculation\n",
    "import numpy as np  \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "docs=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1. get tokens of each document as list\n",
    "\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc.lower()) \\\n",
    "            if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, or lemmatization here\n",
    "    \n",
    "    # create token count dictionary\n",
    "    token_count=nltk.FreqDist(tokens)\n",
    "    \n",
    "    # or you can create dictionary by yourself\n",
    "    #token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "    return token_count\n",
    "\n",
    "# step 2. process all documents to \n",
    "# a dictionary of dictionaries\n",
    "docs_tokens={idx:get_doc_tokens(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "print(docs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 3. get document-term matrix\n",
    "# contruct a document-term matrix where \n",
    "# each row is a doc \n",
    "# each column is a token\n",
    "# and the value is the frequency of the token\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# since we have a small corpus, we can use dataframe \n",
    "# to get document-term matrix\n",
    "# but don't use this when you have a large corpus\n",
    "\n",
    "dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "dtm=dtm.fillna(0)\n",
    "dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 4. get normalized term frequency (tf) matrix\n",
    "\n",
    "# convert dtm to numpy arrays\n",
    "tf=dtm.values\n",
    "\n",
    "# sum the value of each row\n",
    "doc_len=tf.sum(axis=1)\n",
    "print(doc_len)\n",
    "\n",
    "# divide dtm matrix by the doc length matrix\n",
    "tf=np.divide(tf.T, doc_len).T\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 5. get idf\n",
    "\n",
    "# get document freqent\n",
    "df=np.where(tf>0,1,0)\n",
    "#df\n",
    "\n",
    "# get idf\n",
    "idf=np.log(np.divide(len(docs), \\\n",
    "        np.sum(df, axis=0)))+1\n",
    "print(\"\\nIDF Matrix\")\n",
    "print (idf)\n",
    "\n",
    "\n",
    "smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1\n",
    "print(\"\\nSmoothed IDF Matrix\")\n",
    "print(smoothed_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 6. get tf-idf\n",
    "print(\"TF-IDF Matrix\")\n",
    "tf_idf=normalize(tf*idf)\n",
    "print(tf_idf)\n",
    "\n",
    "print(\"\\nSmoothed TF-IDF Matrix\")\n",
    "smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "print(smoothed_tf_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF matrix gives **weight** of each work in each document\n",
    "- Documents:\n",
    "    1. \"Oil prices soar to all-time record\", \n",
    "    2. \"Stocks end up near year end\", \n",
    "    3. \"Money funds rose in latest week\",\n",
    "    4. \"Stocks up; traders eye crude oil prices\",\n",
    "    5. \"Dollar rising broadly on record trade gain\"\n",
    "<img src='tfidf.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 7.2. Find the top three words \n",
    "# of each document by TF-IDF weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. What to do with TF-IDF\n",
    "- This is the feature sapce of text mining (**Bag of Words**, **Vector Space Model**)\n",
    "- Identify important words in each document\n",
    "- Find similar documents\n",
    "    * How to measure simialrity (or distance)? http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.4480&rep=rep1&type=pdf\n",
    "    * Euclidean distance: Euclidean distance is **large** for vectors of high dimension (curse of dimensionality)\n",
    "    * Cosine similarity: The similarity between two documents is a function of the angle between their vectors in the if-idf vector space. \n",
    "      <img src='cosine.png' width=50% />\n",
    "      <img src='cosine_formula.svg' width=50% />\n",
    "      - Example: A=[0,2,1], B=[1,1,2], then\n",
    "      $$cosine(A,B)=\\frac{0*1+2*1+1*2}{\\sqrt{0+4+1}*\\sqrt{1+1+4}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Exercise 7.4.1 Document similarity\n",
    "\n",
    "# package to calculate distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# calculate cosince distance of every pair of documents \n",
    "# convert the distance object into a square matrix form\n",
    "# similarity is 1-distance\n",
    "similarity=1-distance.squareform\\\n",
    "(distance.pdist(tf_idf, 'cosine'))\n",
    "similarity\n",
    "\n",
    "# find top doc similar to first one\n",
    "np.argsort(similarity)[:,::-1][0,0:2]\n",
    "\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(idx,doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Put Everyting together -- Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.corpus import stopwords\n",
    "# numpy is the package for matrix cacluation\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "\n",
    "# Step 1. get tokens of each document as list\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc.lower()) \\\n",
    "            if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, stemming, \n",
    "    # or lemmatization here\n",
    "    \n",
    "    token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "    return token_count\n",
    "\n",
    "def tfidf(docs):\n",
    "    # step 2. process all documents to get list of token list\n",
    "    docs_tokens={idx:get_doc_tokens(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "\n",
    "    # step 3. get document-term matrix\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "      \n",
    "    # step 4. get normalized term frequency (tf) matrix        \n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1)\n",
    "    tf=np.divide(tf.T, doc_len).T\n",
    "    \n",
    "    # step 5. get idf\n",
    "    df=np.where(tf>0,1,0)\n",
    "    #idf=np.log(np.divide(len(docs), \\\n",
    "    #    np.sum(df, axis=0)))+1\n",
    "\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf=tf*smoothed_idf\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
